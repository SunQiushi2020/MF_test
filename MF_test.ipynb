{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MF_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMFfjSWoPW/pZzfwQ62EVhh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunQiushi2020/MF_test/blob/main/MF_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiGHfesVIa-B"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3bFEglkIq_P"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GesThqMIrEn",
        "outputId": "f2b1fc3a-54bc-4705-fbb1-a51337e60f97"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install python3.6\n",
        "!pip install gym\n",
        "!pip install scikit-learn"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Waiting for headers] [1 \r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Waiting for headers] [Co\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rGet:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rHit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Err:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg\n",
            "  The following signatures were invalid: BADSIG F60F4B3D7FA2AF80 cudatools <cudatools@nvidia.com>\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,404 kB]\n",
            "Get:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [372 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,077 kB]\n",
            "Hit:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [402 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,751 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,508 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,175 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [896 kB]\n",
            "Get:24 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [39.5 kB]\n",
            "Fetched 11.9 MB in 4s (2,691 kB/s)\n",
            "Reading package lists... Done\n",
            "W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release: The following signatures were invalid: BADSIG F60F4B3D7FA2AF80 cudatools <cudatools@nvidia.com>\n",
            "W: Failed to fetch https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/Release.gpg  The following signatures were invalid: BADSIG F60F4B3D7FA2AF80 cudatools <cudatools@nvidia.com>\n",
            "W: Some index files failed to download. They have been ignored, or old ones used instead.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3.6 is already the newest version (3.6.9-1~18.04ubuntu1.4).\n",
            "python3.6 set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMtZFuSIIrHf",
        "outputId": "e76a9e7e-d17d-443f-df1a-5f82e4b50670"
      },
      "source": [
        "!pip install magent"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting magent\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/f2/312d5a5a593348c5d8f42bf939e7e5ff25bb70d3f8606c19fa88915ecd2f/magent-0.1.13-cp37-cp37m-manylinux2010_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from magent) (1.19.5)\n",
            "Collecting pygame>=2.0.0.dev10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/9e/c400554dd1d0e562bd4379f35ad5023c68fc120003a58991405850f56f95/pygame-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (11.8MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8MB 241kB/s \n",
            "\u001b[?25hInstalling collected packages: pygame, magent\n",
            "Successfully installed magent-0.1.13 pygame-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p29-gwSBIrKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb83854c-0239-4428-fc43-fdacc61d4f8d"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import magent\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oJB1iJfJ42d"
      },
      "source": [
        "#base\n",
        "#import tensorflow as \n",
        "#import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from magent.gridworld import GridWorld\n",
        "\n",
        "class ValueNet:\n",
        "    def __init__(self, sess, env, handle, name, update_every=5, use_mf=False, learning_rate=1e-4, tau=0.005, gamma=0.95):\n",
        "        # assert isinstance(env, GridWorld)\n",
        "        self.env = env\n",
        "        self.name = name\n",
        "        self._saver = None\n",
        "        self.sess = sess\n",
        "\n",
        "        self.handle = handle\n",
        "        self.view_space = env.get_view_space(handle)\n",
        "        assert len(self.view_space) == 3\n",
        "        self.feature_space = env.get_feature_space(handle)\n",
        "        self.num_actions = env.get_action_space(handle)[0]\n",
        "\n",
        "        self.update_every = update_every\n",
        "        self.use_mf = use_mf  # trigger of using mean field\n",
        "        self.temperature = 0.1\n",
        "\n",
        "        self.lr= learning_rate\n",
        "        self.tau = tau\n",
        "        self.gamma = gamma\n",
        "\n",
        "        with tf.variable_scope(name or \"ValueNet\"):\n",
        "            self.name_scope = tf.get_variable_scope().name\n",
        "            self.obs_input = tf.placeholder(tf.float32, (None,) + self.view_space, name=\"Obs-Input\")\n",
        "            self.feat_input = tf.placeholder(tf.float32, (None,) + self.feature_space, name=\"Feat-Input\")\n",
        "            self.mask = tf.placeholder(tf.float32, shape=(None,), name='Terminate-Mask')\n",
        "\n",
        "            if self.use_mf:\n",
        "                self.act_prob_input = tf.placeholder(tf.float32, (None, self.num_actions), name=\"Act-Prob-Input\")\n",
        "\n",
        "            # TODO: for calculating the Q-value, consider softmax usage\n",
        "            self.act_input = tf.placeholder(tf.int32, (None,), name=\"Act\")\n",
        "            self.act_one_hot = tf.one_hot(self.act_input, depth=self.num_actions, on_value=1.0, off_value=0.0)\n",
        "\n",
        "            with tf.variable_scope(\"Eval-Net\"):\n",
        "                self.eval_name = tf.get_variable_scope().name\n",
        "                self.e_q = self._construct_net(active_func=tf.nn.relu)\n",
        "                self.predict = tf.nn.softmax(self.e_q / self.temperature)\n",
        "                self.e_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.eval_name)\n",
        "\n",
        "            with tf.variable_scope(\"Target-Net\"):\n",
        "                self.target_name = tf.get_variable_scope().name\n",
        "                self.t_q = self._construct_net(active_func=tf.nn.relu)\n",
        "                self.t_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.target_name)\n",
        "\n",
        "            with tf.variable_scope(\"Update\"):\n",
        "                self.update_op = [tf.assign(self.t_variables[i],\n",
        "                                            self.tau * self.e_variables[i] + (1. - self.tau) * self.t_variables[i])\n",
        "                                    for i in range(len(self.t_variables))]\n",
        "\n",
        "            with tf.variable_scope(\"Optimization\"):\n",
        "                self.target_q_input = tf.placeholder(tf.float32, (None,), name=\"Q-Input\")\n",
        "                self.e_q_max = tf.reduce_sum(tf.multiply(self.act_one_hot, self.e_q), axis=1)\n",
        "                self.loss = tf.reduce_sum(tf.square(self.target_q_input - self.e_q_max) * self.mask) / tf.reduce_sum(self.mask)\n",
        "                self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
        "\n",
        "    def _construct_net(self, active_func=None, reuse=False):\n",
        "        conv1 = tf.layers.conv2d(self.obs_input, filters=32, kernel_size=3,\n",
        "                                 activation=active_func, name=\"Conv1\")\n",
        "        conv2 = tf.layers.conv2d(conv1, filters=32, kernel_size=3, activation=active_func,\n",
        "                                 name=\"Conv2\")\n",
        "        flatten_obs = tf.reshape(conv2, [-1, np.prod([v.value for v in conv2.shape[1:]])])\n",
        "\n",
        "        h_obs = tf.layers.dense(flatten_obs, units=256, activation=active_func,\n",
        "                                name=\"Dense-Obs\")\n",
        "        h_emb = tf.layers.dense(self.feat_input, units=32, activation=active_func,\n",
        "                                name=\"Dense-Emb\", reuse=reuse)\n",
        "\n",
        "        concat_layer = tf.concat([h_obs, h_emb], axis=1)\n",
        "\n",
        "        if self.use_mf:\n",
        "            prob_emb = tf.layers.dense(self.act_prob_input, units=64, activation=active_func, name='Prob-Emb')\n",
        "            h_act_prob = tf.layers.dense(prob_emb, units=32, activation=active_func, name=\"Dense-Act-Prob\")\n",
        "            concat_layer = tf.concat([concat_layer, h_act_prob], axis=1)\n",
        "\n",
        "        dense2 = tf.layers.dense(concat_layer, units=128, activation=active_func, name=\"Dense2\")\n",
        "        out = tf.layers.dense(dense2, units=64, activation=active_func, name=\"Dense-Out\")\n",
        "\n",
        "        q = tf.layers.dense(out, units=self.num_actions, name=\"Q-Value\")\n",
        "\n",
        "        return q\n",
        "\n",
        "    @property\n",
        "    def vars(self):\n",
        "        return tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name_scope)\n",
        "\n",
        "    def calc_target_q(self, **kwargs):\n",
        "        \"\"\"Calculate the target Q-value\n",
        "        kwargs: {'obs', 'feature', 'prob', 'dones', 'rewards'}\n",
        "        \"\"\"\n",
        "        feed_dict = {\n",
        "            self.obs_input: kwargs['obs'],\n",
        "            self.feat_input: kwargs['feature']\n",
        "        }\n",
        "\n",
        "        if self.use_mf:\n",
        "            assert kwargs.get('prob', None) is not None\n",
        "            feed_dict[self.act_prob_input] = kwargs['prob']\n",
        "\n",
        "        t_q, e_q = self.sess.run([self.t_q, self.e_q], feed_dict=feed_dict)\n",
        "        act_idx = np.argmax(e_q, axis=1)\n",
        "        q_values = t_q[np.arange(len(t_q)), act_idx]\n",
        "\n",
        "        target_q_value = kwargs['rewards'] + (1. - kwargs['dones']) * q_values.reshape(-1) * self.gamma\n",
        "\n",
        "        return target_q_value\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"Q-learning update\"\"\"\n",
        "        self.sess.run(self.update_op)\n",
        "\n",
        "    def act(self, **kwargs):\n",
        "        \"\"\"Act\n",
        "        kwargs: {'obs', 'feature', 'prob', 'eps'}\n",
        "        \"\"\"\n",
        "        feed_dict = {\n",
        "            self.obs_input: kwargs['state'][0],\n",
        "            self.feat_input: kwargs['state'][1]\n",
        "        }\n",
        "\n",
        "        self.temperature = kwargs['eps']\n",
        "\n",
        "        if self.use_mf:\n",
        "            assert kwargs.get('prob', None) is not None\n",
        "            assert len(kwargs['prob']) == len(kwargs['state'][0])\n",
        "            feed_dict[self.act_prob_input] = kwargs['prob']\n",
        "\n",
        "        actions = self.sess.run(self.predict, feed_dict=feed_dict)\n",
        "        actions = np.argmax(actions, axis=1).astype(np.int32)\n",
        "        return actions\n",
        "\n",
        "    def train(self, **kwargs):\n",
        "        \"\"\"Train the model\n",
        "        kwargs: {'state': [obs, feature], 'target_q', 'prob', 'acts'}\n",
        "        \"\"\"\n",
        "        feed_dict = {\n",
        "            self.obs_input: kwargs['state'][0],\n",
        "            self.feat_input: kwargs['state'][1],\n",
        "            self.target_q_input: kwargs['target_q'],\n",
        "            self.mask: kwargs['masks']\n",
        "        }\n",
        "\n",
        "        if self.use_mf:\n",
        "            assert kwargs.get('prob', None) is not None\n",
        "            feed_dict[self.act_prob_input] = kwargs['prob']\n",
        "\n",
        "        feed_dict[self.act_input] = kwargs['acts']\n",
        "        _, loss, e_q = self.sess.run([self.train_op, self.loss, self.e_q_max], feed_dict=feed_dict)\n",
        "        return loss, {'Eval-Q': np.round(np.mean(e_q), 6), 'Target-Q': np.round(np.mean(kwargs['target_q']), 6)}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ist9uwFHIrPc"
      },
      "source": [
        "#tools\n",
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "import os\n",
        "\n",
        "\n",
        "class Color:\n",
        "    INFO = '\\033[1;34m{}\\033[0m'\n",
        "    WARNING = '\\033[1;33m{}\\033[0m'\n",
        "    ERROR = '\\033[1;31m{}\\033[0m'\n",
        "\n",
        "\n",
        "class Buffer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def push(self, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class MetaBuffer(object):\n",
        "    def __init__(self, shape, max_len, dtype='float32'):\n",
        "        self.max_len = max_len\n",
        "        self.data = np.zeros((max_len,) + shape).astype(dtype)\n",
        "        self.start = 0\n",
        "        self.length = 0\n",
        "        self._flag = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < 0 or idx >= self.length:\n",
        "            raise KeyError()\n",
        "        return self.data[idx]\n",
        "\n",
        "    def sample(self, idx):\n",
        "        return self.data[idx % self.length]\n",
        "\n",
        "    def pull(self):\n",
        "        return self.data[:self.length]\n",
        "\n",
        "    def append(self, value):\n",
        "        start = 0\n",
        "        num = len(value)\n",
        "\n",
        "        if self._flag + num > self.max_len:\n",
        "            tail = self.max_len - self._flag\n",
        "            self.data[self._flag:] = value[:tail]\n",
        "            num -= tail\n",
        "            start = tail\n",
        "            self._flag = 0\n",
        "\n",
        "        self.data[self._flag:self._flag + num] = value[start:]\n",
        "        self._flag += num\n",
        "        self.length = min(self.length + len(value), self.max_len)\n",
        "\n",
        "    def reset_new(self, start, value):\n",
        "        self.data[start:] = value\n",
        "\n",
        "\n",
        "class EpisodesBufferEntry:\n",
        "    \"\"\"Entry for episode buffer\"\"\"\n",
        "    def __init__(self):\n",
        "        self.views = []\n",
        "        self.features = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.probs = []\n",
        "        self.terminal = False\n",
        "\n",
        "    def append(self, view, feature, action, reward, alive, probs=None):\n",
        "        self.views.append(view.copy())\n",
        "        self.features.append(feature.copy())\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        if probs is not None:\n",
        "            self.probs.append(probs)\n",
        "        if not alive:\n",
        "            self.terminal = True\n",
        "\n",
        "\n",
        "class EpisodesBuffer(Buffer):\n",
        "    \"\"\"Replay buffer to store a whole episode for all agents\n",
        "       one entry for one agent\n",
        "    \"\"\"\n",
        "    def __init__(self, use_mean=False):\n",
        "        super().__init__()\n",
        "        self.buffer = {}\n",
        "        self.use_mean = use_mean\n",
        "\n",
        "    def push(self, **kwargs):\n",
        "        view, feature = kwargs['state']\n",
        "        acts = kwargs['acts']\n",
        "        rewards = kwargs['rewards']\n",
        "        alives = kwargs['alives']\n",
        "        ids = kwargs['ids']\n",
        "\n",
        "        if self.use_mean:\n",
        "            probs = kwargs['prob']\n",
        "\n",
        "        buffer = self.buffer\n",
        "        index = np.random.permutation(len(view))\n",
        "\n",
        "        for i in range(len(ids)):\n",
        "            i = index[i]\n",
        "            entry = buffer.get(ids[i])\n",
        "            if entry is None:\n",
        "                entry = EpisodesBufferEntry()\n",
        "                buffer[ids[i]] = entry\n",
        "\n",
        "            if self.use_mean:\n",
        "                entry.append(view[i], feature[i], acts[i], rewards[i], alives[i], probs=probs[i])\n",
        "            else:\n",
        "                entry.append(view[i], feature[i], acts[i], rewards[i], alives[i])\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\" clear replay buffer \"\"\"\n",
        "        self.buffer = {}\n",
        "\n",
        "    def episodes(self):\n",
        "        \"\"\" get episodes \"\"\"\n",
        "        return self.buffer.values()\n",
        "\n",
        "\n",
        "class AgentMemory(object):\n",
        "    def __init__(self, obs_shape, feat_shape, act_n, max_len, use_mean=False):\n",
        "        self.obs0 = MetaBuffer(obs_shape, max_len)\n",
        "        self.feat0 = MetaBuffer(feat_shape, max_len)\n",
        "        self.actions = MetaBuffer((), max_len, dtype='int32')\n",
        "        self.rewards = MetaBuffer((), max_len)\n",
        "        self.terminals = MetaBuffer((), max_len, dtype='bool')\n",
        "        self.use_mean = use_mean\n",
        "\n",
        "        if self.use_mean:\n",
        "            self.prob = MetaBuffer((act_n,), max_len)\n",
        "\n",
        "    def append(self, obs0, feat0, act, reward, alive, prob=None):\n",
        "        self.obs0.append(np.array([obs0]))\n",
        "        self.feat0.append(np.array([feat0]))\n",
        "        self.actions.append(np.array([act], dtype=np.int32))\n",
        "        self.rewards.append(np.array([reward]))\n",
        "        self.terminals.append(np.array([not alive], dtype=np.bool))\n",
        "\n",
        "        if self.use_mean:\n",
        "            self.prob.append(np.array([prob]))\n",
        "\n",
        "    def pull(self):\n",
        "        res = {\n",
        "            'obs0': self.obs0.pull(),\n",
        "            'feat0': self.feat0.pull(),\n",
        "            'act': self.actions.pull(),\n",
        "            'rewards': self.rewards.pull(),\n",
        "            'terminals': self.terminals.pull(),\n",
        "            'prob': None if not self.use_mean else self.prob.pull()\n",
        "        }\n",
        "\n",
        "        return res\n",
        "\n",
        "\n",
        "class MemoryGroup(object):\n",
        "    def __init__(self, obs_shape, feat_shape, act_n, max_len, batch_size, sub_len, use_mean=False):\n",
        "        self.agent = dict()\n",
        "        self.max_len = max_len\n",
        "        self.batch_size = batch_size\n",
        "        self.obs_shape = obs_shape\n",
        "        self.feat_shape = feat_shape\n",
        "        self.sub_len = sub_len\n",
        "        self.use_mean = use_mean\n",
        "        self.act_n = act_n\n",
        "\n",
        "        self.obs0 = MetaBuffer(obs_shape, max_len)\n",
        "        self.feat0 = MetaBuffer(feat_shape, max_len)\n",
        "        self.actions = MetaBuffer((), max_len, dtype='int32')\n",
        "        self.rewards = MetaBuffer((), max_len)\n",
        "        self.terminals = MetaBuffer((), max_len, dtype='bool')\n",
        "        self.masks = MetaBuffer((), max_len, dtype='bool')\n",
        "        if use_mean:\n",
        "            self.prob = MetaBuffer((act_n,), max_len)\n",
        "        self._new_add = 0\n",
        "\n",
        "    def _flush(self, **kwargs):\n",
        "        self.obs0.append(kwargs['obs0'])\n",
        "        self.feat0.append(kwargs['feat0'])\n",
        "        self.actions.append(kwargs['act'])\n",
        "        self.rewards.append(kwargs['rewards'])\n",
        "        self.terminals.append(kwargs['terminals'])\n",
        "\n",
        "        if self.use_mean:\n",
        "            self.prob.append(kwargs['prob'])\n",
        "\n",
        "        mask = np.where(kwargs['terminals'] == True, False, True)\n",
        "        mask[-1] = False\n",
        "        self.masks.append(mask)\n",
        "\n",
        "    def push(self, **kwargs):\n",
        "        for i, _id in enumerate(kwargs['ids']):\n",
        "            if self.agent.get(_id) is None:\n",
        "                self.agent[_id] = AgentMemory(self.obs_shape, self.feat_shape, self.act_n, self.sub_len, use_mean=self.use_mean)\n",
        "            if self.use_mean:\n",
        "                self.agent[_id].append(obs0=kwargs['state'][0][i], feat0=kwargs['state'][1][i], act=kwargs['acts'][i], reward=kwargs['rewards'][i], alive=kwargs['alives'][i], prob=kwargs['prob'][i])\n",
        "            else:\n",
        "                self.agent[_id].append(obs0=kwargs['state'][0][i], feat0=kwargs['state'][1][i], act=kwargs['acts'][i], reward=kwargs['rewards'][i], alive=kwargs['alives'][i])\n",
        "\n",
        "    def tight(self):\n",
        "        ids = list(self.agent.keys())\n",
        "        np.random.shuffle(ids)\n",
        "        for ele in ids:\n",
        "            tmp = self.agent[ele].pull()\n",
        "            self._new_add += len(tmp['obs0'])\n",
        "            self._flush(**tmp)\n",
        "        self.agent = dict()  # clear\n",
        "\n",
        "    def sample(self):\n",
        "        idx = np.random.choice(self.nb_entries, size=self.batch_size)\n",
        "        next_idx = (idx + 1) % self.nb_entries\n",
        "\n",
        "        obs = self.obs0.sample(idx)\n",
        "        obs_next = self.obs0.sample(next_idx)\n",
        "        feature = self.feat0.sample(idx)\n",
        "        feature_next = self.feat0.sample(next_idx)\n",
        "        actions = self.actions.sample(idx)\n",
        "        rewards = self.rewards.sample(idx)\n",
        "        dones = self.terminals.sample(idx)\n",
        "        masks = self.masks.sample(idx)\n",
        "\n",
        "        if self.use_mean:\n",
        "            act_prob = self.prob.sample(idx)\n",
        "            act_next_prob = self.prob.sample(next_idx)\n",
        "            return obs, feature, actions, act_prob, obs_next, feature_next, act_next_prob, rewards, dones, masks\n",
        "        else:\n",
        "            return obs, feature, obs_next, feature_next, dones, rewards, actions, masks\n",
        "\n",
        "    def get_batch_num(self):\n",
        "        print('\\n[INFO] Length of buffer and new add:', len(self.obs0), self._new_add)\n",
        "        res = self._new_add * 2 // self.batch_size\n",
        "        self._new_add = 0\n",
        "        return res\n",
        "\n",
        "    @property\n",
        "    def nb_entries(self):\n",
        "        return len(self.obs0)\n",
        "\n",
        "\n",
        "class SummaryObj:\n",
        "    \"\"\"\n",
        "    Define a summary holder\n",
        "    \"\"\"\n",
        "    def __init__(self, log_dir, log_name, n_group=1):\n",
        "        self.name_set = set()\n",
        "        self.gra = tf.Graph()\n",
        "        self.n_group = n_group\n",
        "\n",
        "        if not os.path.exists(log_dir):\n",
        "            os.makedirs(log_dir)\n",
        "\n",
        "        sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
        "        sess_config.gpu_options.allow_growth = True\n",
        "\n",
        "        with self.gra.as_default():\n",
        "            self.sess = tf.Session(graph=self.gra, config=sess_config)\n",
        "            self.train_writer = tf.summary.FileWriter(log_dir + \"/\" + log_name, graph=tf.get_default_graph())\n",
        "            self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def register(self, name_list):\n",
        "        \"\"\"Register summary operations with a list contains names for these operations\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        name_list: list, contains name whose type is str\n",
        "        \"\"\"\n",
        "\n",
        "        with self.gra.as_default():\n",
        "            for name in name_list:\n",
        "                if name in self.name_set:\n",
        "                    raise Exception(\"You cannot define different operations with same name: `{}`\".format(name))\n",
        "                self.name_set.add(name)\n",
        "                setattr(self, name, [tf.placeholder(tf.float32, shape=None, name='Agent_{}_{}'.format(i, name))\n",
        "                                     for i in range(self.n_group)])\n",
        "                setattr(self, name + \"_op\", [tf.summary.scalar('Agent_{}_{}_op'.format(i, name), getattr(self, name)[i])\n",
        "                                             for i in range(self.n_group)])\n",
        "\n",
        "    def write(self, summary_dict, step):\n",
        "        \"\"\"Write summary related to a certain step\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        summary_dict: dict, summary value dict\n",
        "        step: int, global step\n",
        "        \"\"\"\n",
        "\n",
        "        assert isinstance(summary_dict, dict)\n",
        "\n",
        "        for key, value in summary_dict.items():\n",
        "            if key not in self.name_set:\n",
        "                raise Exception(\"Undefined operation: `{}`\".format(key))\n",
        "            if isinstance(value, list):\n",
        "                for i in range(self.n_group):\n",
        "                    self.train_writer.add_summary(self.sess.run(getattr(self, key + \"_op\")[i], feed_dict={\n",
        "                        getattr(self, key)[i]: value[i]}), global_step=step)\n",
        "            else:\n",
        "                self.train_writer.add_summary(self.sess.run(getattr(self, key + \"_op\")[0], feed_dict={\n",
        "                        getattr(self, key)[0]: value}), global_step=step)\n",
        "\n",
        "\n",
        "class Runner(object):\n",
        "    def __init__(self, sess, env, handles, map_size, max_steps, models,\n",
        "                play_handle, render_every=None, save_every=None, tau=None, log_name=None, log_dir=None, model_dir=None, train=False):\n",
        "        \"\"\"Initialize runner\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        sess: tf.Session\n",
        "            session\n",
        "        env: magent.GridWorld\n",
        "            environment handle\n",
        "        handles: list\n",
        "            group handles\n",
        "        map_size: int\n",
        "            map size of grid world\n",
        "        max_steps: int\n",
        "            the maximum of stages in a episode\n",
        "        render_every: int\n",
        "            render environment interval\n",
        "        save_every: int\n",
        "            states the interval of evaluation for self-play update\n",
        "        models: list\n",
        "            contains models\n",
        "        play_handle: method like\n",
        "            run game\n",
        "        tau: float\n",
        "            tau index for self-play update\n",
        "        log_name: str\n",
        "            define the name of log dir\n",
        "        log_dir: str\n",
        "            donates the directory of logs\n",
        "        model_dir: str\n",
        "            donates the dircetory of models\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.models = models\n",
        "        self.max_steps = max_steps\n",
        "        self.handles = handles\n",
        "        self.map_size = map_size\n",
        "        self.render_every = render_every\n",
        "        self.save_every = save_every\n",
        "        self.play = play_handle\n",
        "        self.model_dir = model_dir\n",
        "        self.train = train\n",
        "\n",
        "        if self.train:\n",
        "            self.summary = SummaryObj(log_name=log_name, log_dir=log_dir)\n",
        "\n",
        "            summary_items = ['ave_agent_reward', 'total_reward', 'kill', \"Sum_Reward\", \"Kill_Sum\"]\n",
        "            self.summary.register(summary_items)  # summary register\n",
        "            self.summary_items = summary_items\n",
        "\n",
        "            assert isinstance(sess, tf.Session)\n",
        "            assert self.models[0].name_scope != self.models[1].name_scope\n",
        "            self.sess = sess\n",
        "\n",
        "            l_vars, r_vars = self.models[0].vars, self.models[1].vars\n",
        "            assert len(l_vars) == len(r_vars)\n",
        "            self.sp_op = [tf.assign(r_vars[i], (1. - tau) * l_vars[i] + tau * r_vars[i])\n",
        "                                for i in range(len(l_vars))]\n",
        "\n",
        "            if not os.path.exists(self.model_dir):\n",
        "                os.makedirs(self.model_dir)\n",
        "\n",
        "    def run(self, variant_eps, iteration, win_cnt=None):\n",
        "        info = {'mian': None, 'opponent': None}\n",
        "\n",
        "        # pass\n",
        "        info['main'] = {'ave_agent_reward': 0., 'total_reward': 0., 'kill': 0.}\n",
        "        info['opponent'] = {'ave_agent_reward': 0., 'total_reward': 0., 'kill': 0.}\n",
        "\n",
        "        max_nums, nums, agent_r_records, total_rewards = self.play(env=self.env, n_round=iteration, map_size=self.map_size, max_steps=self.max_steps, handles=self.handles,\n",
        "                    models=self.models, print_every=50, eps=variant_eps, render=(iteration + 1) % self.render_every if self.render_every > 0 else False, train=self.train)\n",
        "\n",
        "        for i, tag in enumerate(['main', 'opponent']):\n",
        "            info[tag]['total_reward'] = total_rewards[i]\n",
        "            info[tag]['kill'] = max_nums[i] - nums[1 - i]\n",
        "            info[tag]['ave_agent_reward'] = agent_r_records[i]\n",
        "\n",
        "        if self.train:\n",
        "            print('\\n[INFO] {}'.format(info['main']))\n",
        "\n",
        "            # if self.save_every and (iteration + 1) % self.save_every == 0:\n",
        "            if info['main']['total_reward'] > info['opponent']['total_reward']:\n",
        "                print(Color.INFO.format('\\n[INFO] Begin self-play Update ...'))\n",
        "                self.sess.run(self.sp_op)\n",
        "                print(Color.INFO.format('[INFO] Self-play Updated!\\n'))\n",
        "\n",
        "                print(Color.INFO.format('[INFO] Saving model ...'))\n",
        "                self.models[0].save(self.model_dir + '-0', iteration)\n",
        "                self.models[1].save(self.model_dir + '-1', iteration)\n",
        "\n",
        "                self.summary.write(info['main'], iteration)\n",
        "        else:\n",
        "            print('\\n[INFO] {0} \\n {1}'.format(info['main'], info['opponent']))\n",
        "            if info['main']['kill'] > info['opponent']['kill']:\n",
        "                win_cnt['main'] += 1\n",
        "            elif info['main']['kill'] < info['opponent']['kill']:\n",
        "                win_cnt['opponent'] += 1\n",
        "            else:\n",
        "                win_cnt['main'] += 1\n",
        "                win_cnt['opponent'] += 1\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqrZusUCIrSB"
      },
      "source": [
        "#Q_learning\n",
        "import os\n",
        "#import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class DQN(ValueNet):\n",
        "    def __init__(self, sess, name, handle, env, sub_len, memory_size=2**10, batch_size=64, update_every=5):\n",
        "\n",
        "        super().__init__(sess, env, handle, name, update_every=update_every)\n",
        "\n",
        "        self.replay_buffer = MemoryGroup(self.view_space, self.feature_space, self.num_actions, memory_size, batch_size, sub_len)\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def flush_buffer(self, **kwargs):\n",
        "        self.replay_buffer.push(**kwargs)\n",
        "\n",
        "    def train(self):\n",
        "        self.replay_buffer.tight()\n",
        "        batch_num = self.replay_buffer.get_batch_num()\n",
        "\n",
        "        for i in range(batch_num):\n",
        "            obs, feats, obs_next, feat_next, dones, rewards, actions, masks = self.replay_buffer.sample()\n",
        "            target_q = self.calc_target_q(obs=obs_next, feature=feat_next, rewards=rewards, dones=dones)\n",
        "            loss, q = super().train(state=[obs, feats], target_q=target_q, acts=actions, masks=masks)\n",
        "\n",
        "            self.update()\n",
        "\n",
        "            if i % 50 == 0:\n",
        "                print('[*] LOSS:', loss, '/ Q:', q)\n",
        "\n",
        "    def save(self, dir_path, step=0):\n",
        "        model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, self.name_scope)\n",
        "        saver = tf.train.Saver(model_vars)\n",
        "\n",
        "        file_path = os.path.join(dir_path, \"dqn_{}\".format(step))\n",
        "        saver.save(self.sess, file_path)\n",
        "\n",
        "        print(\"[*] Model saved at: {}\".format(file_path))\n",
        "\n",
        "    def load(self, dir_path, step=0):\n",
        "        model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, self.name_scope)\n",
        "        saver = tf.train.Saver(model_vars)\n",
        "\n",
        "        file_path = os.path.join(dir_path, \"dqn_{}\".format(step))\n",
        "\n",
        "        saver.restore(self.sess, file_path)\n",
        "        print(\"[*] Loaded model from {}\".format(file_path))\n",
        "\n",
        "\n",
        "class MFQ(ValueNet):\n",
        "    def __init__(self, sess, name, handle, env, sub_len, eps=1.0, update_every=5, memory_size=2**10, batch_size=64):\n",
        "        super().__init__(sess, env, handle, name, use_mf=True, update_every=update_every)\n",
        "\n",
        "        config = {\n",
        "            'max_len': memory_size,\n",
        "            'batch_size': batch_size,\n",
        "            'obs_shape': self.view_space,\n",
        "            'feat_shape': self.feature_space,\n",
        "            'act_n': self.num_actions,\n",
        "            'use_mean': True,\n",
        "            'sub_len': sub_len\n",
        "        }\n",
        "\n",
        "        self.train_ct = 0\n",
        "        self.replay_buffer = MemoryGroup(**config)\n",
        "        self.update_every = update_every\n",
        "\n",
        "    def flush_buffer(self, **kwargs):\n",
        "        self.replay_buffer.push(**kwargs)\n",
        "\n",
        "    def train(self):\n",
        "        self.replay_buffer.tight()\n",
        "        batch_name = self.replay_buffer.get_batch_num()\n",
        "\n",
        "        for i in range(batch_name):\n",
        "            obs, feat, acts, act_prob, obs_next, feat_next, act_prob_next, rewards, dones, masks = self.replay_buffer.sample()\n",
        "            target_q = self.calc_target_q(obs=obs_next, feature=feat_next, rewards=rewards, dones=dones, prob=act_prob_next)\n",
        "            loss, q = super().train(state=[obs, feat], target_q=target_q, prob=act_prob, acts=acts, masks=masks)\n",
        "\n",
        "            self.update()\n",
        "\n",
        "            if i % 50 == 0:\n",
        "                print('[*] LOSS:', loss, '/ Q:', q)\n",
        "\n",
        "\n",
        "    def save(self, dir_path, step=0):\n",
        "        model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, self.name_scope)\n",
        "        saver = tf.train.Saver(model_vars)\n",
        "\n",
        "        file_path = os.path.join(dir_path, \"dqn_{}\".format(step))\n",
        "        saver.save(self.sess, file_path)\n",
        "\n",
        "        print(\"[*] Model saved at: {}\".format(file_path))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_uC0sLd3P_H"
      },
      "source": [
        "#ac\n",
        "import os\n",
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "class ActorCritic:\n",
        "    def __init__(self, sess, name, handle, env, value_coef=0.1, ent_coef=0.08, gamma=0.95, batch_size=64, learning_rate=1e-4):\n",
        "        self.sess = sess\n",
        "        self.env = env\n",
        "\n",
        "        self.name = name\n",
        "        self.view_space = env.get_view_space(handle)\n",
        "        self.feature_space = env.get_feature_space(handle)\n",
        "        self.num_actions = env.get_action_space(handle)[0]\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.value_coef = value_coef  # coefficient of value in the total loss\n",
        "        self.ent_coef = ent_coef  # coefficient of entropy in the total loss\n",
        "\n",
        "        # init training buffers\n",
        "        self.view_buf = np.empty((1,) + self.view_space)\n",
        "        self.feature_buf = np.empty((1,) + self.feature_space)\n",
        "        self.action_buf = np.empty(1, dtype=np.int32)\n",
        "        self.reward_buf = np.empty(1, dtype=np.float32)\n",
        "        self.replay_buffer = EpisodesBuffer()\n",
        "\n",
        "        with tf.variable_scope(name):\n",
        "            self.name_scope = tf.get_variable_scope().name\n",
        "            self._create_network(self.view_space, self.feature_space)\n",
        "    \n",
        "    @property\n",
        "    def vars(self):\n",
        "        return tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name_scope)\n",
        "    \n",
        "    def flush_buffer(self, **kwargs):\n",
        "        self.replay_buffer.push(**kwargs)\n",
        "\n",
        "    def act(self, **kwargs):\n",
        "        action = self.sess.run(self.calc_action, {\n",
        "            self.input_view: kwargs['state'][0],\n",
        "            self.input_feature: kwargs['state'][1]\n",
        "        })\n",
        "        return action.astype(np.int32).reshape((-1,))\n",
        "\n",
        "    def _create_network(self, view_space, feature_space):\n",
        "        input_view = tf.placeholder(tf.float32, (None,) + view_space)\n",
        "        input_feature = tf.placeholder(tf.float32, (None,) + feature_space)\n",
        "        action = tf.placeholder(tf.int32, [None])\n",
        "\n",
        "        reward = tf.placeholder(tf.float32, [None])\n",
        "\n",
        "        hidden_size = [256]\n",
        "\n",
        "        # fully connected\n",
        "        flatten_view = tf.reshape(input_view, [-1, np.prod([v.value for v in input_view.shape[1:]])])\n",
        "        h_view = tf.layers.dense(flatten_view, units=hidden_size[0], activation=tf.nn.relu)\n",
        "\n",
        "        h_emb = tf.layers.dense(input_feature,  units=hidden_size[0], activation=tf.nn.relu)\n",
        "\n",
        "        dense = tf.concat([h_view, h_emb], axis=1)\n",
        "        dense = tf.layers.dense(dense, units=hidden_size[0] * 2, activation=tf.nn.relu)\n",
        "\n",
        "        policy = tf.layers.dense(dense / 0.1, units=self.num_actions, activation=tf.nn.softmax)\n",
        "        policy = tf.clip_by_value(policy, 1e-10, 1-1e-10)\n",
        "\n",
        "        self.calc_action = tf.multinomial(tf.log(policy), 1)\n",
        "\n",
        "        value = tf.layers.dense(dense, units=1)\n",
        "        value = tf.reshape(value, (-1,))\n",
        "\n",
        "        action_mask = tf.one_hot(action, self.num_actions)\n",
        "        advantage = tf.stop_gradient(reward - value)\n",
        "\n",
        "        log_policy = tf.log(policy + 1e-6)\n",
        "        log_prob = tf.reduce_sum(log_policy * action_mask, axis=1)\n",
        "\n",
        "        pg_loss = -tf.reduce_mean(advantage * log_prob)\n",
        "        vf_loss = self.value_coef * tf.reduce_mean(tf.square(reward - value))\n",
        "        neg_entropy = self.ent_coef * tf.reduce_mean(tf.reduce_sum(policy * log_policy, axis=1))\n",
        "        total_loss = pg_loss + vf_loss + neg_entropy\n",
        "\n",
        "        # train op (clip gradient)\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "        gradients, variables = zip(*optimizer.compute_gradients(total_loss))\n",
        "        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "        self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "        train_op = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(total_loss)\n",
        "\n",
        "        self.input_view = input_view\n",
        "        self.input_feature = input_feature\n",
        "        self.action = action\n",
        "        self.reward = reward\n",
        "\n",
        "        self.policy, self.value = policy, value\n",
        "        self.train_op = train_op\n",
        "        self.pg_loss, self.vf_loss, self.reg_loss = pg_loss, vf_loss, neg_entropy\n",
        "        self.total_loss = total_loss\n",
        "\n",
        "    def train(self):\n",
        "        # calc buffer size\n",
        "        n = 0\n",
        "        # batch_data = sample_buffer.episodes()\n",
        "        batch_data = self.replay_buffer.episodes()\n",
        "        self.replay_buffer = EpisodesBuffer()\n",
        "\n",
        "        for episode in batch_data:\n",
        "            n += len(episode.rewards)\n",
        "\n",
        "        self.view_buf.resize((n,) + self.view_space)\n",
        "        self.feature_buf.resize((n,) + self.feature_space)\n",
        "        self.action_buf.resize(n)\n",
        "        self.reward_buf.resize(n)\n",
        "        view, feature = self.view_buf, self.feature_buf\n",
        "        action, reward = self.action_buf, self.reward_buf\n",
        "\n",
        "        ct = 0\n",
        "        gamma = self.gamma\n",
        "        # collect episodes from multiple separate buffers to a continuous buffer\n",
        "        for episode in batch_data:\n",
        "            v, f, a, r = episode.views, episode.features, episode.actions, episode.rewards\n",
        "            m = len(episode.rewards)\n",
        "\n",
        "            r = np.array(r)\n",
        "\n",
        "            keep = self.sess.run(self.value, feed_dict={\n",
        "                self.input_view: [v[-1]],\n",
        "                self.input_feature: [f[-1]],\n",
        "            })[0]\n",
        "\n",
        "            for i in reversed(range(m)):\n",
        "                keep = keep * gamma + r[i]\n",
        "                r[i] = keep\n",
        "\n",
        "            view[ct:ct + m] = v\n",
        "            feature[ct:ct + m] = f\n",
        "            action[ct:ct + m] = a\n",
        "            reward[ct:ct + m] = r\n",
        "            ct += m\n",
        "\n",
        "        assert n == ct\n",
        "\n",
        "        # train\n",
        "        _, pg_loss, vf_loss, ent_loss, state_value = self.sess.run(\n",
        "            [self.train_op, self.pg_loss, self.vf_loss, self.reg_loss, self.value], feed_dict={\n",
        "                self.input_view: view,\n",
        "                self.input_feature: feature,\n",
        "                self.action: action,\n",
        "                self.reward: reward,\n",
        "            })\n",
        "\n",
        "        print('[*] PG_LOSS:', np.round(pg_loss, 6), '/ VF_LOSS:', np.round(vf_loss, 6), '/ ENT_LOSS:', np.round(ent_loss), '/ Value:', np.mean(state_value))\n",
        "\n",
        "    def save(self, dir_path, step=0):\n",
        "        model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, self.name_scope)\n",
        "        saver = tf.train.Saver(model_vars)\n",
        "\n",
        "        file_path = os.path.join(dir_path, \"ac_{}\".format(step))\n",
        "        saver.save(self.sess, file_path)\n",
        "\n",
        "        print(\"[*] Model saved at: {}\".format(file_path))\n",
        "\n",
        "    def load(self, dir_path, step=0):\n",
        "        model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, self.name_scope)\n",
        "        saver = tf.train.Saver(model_vars)\n",
        "\n",
        "        file_path = os.path.join(dir_path, \"ac_{}\".format(step))\n",
        "\n",
        "        saver.restore(self.sess, file_path)\n",
        "        print(\"[*] Loaded model from {}\".format(file_path))\n",
        "\n",
        "\n",
        "class MFAC:\n",
        "    def __init__(self, sess, name, handle, env, value_coef=0.1, ent_coef=0.08, gamma=0.95, batch_size=64, learning_rate=1e-4):\n",
        "        self.sess = sess\n",
        "        self.env = env\n",
        "        self.name = name\n",
        "\n",
        "        self.view_space = env.get_view_space(handle)\n",
        "        self.feature_space = env.get_feature_space(handle)\n",
        "        self.num_actions = env.get_action_space(handle)[0]\n",
        "        self.reward_decay = gamma\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.value_coef = value_coef  # coefficient of value in the total loss\n",
        "        self.ent_coef = ent_coef  # coefficient of entropy in the total loss\n",
        "\n",
        "        # init training buffers\n",
        "        self.view_buf = np.empty((1,) + self.view_space)\n",
        "        self.feature_buf = np.empty((1,) + self.feature_space)\n",
        "        self.action_buf = np.empty(1, dtype=np.int32)\n",
        "        self.reward_buf = np.empty(1, dtype=np.float32)\n",
        "        self.replay_buffer = EpisodesBuffer(use_mean=True)\n",
        "\n",
        "        with tf.variable_scope(name):\n",
        "            self.name_scope = tf.get_variable_scope().name\n",
        "            self._create_network(self.view_space, self.feature_space, )\n",
        "    \n",
        "    @property\n",
        "    def vars(self):\n",
        "        return tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name_scope)\n",
        "    \n",
        "    def flush_buffer(self, **kwargs):\n",
        "        self.replay_buffer.push(**kwargs)\n",
        "\n",
        "    def act(self, **kwargs):\n",
        "        action = self.sess.run(self.calc_action, {\n",
        "            self.input_view: kwargs['state'][0],\n",
        "            self.input_feature: kwargs['state'][1]\n",
        "        })\n",
        "        return action.astype(np.int32).reshape((-1,))\n",
        "\n",
        "    def _create_network(self, view_space, feature_space):\n",
        "        # input\n",
        "        input_view = tf.placeholder(tf.float32, (None,) + view_space)\n",
        "        input_feature = tf.placeholder(tf.float32, (None,) + feature_space)\n",
        "        input_act_prob = tf.placeholder(tf.float32, (None, self.num_actions))\n",
        "        action = tf.placeholder(tf.int32, [None])\n",
        "\n",
        "        reward = tf.placeholder(tf.float32, [None])\n",
        "\n",
        "        hidden_size = [256]\n",
        "\n",
        "        # fully connected\n",
        "        flatten_view = tf.reshape(input_view, [-1, np.prod([v.value for v in input_view.shape[1:]])])\n",
        "        h_view = tf.layers.dense(flatten_view, units=hidden_size[0], activation=tf.nn.relu)\n",
        "\n",
        "        h_emb = tf.layers.dense(input_feature,  units=hidden_size[0], activation=tf.nn.relu)\n",
        "\n",
        "        concat_layer = tf.concat([h_view, h_emb], axis=1)\n",
        "        dense = tf.layers.dense(concat_layer, units=hidden_size[0] * 2, activation=tf.nn.relu)\n",
        "\n",
        "        policy = tf.layers.dense(dense / 0.1, units=self.num_actions, activation=tf.nn.softmax)\n",
        "        policy = tf.clip_by_value(policy, 1e-10, 1-1e-10)\n",
        "\n",
        "        self.calc_action = tf.multinomial(tf.log(policy), 1)\n",
        "\n",
        "        # for value obtain\n",
        "        emb_prob = tf.layers.dense(input_act_prob, units=64, activation=tf.nn.relu)\n",
        "        dense_prob = tf.layers.dense(emb_prob, units=32, activation=tf.nn.relu)\n",
        "        concat_layer = tf.concat([concat_layer, dense_prob], axis=1)\n",
        "        dense = tf.layers.dense(concat_layer, units=hidden_size[0], activation=tf.nn.relu)\n",
        "        value = tf.layers.dense(dense, units=1)\n",
        "        value = tf.reshape(value, (-1,))\n",
        "\n",
        "        action_mask = tf.one_hot(action, self.num_actions)\n",
        "        advantage = tf.stop_gradient(reward - value)\n",
        "\n",
        "        log_policy = tf.log(policy + 1e-6)\n",
        "        log_prob = tf.reduce_sum(log_policy * action_mask, axis=1)\n",
        "\n",
        "        pg_loss = -tf.reduce_mean(advantage * log_prob)\n",
        "        vf_loss = self.value_coef * tf.reduce_mean(tf.square(reward - value))\n",
        "        neg_entropy = self.ent_coef * tf.reduce_mean(tf.reduce_sum(policy * log_policy, axis=1))\n",
        "        total_loss = pg_loss + vf_loss + neg_entropy\n",
        "\n",
        "        # train op (clip gradient)\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "        gradients, variables = zip(*optimizer.compute_gradients(total_loss))\n",
        "        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "        self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "        train_op = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(total_loss)\n",
        "\n",
        "        self.input_view = input_view\n",
        "        self.input_feature = input_feature\n",
        "        self.input_act_prob = input_act_prob\n",
        "        self.action = action\n",
        "        self.reward = reward\n",
        "\n",
        "        self.policy, self.value = policy, value\n",
        "        self.train_op = train_op\n",
        "        self.pg_loss, self.vf_loss, self.reg_loss = pg_loss, vf_loss, neg_entropy\n",
        "        self.total_loss = total_loss\n",
        "\n",
        "    def train(self):\n",
        "        # calc buffer size\n",
        "        n = 0\n",
        "        # batch_data = sample_buffer.episodes()\n",
        "        batch_data = self.replay_buffer.episodes()\n",
        "        self.replay_buffer = EpisodesBuffer(use_mean=True)\n",
        "\n",
        "        for episode in batch_data:\n",
        "            n += len(episode.rewards)\n",
        "\n",
        "        self.view_buf.resize((n,) + self.view_space)\n",
        "        self.feature_buf.resize((n,) + self.feature_space)\n",
        "        self.action_buf.resize(n)\n",
        "        self.reward_buf.resize(n)\n",
        "        view, feature = self.view_buf, self.feature_buf\n",
        "        action, reward = self.action_buf, self.reward_buf\n",
        "        act_prob_buff = np.zeros((n, self.num_actions), dtype=np.float32)\n",
        "\n",
        "        ct = 0\n",
        "        gamma = self.reward_decay\n",
        "        # collect episodes from multiple separate buffers to a continuous buffer\n",
        "        for k, episode in enumerate(batch_data):\n",
        "            v, f, a, r, prob = episode.views, episode.features, episode.actions, episode.rewards, episode.probs\n",
        "            m = len(episode.rewards)\n",
        "\n",
        "    def save(self, dir_path, step=0):\n",
        "        model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, self.name_scope)\n",
        "        saver = tf.train.Saver(model_vars)\n",
        "\n",
        "        file_path = os.path.join(dir_path, \"ac_{}\".format(step))\n",
        "        saver.save(self.sess, file_path)\n",
        "\n",
        "        print(\"[*] Model saved at: {}\".format(file_path))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py1NEZ2p3QGe"
      },
      "source": [
        "#spawn ai\n",
        "AC = ActorCritic\n",
        "MFAC = MFAC\n",
        "IL = DQN\n",
        "MFQ = MFQ\n",
        "\n",
        "\n",
        "def spawn_ai(algo_name, sess, env, handle, human_name, max_steps):\n",
        "    if algo_name == 'mfq':\n",
        "        model = MFQ(sess, human_name, handle, env, max_steps, memory_size=80000)\n",
        "    elif algo_name == 'mfac':\n",
        "        model = MFAC(sess, human_name, handle, env)\n",
        "    elif algo_name == 'ac':\n",
        "        model = AC(sess, human_name, handle, env)\n",
        "    elif algo_name == 'il':\n",
        "        model = IL(sess, human_name, handle, env, max_steps, memory_size=80000)\n",
        "    return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irjXX0Q03QJg"
      },
      "source": [
        "#senario_battle\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def generate_map(env, map_size, handles):\n",
        "    \"\"\" generate a map, which consists of two squares of agents\"\"\"\n",
        "    width = height = map_size\n",
        "    init_num = map_size * map_size * 0.04\n",
        "    gap = 3\n",
        "\n",
        "    leftID = random.randint(0, 1)\n",
        "    rightID = 1 - leftID\n",
        "\n",
        "    # left\n",
        "    n = init_num\n",
        "    side = int(math.sqrt(n)) * 2\n",
        "    pos = []\n",
        "    for x in range(width//2 - gap - side, width//2 - gap - side + side, 2):\n",
        "        for y in range((height - side)//2, (height - side)//2 + side, 2):\n",
        "            pos.append([x, y, 0])\n",
        "    env.add_agents(handles[leftID], method=\"custom\", pos=pos)\n",
        "\n",
        "    # right\n",
        "    n = init_num\n",
        "    side = int(math.sqrt(n)) * 2\n",
        "    pos = []\n",
        "    for x in range(width//2 + gap, width//2 + gap + side, 2):\n",
        "        for y in range((height - side)//2, (height - side)//2 + side, 2):\n",
        "            pos.append([x, y, 0])\n",
        "    env.add_agents(handles[rightID], method=\"custom\", pos=pos)\n",
        "\n",
        "\n",
        "def play(env, n_round, map_size, max_steps, handles, models, print_every, eps=1.0, render=False, train=False):\n",
        "    \"\"\"play a ground and train\"\"\"\n",
        "    env.reset()\n",
        "    generate_map(env, map_size, handles)\n",
        "\n",
        "    step_ct = 0\n",
        "    done = False\n",
        "\n",
        "    n_group = len(handles)\n",
        "    state = [None for _ in range(n_group)]\n",
        "    acts = [None for _ in range(n_group)]\n",
        "    ids = [None for _ in range(n_group)]\n",
        "\n",
        "    alives = [None for _ in range(n_group)]\n",
        "    rewards = [None for _ in range(n_group)]\n",
        "    nums = [env.get_num(handle) for handle in handles]\n",
        "    max_nums = nums.copy()\n",
        "\n",
        "    loss = [None for _ in range(n_group)]\n",
        "    eval_q = [None for _ in range(n_group)]\n",
        "    n_action = [env.get_action_space(handles[0])[0], env.get_action_space(handles[1])[0]]\n",
        "\n",
        "    print(\"\\n\\n[*] ROUND #{0}, EPS: {1:.2f} NUMBER: {2}\".format(n_round, eps, nums))\n",
        "    mean_rewards = [[] for _ in range(n_group)]\n",
        "    total_rewards = [[] for _ in range(n_group)]\n",
        "\n",
        "    former_act_prob = [np.zeros((1, env.get_action_space(handles[0])[0])), np.zeros((1, env.get_action_space(handles[1])[0]))]\n",
        "\n",
        "    while not done and step_ct < max_steps:\n",
        "        # take actions for every model\n",
        "        for i in range(n_group):\n",
        "            state[i] = list(env.get_observation(handles[i]))\n",
        "            ids[i] = env.get_agent_id(handles[i])\n",
        "\n",
        "        for i in range(n_group):\n",
        "            former_act_prob[i] = np.tile(former_act_prob[i], (len(state[i][0]), 1))\n",
        "            acts[i] = models[i].act(state=state[i], prob=former_act_prob[i], eps=eps)\n",
        "\n",
        "        for i in range(n_group):\n",
        "            env.set_action(handles[i], acts[i])\n",
        "\n",
        "        # simulate one step\n",
        "        done = env.step()\n",
        "\n",
        "        for i in range(n_group):\n",
        "            rewards[i] = env.get_reward(handles[i])\n",
        "            alives[i] = env.get_alive(handles[i])\n",
        "\n",
        "        buffer = {\n",
        "            'state': state[0], 'acts': acts[0], 'rewards': rewards[0],\n",
        "            'alives': alives[0], 'ids': ids[0]\n",
        "        }\n",
        "\n",
        "        buffer['prob'] = former_act_prob[0]\n",
        "\n",
        "        for i in range(n_group):\n",
        "            former_act_prob[i] = np.mean(list(map(lambda x: np.eye(n_action[i])[x], acts[i])), axis=0, keepdims=True)\n",
        "\n",
        "        if train:\n",
        "            models[0].flush_buffer(**buffer)\n",
        "\n",
        "        # stat info\n",
        "        nums = [env.get_num(handle) for handle in handles]\n",
        "\n",
        "        for i in range(n_group):\n",
        "            sum_reward = sum(rewards[i])\n",
        "            rewards[i] = sum_reward / nums[i]\n",
        "            mean_rewards[i].append(rewards[i])\n",
        "            total_rewards[i].append(sum_reward)\n",
        "\n",
        "        if render:\n",
        "            env.render()\n",
        "\n",
        "        # clear dead agents\n",
        "        env.clear_dead()\n",
        "\n",
        "        info = {\"Ave-Reward\": np.round(rewards, decimals=6), \"NUM\": nums}\n",
        "\n",
        "        step_ct += 1\n",
        "\n",
        "        if step_ct % print_every == 0:\n",
        "            print(\"> step #{}, info: {}\".format(step_ct, info))\n",
        "\n",
        "    if train:\n",
        "        models[0].train()\n",
        "\n",
        "    for i in range(n_group):\n",
        "        mean_rewards[i] = sum(mean_rewards[i]) / len(mean_rewards[i])\n",
        "        total_rewards[i] = sum(total_rewards[i])\n",
        "\n",
        "    return max_nums, nums, mean_rewards, total_rewards\n",
        "\n",
        "\n",
        "def battle(env, n_round, map_size, max_steps, handles, models, print_every, eps=1.0, render=False, train=False):\n",
        "    \"\"\"play a ground and train\"\"\"\n",
        "    env.reset()\n",
        "    generate_map(env, map_size, handles)\n",
        "\n",
        "    step_ct = 0\n",
        "    done = False\n",
        "\n",
        "    n_group = len(handles)\n",
        "    state = [None for _ in range(n_group)]\n",
        "    acts = [None for _ in range(n_group)]\n",
        "    ids = [None for _ in range(n_group)]\n",
        "\n",
        "    alives = [None for _ in range(n_group)]\n",
        "    rewards = [None for _ in range(n_group)]\n",
        "    nums = [env.get_num(handle) for handle in handles]\n",
        "    max_nums = nums.copy()\n",
        "\n",
        "    n_action = [env.get_action_space(handles[0])[0], env.get_action_space(handles[1])[0]]\n",
        "\n",
        "    print(\"\\n\\n[*] ROUND #{0}, EPS: {1:.2f} NUMBER: {2}\".format(n_round, eps, nums))\n",
        "    mean_rewards = [[] for _ in range(n_group)]\n",
        "    total_rewards = [[] for _ in range(n_group)]\n",
        "\n",
        "    former_act_prob = [np.zeros((1, env.get_action_space(handles[0])[0])), np.zeros((1, env.get_action_space(handles[1])[0]))]\n",
        "\n",
        "    while not done and step_ct < max_steps:\n",
        "        # take actions for every model\n",
        "        for i in range(n_group):\n",
        "            state[i] = list(env.get_observation(handles[i]))\n",
        "            ids[i] = env.get_agent_id(handles[i])\n",
        "\n",
        "        for i in range(n_group):\n",
        "            former_act_prob[i] = np.tile(former_act_prob[i], (len(state[i][0]), 1))\n",
        "            acts[i] = models[i].act(state=state[i], prob=former_act_prob[i], eps=eps)\n",
        "\n",
        "        for i in range(n_group):\n",
        "            env.set_action(handles[i], acts[i])\n",
        "\n",
        "        # simulate one step\n",
        "        done = env.step()\n",
        "\n",
        "        for i in range(n_group):\n",
        "            rewards[i] = env.get_reward(handles[i])\n",
        "            alives[i] = env.get_alive(handles[i])\n",
        "\n",
        "        for i in range(n_group):\n",
        "            former_act_prob[i] = np.mean(list(map(lambda x: np.eye(n_action[i])[x], acts[i])), axis=0, keepdims=True)\n",
        "\n",
        "        # stat info\n",
        "        nums = [env.get_num(handle) for handle in handles]\n",
        "\n",
        "        for i in range(n_group):\n",
        "            sum_reward = sum(rewards[i])\n",
        "            rewards[i] = sum_reward / nums[i]\n",
        "            mean_rewards[i].append(rewards[i])\n",
        "            total_rewards[i].append(sum_reward)\n",
        "\n",
        "        if render:\n",
        "            env.render()\n",
        "\n",
        "        # clear dead agents\n",
        "        env.clear_dead()\n",
        "\n",
        "        info = {\"Ave-Reward\": np.round(rewards, decimals=6), \"NUM\": nums}\n",
        "\n",
        "        step_ct += 1\n",
        "\n",
        "        if step_ct % print_every == 0:\n",
        "            print(\"> step #{}, info: {}\".format(step_ct, info))\n",
        "\n",
        "    for i in range(n_group):\n",
        "        mean_rewards[i] = sum(mean_rewards[i]) / len(mean_rewards[i])\n",
        "        total_rewards[i] = sum(total_rewards[i])\n",
        "\n",
        "    return max_nums, nums, mean_rewards, total_rewards"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFROjN4w3QMe"
      },
      "source": [
        "!mkdir /content/examples\n",
        "!mkdir /content/examples/battle_model\n",
        "!mkdir /content/examples/battle_model/build\n",
        "!mkdir /content/examples/battle_model/build/render"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v5shBws3nm_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e16f1e58-e921-40de-a271-b093881d0ff2"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "#import tensorflow as tf\n",
        "import numpy as np\n",
        "import magent\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "tf.reset_default_graph()\n",
        "\n",
        "BASE_DIR = '/content'\n",
        "\n",
        "def linear_decay(epoch, x, y):\n",
        "    min_v, max_v = y[0], y[-1]\n",
        "    start, end = x[0], x[-1]\n",
        "\n",
        "    if epoch == start:\n",
        "        return min_v\n",
        "\n",
        "    eps = min_v\n",
        "\n",
        "    for i, x_i in enumerate(x):\n",
        "        if epoch <= x_i:\n",
        "            interval = (y[i] - y[i - 1]) / (x_i - x[i - 1])\n",
        "            eps = interval * (epoch - x[i - 1]) + y[i - 1]\n",
        "            break\n",
        "\n",
        "    return eps\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    algo = 'mfq'\n",
        "    save_every = 10\n",
        "    update_every = 5\n",
        "    n_round = 500\n",
        "    map_size = 40\n",
        "    max_steps = 400\n",
        "    render = False\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize the environment\n",
        "    env = magent.GridWorld('battle', map_size=map_size)\n",
        "    env.set_render_dir(os.path.join(BASE_DIR, 'examples/battle_model', 'build/render'))\n",
        "    handles = env.get_handles()\n",
        "\n",
        "    tf_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
        "    tf_config.gpu_options.allow_growth = True\n",
        "\n",
        "    log_dir = os.path.join(BASE_DIR,'data/tmp'.format(algo))\n",
        "    model_dir = os.path.join(BASE_DIR, 'data/models/{}'.format(algo))\n",
        "\n",
        "    if algo in ['mfq', 'mfac']:\n",
        "        use_mf = True\n",
        "    else:\n",
        "        use_mf = False\n",
        "\n",
        "    start_from = 0\n",
        "\n",
        "    sess = tf.Session(config=tf_config)\n",
        "    models = [spawn_ai(algo, sess, env, handles[0], algo + '-me', max_steps), spawn_ai(algo, sess, env, handles[1], algo + '-opponent', max_steps)]\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    runner = Runner(sess, env, handles, map_size, max_steps, models, play,\n",
        "                            render_every=save_every if render else 0, save_every=save_every, tau=0.01, log_name=algo,\n",
        "                            log_dir=log_dir, model_dir=model_dir, train=True)\n",
        "\n",
        "    for k in range(start_from, start_from + n_round):\n",
        "        eps = linear_decay(k, [0, int(n_round * 0.8), n_round], [1, 0.2, 0.1])\n",
        "        runner.run(eps, k)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 33]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 33]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 33]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 33]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3827\n",
            "[*] LOSS: 0.18896228 / Q: {'Eval-Q': 1.779265, 'Target-Q': 1.833235}\n",
            "[*] LOSS: 0.08498861 / Q: {'Eval-Q': 1.761196, 'Target-Q': 1.657127}\n",
            "[*] LOSS: 0.15613467 / Q: {'Eval-Q': 1.410274, 'Target-Q': 1.332624}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.003669837357013158, 'total_reward': 138.46499954909086, 'kill': 31}\n",
            "\n",
            "\n",
            "[*] ROUND #53, EPS: 0.97 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.010385, 0.006765]), 'NUM': [26, 34]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 11]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2911\n",
            "[*] LOSS: 0.26998398 / Q: {'Eval-Q': 2.79432, 'Target-Q': 2.61527}\n",
            "[*] LOSS: 0.2502508 / Q: {'Eval-Q': 1.482199, 'Target-Q': 1.386993}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.13383204259235587, 'total_reward': 250.46499881148338, 'kill': 53}\n",
            "\n",
            "\n",
            "[*] ROUND #54, EPS: 0.97 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.199375, 0.17375 ]), 'NUM': [24, 28]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005,  0.015]), 'NUM': [4, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 4]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 4]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 4]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 4]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 4]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 4]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3249\n",
            "[*] LOSS: 0.13221204 / Q: {'Eval-Q': 1.517668, 'Target-Q': 1.495176}\n",
            "[*] LOSS: 0.43134657 / Q: {'Eval-Q': 1.911714, 'Target-Q': 1.786055}\n",
            "[*] LOSS: 0.20533517 / Q: {'Eval-Q': 1.985202, 'Target-Q': 1.854782}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03920255844005407, 'total_reward': 285.6699985610321, 'kill': 60}\n",
            "\n",
            "\n",
            "[*] ROUND #55, EPS: 0.97 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.411667, -0.008393]), 'NUM': [12, 28]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2155\n",
            "[*] LOSS: 0.118382305 / Q: {'Eval-Q': 2.515889, 'Target-Q': 2.396734}\n",
            "[*] LOSS: 0.17893398 / Q: {'Eval-Q': 2.339888, 'Target-Q': 2.304942}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.140027525560895, 'total_reward': 212.94499893020838, 'kill': 43}\n",
            "\n",
            "\n",
            "[*] ROUND #56, EPS: 0.97 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.004868,  0.180185]), 'NUM': [38, 27]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005   ,  0.028333]), 'NUM': [10, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4152\n",
            "[*] LOSS: 0.12211058 / Q: {'Eval-Q': 1.941328, 'Target-Q': 1.913874}\n",
            "[*] LOSS: 0.21359289 / Q: {'Eval-Q': 2.621417, 'Target-Q': 2.350928}\n",
            "[*] LOSS: 0.1719678 / Q: {'Eval-Q': 2.176914, 'Target-Q': 2.018868}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07432789687759794, 'total_reward': 291.60999810975045, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_56\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_56\n",
            "\n",
            "\n",
            "[*] ROUND #57, EPS: 0.97 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.028611, 0.158636]), 'NUM': [18, 33]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2340\n",
            "[*] LOSS: 0.18143111 / Q: {'Eval-Q': 2.046094, 'Target-Q': 2.003913}\n",
            "[*] LOSS: 0.35635018 / Q: {'Eval-Q': 2.370574, 'Target-Q': 1.978474}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.20350618530634232, 'total_reward': 226.21999892778695, 'kill': 46}\n",
            "\n",
            "\n",
            "[*] ROUND #58, EPS: 0.97 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.607812, 0.1914  ]), 'NUM': [16, 25]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2254\n",
            "[*] LOSS: 0.22729033 / Q: {'Eval-Q': 2.274487, 'Target-Q': 2.141687}\n",
            "[*] LOSS: 0.2329395 / Q: {'Eval-Q': 2.988498, 'Target-Q': 2.714578}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1613341964256162, 'total_reward': 215.34999867435545, 'kill': 50}\n",
            "\n",
            "\n",
            "[*] ROUND #59, EPS: 0.97 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.3075  , -0.004808]), 'NUM': [16, 26]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 17]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 17]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2355\n",
            "[*] LOSS: 0.37961534 / Q: {'Eval-Q': 2.974578, 'Target-Q': 2.656888}\n",
            "[*] LOSS: 0.28849265 / Q: {'Eval-Q': 2.624545, 'Target-Q': 2.53825}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09317454379861456, 'total_reward': 221.54499888326973, 'kill': 47}\n",
            "\n",
            "\n",
            "[*] ROUND #60, EPS: 0.97 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.273333, 0.336552]), 'NUM': [18, 29]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 8]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 8]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 8]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 8]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 8]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 8]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 8]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2929\n",
            "[*] LOSS: 0.19158874 / Q: {'Eval-Q': 2.71306, 'Target-Q': 2.568586}\n",
            "[*] LOSS: 0.19346896 / Q: {'Eval-Q': 2.638782, 'Target-Q': 2.53926}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.042320517551768244, 'total_reward': 268.7699986938387, 'kill': 56}\n",
            "\n",
            "\n",
            "[*] ROUND #61, EPS: 0.97 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.121829, 0.018462]), 'NUM': [41, 13]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3292\n",
            "[*] LOSS: 0.2724793 / Q: {'Eval-Q': 2.910948, 'Target-Q': 2.760854}\n",
            "[*] LOSS: 0.42989618 / Q: {'Eval-Q': 2.392439, 'Target-Q': 2.52125}\n",
            "[*] LOSS: 0.18106443 / Q: {'Eval-Q': 2.719068, 'Target-Q': 2.703183}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.0990336289574603, 'total_reward': 322.7849985053763, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_61\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_61\n",
            "\n",
            "\n",
            "[*] ROUND #62, EPS: 0.97 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.012857, 0.021087]), 'NUM': [28, 23]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 18]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 18]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 18]}\n",
            "> step #250, info: {'Ave-Reward': array([0.023571, 0.000556]), 'NUM': [7, 18]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 13]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 13]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 13]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4588\n",
            "[*] LOSS: 0.2672914 / Q: {'Eval-Q': 2.654909, 'Target-Q': 2.460083}\n",
            "[*] LOSS: 0.15542398 / Q: {'Eval-Q': 2.234082, 'Target-Q': 2.170097}\n",
            "[*] LOSS: 0.19131908 / Q: {'Eval-Q': 2.15172, 'Target-Q': 2.274345}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.019497914709384827, 'total_reward': 227.56999893672764, 'kill': 51}\n",
            "\n",
            "\n",
            "[*] ROUND #63, EPS: 0.97 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.162742, 0.014062]), 'NUM': [31, 16]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3058\n",
            "[*] LOSS: 0.42419204 / Q: {'Eval-Q': 2.948312, 'Target-Q': 2.873657}\n",
            "[*] LOSS: 0.1940862 / Q: {'Eval-Q': 2.580248, 'Target-Q': 2.496723}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1031641561688812, 'total_reward': 303.1199985239655, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_63\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_63\n",
            "\n",
            "\n",
            "[*] ROUND #64, EPS: 0.97 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.531842, 0.01    ]), 'NUM': [19, 14]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005,  0.095]), 'NUM': [8, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5130\n",
            "[*] LOSS: 0.17490073 / Q: {'Eval-Q': 2.734374, 'Target-Q': 2.526011}\n",
            "[*] LOSS: 0.19069088 / Q: {'Eval-Q': 2.984257, 'Target-Q': 2.722518}\n",
            "[*] LOSS: 0.22940609 / Q: {'Eval-Q': 2.246126, 'Target-Q': 2.336244}\n",
            "[*] LOSS: 0.26986557 / Q: {'Eval-Q': 2.480532, 'Target-Q': 2.299697}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02287124227595947, 'total_reward': 277.7299988390878, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_64\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_64\n",
            "\n",
            "\n",
            "[*] ROUND #65, EPS: 0.97 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.191154, 0.004545]), 'NUM': [26, 11]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [24, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.000833, -0.005   ]), 'NUM': [24, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [24, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [24, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [24, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [24, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [24, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 10654\n",
            "[*] LOSS: 0.32758552 / Q: {'Eval-Q': 2.289075, 'Target-Q': 2.20764}\n",
            "[*] LOSS: 0.1551316 / Q: {'Eval-Q': 2.474716, 'Target-Q': 2.477188}\n",
            "[*] LOSS: 0.17053775 / Q: {'Eval-Q': 2.079773, 'Target-Q': 2.09979}\n",
            "[*] LOSS: 0.7998003 / Q: {'Eval-Q': 2.622039, 'Target-Q': 2.4771}\n",
            "[*] LOSS: 0.37053105 / Q: {'Eval-Q': 3.127817, 'Target-Q': 2.976419}\n",
            "[*] LOSS: 0.15961638 / Q: {'Eval-Q': 1.990229, 'Target-Q': 1.993304}\n",
            "[*] LOSS: 0.16140842 / Q: {'Eval-Q': 2.43853, 'Target-Q': 2.372956}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01725380639674086, 'total_reward': 263.7299995813519, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_65\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_65\n",
            "\n",
            "\n",
            "[*] ROUND #66, EPS: 0.97 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.016786, 0.237857]), 'NUM': [14, 21]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 19]}\n",
            "> step #150, info: {'Ave-Reward': array([ 0.02 , -0.005]), 'NUM': [4, 19]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 19]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2849\n",
            "[*] LOSS: 0.09666597 / Q: {'Eval-Q': 2.526737, 'Target-Q': 2.387265}\n",
            "[*] LOSS: 0.21322256 / Q: {'Eval-Q': 2.718892, 'Target-Q': 2.482045}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01726833126216171, 'total_reward': 229.76999905332923, 'kill': 45}\n",
            "\n",
            "\n",
            "[*] ROUND #67, EPS: 0.97 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 4]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 4]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 9845\n",
            "[*] LOSS: 0.35756174 / Q: {'Eval-Q': 2.761336, 'Target-Q': 2.392816}\n",
            "[*] LOSS: 0.18867767 / Q: {'Eval-Q': 2.423849, 'Target-Q': 2.246817}\n",
            "[*] LOSS: 0.44025624 / Q: {'Eval-Q': 2.347307, 'Target-Q': 2.369243}\n",
            "[*] LOSS: 0.5588947 / Q: {'Eval-Q': 2.396422, 'Target-Q': 2.35037}\n",
            "[*] LOSS: 0.10811521 / Q: {'Eval-Q': 2.395063, 'Target-Q': 1.99083}\n",
            "[*] LOSS: 0.15302745 / Q: {'Eval-Q': 3.001674, 'Target-Q': 2.772645}\n",
            "[*] LOSS: 0.20294802 / Q: {'Eval-Q': 2.436824, 'Target-Q': 2.058336}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.0171938633672836, 'total_reward': 278.0849994905293, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_67\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_67\n",
            "\n",
            "\n",
            "[*] ROUND #68, EPS: 0.97 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.004667,  0.811667]), 'NUM': [15, 6]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6532\n",
            "[*] LOSS: 0.3302091 / Q: {'Eval-Q': 2.603334, 'Target-Q': 2.542764}\n",
            "[*] LOSS: 0.67584455 / Q: {'Eval-Q': 2.3512, 'Target-Q': 2.136794}\n",
            "[*] LOSS: 0.27004796 / Q: {'Eval-Q': 2.78587, 'Target-Q': 2.863893}\n",
            "[*] LOSS: 0.13268107 / Q: {'Eval-Q': 2.555579, 'Target-Q': 2.457883}\n",
            "[*] LOSS: 0.37063232 / Q: {'Eval-Q': 2.820403, 'Target-Q': 2.901856}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02084914528691539, 'total_reward': 284.09499910939485, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_68\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_68\n",
            "\n",
            "\n",
            "[*] ROUND #69, EPS: 0.97 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 12]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 9227\n",
            "[*] LOSS: 0.20060872 / Q: {'Eval-Q': 2.270182, 'Target-Q': 2.082332}\n",
            "[*] LOSS: 0.26706776 / Q: {'Eval-Q': 2.668376, 'Target-Q': 2.582182}\n",
            "[*] LOSS: 0.10643618 / Q: {'Eval-Q': 2.330444, 'Target-Q': 2.236715}\n",
            "[*] LOSS: 0.22779685 / Q: {'Eval-Q': 2.577159, 'Target-Q': 2.671426}\n",
            "[*] LOSS: 0.14401466 / Q: {'Eval-Q': 2.135663, 'Target-Q': 2.231674}\n",
            "[*] LOSS: 0.26474354 / Q: {'Eval-Q': 2.424446, 'Target-Q': 2.389584}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01962180992066702, 'total_reward': 265.78499942086637, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_69\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_69\n",
            "\n",
            "\n",
            "[*] ROUND #70, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.000238, -0.005   ]), 'NUM': [21, 10]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005   ,  0.011667]), 'NUM': [19, 6]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.011333,  0.995   ]), 'NUM': [15, 5]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 5]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 5]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 5]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 5]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 5]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6561\n",
            "[*] LOSS: 0.17397569 / Q: {'Eval-Q': 1.916235, 'Target-Q': 1.763643}\n",
            "[*] LOSS: 0.23117 / Q: {'Eval-Q': 1.913976, 'Target-Q': 1.749816}\n",
            "[*] LOSS: 0.31016117 / Q: {'Eval-Q': 2.335232, 'Target-Q': 2.216679}\n",
            "[*] LOSS: 0.11983511 / Q: {'Eval-Q': 1.741028, 'Target-Q': 1.61366}\n",
            "[*] LOSS: 0.38109082 / Q: {'Eval-Q': 2.092607, 'Target-Q': 1.923719}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.017205035853947937, 'total_reward': 243.3649991080165, 'kill': 59}\n",
            "\n",
            "\n",
            "[*] ROUND #71, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.001667, 0.001667]), 'NUM': [15, 30]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005   , -0.001296]), 'NUM': [9, 27]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 25]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 25]}\n",
            "> step #250, info: {'Ave-Reward': array([ 1.628333, -0.008958]), 'NUM': [3, 24]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005,  0.   ]), 'NUM': [2, 20]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 20]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 20]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3501\n",
            "[*] LOSS: 0.16040692 / Q: {'Eval-Q': 2.058042, 'Target-Q': 2.068072}\n",
            "[*] LOSS: 0.10506023 / Q: {'Eval-Q': 2.637354, 'Target-Q': 2.575336}\n",
            "[*] LOSS: 0.24351417 / Q: {'Eval-Q': 1.871905, 'Target-Q': 1.833564}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03492785889158721, 'total_reward': 179.7099991608411, 'kill': 44}\n",
            "\n",
            "\n",
            "[*] ROUND #72, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.828333, 0.011053]), 'NUM': [6, 19]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 18]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1997\n",
            "[*] LOSS: 0.32261503 / Q: {'Eval-Q': 2.425458, 'Target-Q': 2.278714}\n",
            "[*] LOSS: 0.19704106 / Q: {'Eval-Q': 2.751298, 'Target-Q': 2.776656}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.06953297095327932, 'total_reward': 214.73499900009483, 'kill': 46}\n",
            "\n",
            "\n",
            "[*] ROUND #73, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.004444,  0.199   ]), 'NUM': [9, 25]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "> step #200, info: {'Ave-Reward': array([ 0.095, -0.005]), 'NUM': [1, 21]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2451\n",
            "[*] LOSS: 0.14433658 / Q: {'Eval-Q': 2.261943, 'Target-Q': 2.129668}\n",
            "[*] LOSS: 0.2090081 / Q: {'Eval-Q': 2.419953, 'Target-Q': 2.365996}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02118177732894265, 'total_reward': 203.5599990412593, 'kill': 43}\n",
            "\n",
            "\n",
            "[*] ROUND #74, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 3]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7865\n",
            "[*] LOSS: 0.31287557 / Q: {'Eval-Q': 2.190564, 'Target-Q': 2.391311}\n",
            "[*] LOSS: 0.63236016 / Q: {'Eval-Q': 2.631242, 'Target-Q': 2.701503}\n",
            "[*] LOSS: 0.18174201 / Q: {'Eval-Q': 2.161895, 'Target-Q': 2.056525}\n",
            "[*] LOSS: 0.44155458 / Q: {'Eval-Q': 1.945977, 'Target-Q': 1.655161}\n",
            "[*] LOSS: 0.51138496 / Q: {'Eval-Q': 2.466589, 'Target-Q': 2.285709}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01933851390310911, 'total_reward': 290.8199991621077, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_74\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_74\n",
            "\n",
            "\n",
            "[*] ROUND #75, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([1.27    , 0.010769]), 'NUM': [4, 13]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 10]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1882\n",
            "[*] LOSS: 0.2615606 / Q: {'Eval-Q': 2.419549, 'Target-Q': 2.444169}\n",
            "[*] LOSS: 0.9433788 / Q: {'Eval-Q': 2.11115, 'Target-Q': 2.172148}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08020606669694995, 'total_reward': 253.00999881420285, 'kill': 54}\n",
            "\n",
            "\n",
            "[*] ROUND #76, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.005   , 1.409286]), 'NUM': [11, 7]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4636\n",
            "[*] LOSS: 0.058603726 / Q: {'Eval-Q': 1.83108, 'Target-Q': 1.78946}\n",
            "[*] LOSS: 0.30256674 / Q: {'Eval-Q': 1.554865, 'Target-Q': 1.515419}\n",
            "[*] LOSS: 0.26531798 / Q: {'Eval-Q': 2.729672, 'Target-Q': 2.482343}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.029152666229708565, 'total_reward': 297.7999989176169, 'kill': 62}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_76\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_76\n",
            "\n",
            "\n",
            "[*] ROUND #77, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.0255  , 0.449545]), 'NUM': [10, 11]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 6]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 4]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 4]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 4]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 4]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 4]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 4]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2629\n",
            "[*] LOSS: 0.17268737 / Q: {'Eval-Q': 2.305396, 'Target-Q': 2.306815}\n",
            "[*] LOSS: 0.27170655 / Q: {'Eval-Q': 2.212865, 'Target-Q': 2.196015}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.044254348163420935, 'total_reward': 269.264998357743, 'kill': 60}\n",
            "\n",
            "\n",
            "[*] ROUND #78, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.001061, -0.005   ]), 'NUM': [33, 5]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [32, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [31, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [31, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [31, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [31, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [31, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [31, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 13281\n",
            "[*] LOSS: 0.12147999 / Q: {'Eval-Q': 1.961577, 'Target-Q': 1.782467}\n",
            "[*] LOSS: 0.07940487 / Q: {'Eval-Q': 1.810386, 'Target-Q': 1.554047}\n",
            "[*] LOSS: 0.15744533 / Q: {'Eval-Q': 2.005424, 'Target-Q': 1.7751}\n",
            "[*] LOSS: 0.19513555 / Q: {'Eval-Q': 2.417044, 'Target-Q': 2.218756}\n",
            "[*] LOSS: 0.33554533 / Q: {'Eval-Q': 2.022495, 'Target-Q': 1.787794}\n",
            "[*] LOSS: 0.3071781 / Q: {'Eval-Q': 1.860673, 'Target-Q': 1.846473}\n",
            "[*] LOSS: 0.21021622 / Q: {'Eval-Q': 2.120116, 'Target-Q': 2.008386}\n",
            "[*] LOSS: 0.17395103 / Q: {'Eval-Q': 1.693234, 'Target-Q': 1.537648}\n",
            "[*] LOSS: 0.14574145 / Q: {'Eval-Q': 1.834304, 'Target-Q': 1.834288}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.014129223921992482, 'total_reward': 259.95999977272004, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_78\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_78\n",
            "\n",
            "\n",
            "[*] ROUND #79, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.379808, 0.00875 ]), 'NUM': [26, 8]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 10241\n",
            "[*] LOSS: 0.31063056 / Q: {'Eval-Q': 1.984002, 'Target-Q': 2.156688}\n",
            "[*] LOSS: 0.08813098 / Q: {'Eval-Q': 1.875728, 'Target-Q': 1.935386}\n",
            "[*] LOSS: 0.35693207 / Q: {'Eval-Q': 2.335258, 'Target-Q': 2.218071}\n",
            "[*] LOSS: 0.30226946 / Q: {'Eval-Q': 1.583059, 'Target-Q': 1.656066}\n",
            "[*] LOSS: 0.10487582 / Q: {'Eval-Q': 1.699383, 'Target-Q': 1.601627}\n",
            "[*] LOSS: 0.32192206 / Q: {'Eval-Q': 1.474597, 'Target-Q': 1.584106}\n",
            "[*] LOSS: 0.63922626 / Q: {'Eval-Q': 2.05076, 'Target-Q': 2.141902}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01726660715818917, 'total_reward': 274.29999944381416, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_79\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_79\n",
            "\n",
            "\n",
            "[*] ROUND #80, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([1.975 , 0.0055]), 'NUM': [5, 20]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 17]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 17]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 17]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 17]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2077\n",
            "[*] LOSS: 0.2510525 / Q: {'Eval-Q': 2.52354, 'Target-Q': 2.58187}\n",
            "[*] LOSS: 0.17909837 / Q: {'Eval-Q': 1.743096, 'Target-Q': 1.700846}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.026957362786378836, 'total_reward': 201.33499908912927, 'kill': 47}\n",
            "\n",
            "\n",
            "[*] ROUND #81, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.011667, 0.009286]), 'NUM': [18, 7]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7657\n",
            "[*] LOSS: 0.355189 / Q: {'Eval-Q': 1.968162, 'Target-Q': 1.88003}\n",
            "[*] LOSS: 0.097414054 / Q: {'Eval-Q': 2.366262, 'Target-Q': 2.309296}\n",
            "[*] LOSS: 0.18195425 / Q: {'Eval-Q': 1.766733, 'Target-Q': 1.712106}\n",
            "[*] LOSS: 0.40550792 / Q: {'Eval-Q': 1.922935, 'Target-Q': 2.083957}\n",
            "[*] LOSS: 0.20012441 / Q: {'Eval-Q': 1.416112, 'Target-Q': 1.301153}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.020693358656247482, 'total_reward': 281.95499918144196, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_81\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_81\n",
            "\n",
            "\n",
            "[*] ROUND #82, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.02875 ,  0.258158]), 'NUM': [4, 19]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2160\n",
            "[*] LOSS: 0.18915042 / Q: {'Eval-Q': 1.95452, 'Target-Q': 1.964045}\n",
            "[*] LOSS: 0.14400259 / Q: {'Eval-Q': 2.080278, 'Target-Q': 2.018966}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.012854379119525987, 'total_reward': 203.61499909404665, 'kill': 45}\n",
            "\n",
            "\n",
            "[*] ROUND #83, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.001774, -0.005   ]), 'NUM': [31, 10]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [30, 6]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.001667, -0.005   ]), 'NUM': [30, 4]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.001429,  0.028333]), 'NUM': [28, 3]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [27, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.001,  0.045]), 'NUM': [25, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [24, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 11898\n",
            "[*] LOSS: 0.5329747 / Q: {'Eval-Q': 1.833088, 'Target-Q': 1.701245}\n",
            "[*] LOSS: 0.36285555 / Q: {'Eval-Q': 1.504297, 'Target-Q': 1.50091}\n",
            "[*] LOSS: 0.119979076 / Q: {'Eval-Q': 1.918452, 'Target-Q': 1.488011}\n",
            "[*] LOSS: 0.21066487 / Q: {'Eval-Q': 2.236316, 'Target-Q': 2.167724}\n",
            "[*] LOSS: 0.21341327 / Q: {'Eval-Q': 2.07921, 'Target-Q': 2.10469}\n",
            "[*] LOSS: 0.3671795 / Q: {'Eval-Q': 2.070254, 'Target-Q': 2.190124}\n",
            "[*] LOSS: 0.17315477 / Q: {'Eval-Q': 1.757614, 'Target-Q': 1.867938}\n",
            "[*] LOSS: 0.06932408 / Q: {'Eval-Q': 1.697263, 'Target-Q': 1.526343}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.015655677790328816, 'total_reward': 268.6099994806573, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_83\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_83\n",
            "\n",
            "\n",
            "[*] ROUND #84, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [48, 27]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.205638, -0.019615]), 'NUM': [47, 13]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [46, 4]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [46, 3]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [46, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 14210\n",
            "[*] LOSS: 0.19513893 / Q: {'Eval-Q': 1.809508, 'Target-Q': 1.76736}\n",
            "[*] LOSS: 0.16480455 / Q: {'Eval-Q': 1.960288, 'Target-Q': 1.954355}\n",
            "[*] LOSS: 0.4028574 / Q: {'Eval-Q': 1.539828, 'Target-Q': 1.660251}\n",
            "[*] LOSS: 0.3609907 / Q: {'Eval-Q': 1.541223, 'Target-Q': 1.583662}\n",
            "[*] LOSS: 0.37733513 / Q: {'Eval-Q': 1.652244, 'Target-Q': 1.754251}\n",
            "[*] LOSS: 0.31478542 / Q: {'Eval-Q': 1.952851, 'Target-Q': 1.900391}\n",
            "[*] LOSS: 0.20606838 / Q: {'Eval-Q': 2.032027, 'Target-Q': 1.883829}\n",
            "[*] LOSS: 0.35635096 / Q: {'Eval-Q': 1.747037, 'Target-Q': 1.736556}\n",
            "[*] LOSS: 0.2684312 / Q: {'Eval-Q': 1.947399, 'Target-Q': 1.776355}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.017568493489898782, 'total_reward': 265.43999980948865, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_84\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_84\n",
            "\n",
            "\n",
            "[*] ROUND #85, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005   , -0.000652]), 'NUM': [4, 23]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1948\n",
            "[*] LOSS: 0.43694493 / Q: {'Eval-Q': 2.065299, 'Target-Q': 2.18344}\n",
            "[*] LOSS: 0.24069187 / Q: {'Eval-Q': 2.845489, 'Target-Q': 2.672451}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09863000610499958, 'total_reward': 204.7799990940839, 'kill': 43}\n",
            "\n",
            "\n",
            "[*] ROUND #86, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.004375, 0.009286]), 'NUM': [32, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2862\n",
            "[*] LOSS: 0.14838338 / Q: {'Eval-Q': 1.677439, 'Target-Q': 1.67702}\n",
            "[*] LOSS: 0.10517228 / Q: {'Eval-Q': 2.353825, 'Target-Q': 2.204704}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11581095656534325, 'total_reward': 313.04999842494726, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_86\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_86\n",
            "\n",
            "\n",
            "[*] ROUND #87, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.003333,  0.167414]), 'NUM': [3, 29]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1897\n",
            "[*] LOSS: 0.26233283 / Q: {'Eval-Q': 2.162677, 'Target-Q': 2.300273}\n",
            "[*] LOSS: 0.11060464 / Q: {'Eval-Q': 1.874914, 'Target-Q': 1.643485}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12574000371607813, 'total_reward': 156.23499928880483, 'kill': 37}\n",
            "\n",
            "\n",
            "[*] ROUND #88, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.0125  , 0.300882]), 'NUM': [6, 17]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 16]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 16]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1979\n",
            "[*] LOSS: 0.11180071 / Q: {'Eval-Q': 1.605502, 'Target-Q': 1.659423}\n",
            "[*] LOSS: 0.42157996 / Q: {'Eval-Q': 1.674209, 'Target-Q': 1.745434}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.05899100588109012, 'total_reward': 230.32499894686043, 'kill': 48}\n",
            "\n",
            "\n",
            "[*] ROUND #89, EPS: 0.96 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [41, 26]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [39, 21]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [38, 19]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.002297, -0.005   ]), 'NUM': [37, 17]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [37, 16]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [37, 16]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [37, 16]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [37, 16]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 15662\n",
            "[*] LOSS: 0.10825834 / Q: {'Eval-Q': 1.67384, 'Target-Q': 1.665332}\n",
            "[*] LOSS: 0.13957518 / Q: {'Eval-Q': 1.728114, 'Target-Q': 1.790068}\n",
            "[*] LOSS: 0.17492591 / Q: {'Eval-Q': 2.177166, 'Target-Q': 2.017303}\n",
            "[*] LOSS: 0.15125738 / Q: {'Eval-Q': 1.451294, 'Target-Q': 1.460957}\n",
            "[*] LOSS: 0.15425499 / Q: {'Eval-Q': 1.793515, 'Target-Q': 1.609199}\n",
            "[*] LOSS: 0.1046576 / Q: {'Eval-Q': 1.68019, 'Target-Q': 1.621958}\n",
            "[*] LOSS: 0.21628708 / Q: {'Eval-Q': 1.640267, 'Target-Q': 1.747}\n",
            "[*] LOSS: 0.10243799 / Q: {'Eval-Q': 1.51823, 'Target-Q': 1.451219}\n",
            "[*] LOSS: 0.15566716 / Q: {'Eval-Q': 1.616344, 'Target-Q': 1.619595}\n",
            "[*] LOSS: 0.15295498 / Q: {'Eval-Q': 1.926378, 'Target-Q': 1.80718}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.007987860399966192, 'total_reward': 159.1250002933666, 'kill': 48}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_89\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_89\n",
            "\n",
            "\n",
            "[*] ROUND #90, EPS: 0.95 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.018462, 0.5055  ]), 'NUM': [13, 10]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4647\n",
            "[*] LOSS: 0.15669969 / Q: {'Eval-Q': 2.178612, 'Target-Q': 2.159578}\n",
            "[*] LOSS: 0.17322564 / Q: {'Eval-Q': 2.227364, 'Target-Q': 2.279036}\n",
            "[*] LOSS: 0.08183047 / Q: {'Eval-Q': 1.632999, 'Target-Q': 1.684243}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.023356645707509883, 'total_reward': 282.74999894946814, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_90\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_90\n",
            "\n",
            "\n",
            "[*] ROUND #91, EPS: 0.95 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.014091, 0.383462]), 'NUM': [11, 26]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 23]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2199\n",
            "[*] LOSS: 0.7898359 / Q: {'Eval-Q': 2.402325, 'Target-Q': 2.501373}\n",
            "[*] LOSS: 0.2282481 / Q: {'Eval-Q': 2.397524, 'Target-Q': 2.13268}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.05246872662830523, 'total_reward': 186.22499911859632, 'kill': 41}\n",
            "\n",
            "\n",
            "[*] ROUND #92, EPS: 0.95 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.00375 ,  0.280714]), 'NUM': [8, 35]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005   , -0.002059]), 'NUM': [1, 34]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2080\n",
            "[*] LOSS: 0.102703065 / Q: {'Eval-Q': 2.042339, 'Target-Q': 1.645971}\n",
            "[*] LOSS: 0.30064493 / Q: {'Eval-Q': 2.092366, 'Target-Q': 2.163539}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.05288006336315716, 'total_reward': 121.41999932099134, 'kill': 30}\n",
            "\n",
            "\n",
            "[*] ROUND #93, EPS: 0.95 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.011667, 0.022273]), 'NUM': [24, 11]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7632\n",
            "[*] LOSS: 0.22836503 / Q: {'Eval-Q': 2.235502, 'Target-Q': 2.168379}\n",
            "[*] LOSS: 0.15048793 / Q: {'Eval-Q': 1.636058, 'Target-Q': 1.652257}\n",
            "[*] LOSS: 0.26792607 / Q: {'Eval-Q': 2.279894, 'Target-Q': 2.139607}\n",
            "[*] LOSS: 0.18257791 / Q: {'Eval-Q': 2.472894, 'Target-Q': 2.074669}\n",
            "[*] LOSS: 0.1985167 / Q: {'Eval-Q': 2.444544, 'Target-Q': 2.459121}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.019405783315448705, 'total_reward': 277.8849989846349, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_93\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_93\n",
            "\n",
            "\n",
            "[*] ROUND #94, EPS: 0.95 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.095   , -0.002368]), 'NUM': [2, 38]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1812\n",
            "[*] LOSS: 0.091807544 / Q: {'Eval-Q': 2.14328, 'Target-Q': 2.107956}\n",
            "[*] LOSS: 0.43727505 / Q: {'Eval-Q': 1.730536, 'Target-Q': 1.776029}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12968213674533227, 'total_reward': 129.25999944657087, 'kill': 27}\n",
            "\n",
            "\n",
            "[*] ROUND #95, EPS: 0.95 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.001667, -0.005   ]), 'NUM': [30, 5]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [29, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5182\n",
            "[*] LOSS: 0.14540328 / Q: {'Eval-Q': 2.675204, 'Target-Q': 2.450626}\n",
            "[*] LOSS: 0.22810599 / Q: {'Eval-Q': 1.985254, 'Target-Q': 2.117205}\n",
            "[*] LOSS: 0.44689497 / Q: {'Eval-Q': 2.254592, 'Target-Q': 2.377313}\n",
            "[*] LOSS: 0.17930816 / Q: {'Eval-Q': 1.838766, 'Target-Q': 1.844566}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.04986221433515561, 'total_reward': 298.76499856542796, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_95\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_95\n",
            "\n",
            "\n",
            "[*] ROUND #96, EPS: 0.95 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.004286,  0.15629 ]), 'NUM': [7, 31]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 29]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 29]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2067\n",
            "[*] LOSS: 0.14510596 / Q: {'Eval-Q': 1.594025, 'Target-Q': 1.663021}\n",
            "[*] LOSS: 0.24904522 / Q: {'Eval-Q': 2.152304, 'Target-Q': 2.104261}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.056818777080531103, 'total_reward': 154.7849992820993, 'kill': 35}\n",
            "\n",
            "\n",
            "[*] ROUND #97, EPS: 0.95 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.001029, 0.6075  ]), 'NUM': [34, 8]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [29, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5306\n",
            "[*] LOSS: 0.12383431 / Q: {'Eval-Q': 1.761889, 'Target-Q': 1.731933}\n",
            "[*] LOSS: 0.19961473 / Q: {'Eval-Q': 2.250086, 'Target-Q': 2.365009}\n",
            "[*] LOSS: 0.12499433 / Q: {'Eval-Q': 1.526801, 'Target-Q': 1.479976}\n",
            "[*] LOSS: 0.17202844 / Q: {'Eval-Q': 1.864396, 'Target-Q': 1.718519}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.04661334752075176, 'total_reward': 294.84499844908714, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_97\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_97\n",
            "\n",
            "\n",
            "[*] ROUND #98, EPS: 0.95 NUMBER: [64, 64]\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2123\n",
            "[*] LOSS: 0.15986101 / Q: {'Eval-Q': 1.915528, 'Target-Q': 2.04325}\n",
            "[*] LOSS: 0.16706787 / Q: {'Eval-Q': 1.636425, 'Target-Q': 1.473205}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.16881697384891697, 'total_reward': 314.3549980157986, 'kill': 62}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_98\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_98\n",
            "\n",
            "\n",
            "[*] ROUND #99, EPS: 0.95 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005   , -0.001429]), 'NUM': [2, 28]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2228\n",
            "[*] LOSS: 0.20322424 / Q: {'Eval-Q': 1.678699, 'Target-Q': 1.650177}\n",
            "[*] LOSS: 0.3113779 / Q: {'Eval-Q': 2.230698, 'Target-Q': 2.082521}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.034434265588835854, 'total_reward': 167.67499929200858, 'kill': 38}\n",
            "\n",
            "\n",
            "[*] ROUND #100, EPS: 0.95 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.001552, -0.005   ]), 'NUM': [29, 3]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.001552, -0.005   ]), 'NUM': [29, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3795\n",
            "[*] LOSS: 0.20712218 / Q: {'Eval-Q': 1.758924, 'Target-Q': 1.650457}\n",
            "[*] LOSS: 0.44285816 / Q: {'Eval-Q': 2.667366, 'Target-Q': 2.670639}\n",
            "[*] LOSS: 0.1469142 / Q: {'Eval-Q': 2.073032, 'Target-Q': 1.834755}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07276682573227322, 'total_reward': 293.0999983288348, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_100\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_100\n",
            "\n",
            "\n",
            "[*] ROUND #101, EPS: 0.95 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.247381, 0.038571]), 'NUM': [21, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2324\n",
            "[*] LOSS: 0.22716787 / Q: {'Eval-Q': 2.439065, 'Target-Q': 2.003439}\n",
            "[*] LOSS: 0.16785805 / Q: {'Eval-Q': 2.444544, 'Target-Q': 2.381501}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1581441154607757, 'total_reward': 313.1049980549142, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_101\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_101\n",
            "\n",
            "\n",
            "[*] ROUND #102, EPS: 0.95 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005   , -0.002059]), 'NUM': [2, 34]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 34]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005   , -0.002059]), 'NUM': [1, 34]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1962\n",
            "[*] LOSS: 0.23451586 / Q: {'Eval-Q': 3.356566, 'Target-Q': 3.387192}\n",
            "[*] LOSS: 0.52503043 / Q: {'Eval-Q': 2.528298, 'Target-Q': 2.52759}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.016495606636947387, 'total_reward': 128.20999942254275, 'kill': 30}\n",
            "\n",
            "\n",
            "[*] ROUND #103, EPS: 0.95 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.002692, 0.009286]), 'NUM': [26, 7]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [24, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3792\n",
            "[*] LOSS: 0.40244177 / Q: {'Eval-Q': 3.141147, 'Target-Q': 3.329876}\n",
            "[*] LOSS: 0.37913913 / Q: {'Eval-Q': 2.142176, 'Target-Q': 2.194354}\n",
            "[*] LOSS: 0.36389542 / Q: {'Eval-Q': 2.609169, 'Target-Q': 2.627257}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07313414907244599, 'total_reward': 301.43999870307744, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_103\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_103\n",
            "\n",
            "\n",
            "[*] ROUND #104, EPS: 0.95 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.128472, 1.63    ]), 'NUM': [36, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3020\n",
            "[*] LOSS: 0.17349909 / Q: {'Eval-Q': 2.684354, 'Target-Q': 2.558994}\n",
            "[*] LOSS: 0.19141857 / Q: {'Eval-Q': 2.710482, 'Target-Q': 2.8503}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1002698583947532, 'total_reward': 313.6549982763827, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_104\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_104\n",
            "\n",
            "\n",
            "[*] ROUND #105, EPS: 0.95 NUMBER: [64, 64]\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1801\n",
            "[*] LOSS: 0.31794053 / Q: {'Eval-Q': 3.017285, 'Target-Q': 3.074003}\n",
            "[*] LOSS: 0.2680766 / Q: {'Eval-Q': 2.683854, 'Target-Q': 2.657277}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.16197307829174212, 'total_reward': 177.61499913968146, 'kill': 39}\n",
            "\n",
            "\n",
            "[*] ROUND #106, EPS: 0.95 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.004,  0.62 ]), 'NUM': [10, 16]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2005\n",
            "[*] LOSS: 0.20341271 / Q: {'Eval-Q': 2.88354, 'Target-Q': 2.875702}\n",
            "[*] LOSS: 0.2755838 / Q: {'Eval-Q': 2.71462, 'Target-Q': 2.415567}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.18394951399902434, 'total_reward': 256.7949987985194, 'kill': 52}\n",
            "\n",
            "\n",
            "[*] ROUND #107, EPS: 0.95 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.009286, 0.010385]), 'NUM': [14, 13]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4192\n",
            "[*] LOSS: 0.47224796 / Q: {'Eval-Q': 2.760154, 'Target-Q': 2.559858}\n",
            "[*] LOSS: 0.48575163 / Q: {'Eval-Q': 3.409314, 'Target-Q': 3.476814}\n",
            "[*] LOSS: 1.0125127 / Q: {'Eval-Q': 3.141628, 'Target-Q': 3.023887}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03054743188945043, 'total_reward': 290.8299986803904, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_107\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_107\n",
            "\n",
            "\n",
            "[*] ROUND #108, EPS: 0.95 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.002674,  0.009286]), 'NUM': [43, 7]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.002619, -0.005   ]), 'NUM': [42, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5010\n",
            "[*] LOSS: 0.11158197 / Q: {'Eval-Q': 2.226652, 'Target-Q': 2.178459}\n",
            "[*] LOSS: 0.15019754 / Q: {'Eval-Q': 3.001574, 'Target-Q': 2.808218}\n",
            "[*] LOSS: 0.36573178 / Q: {'Eval-Q': 2.796649, 'Target-Q': 2.779872}\n",
            "[*] LOSS: 0.40719894 / Q: {'Eval-Q': 3.518035, 'Target-Q': 3.227361}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.05529217727336682, 'total_reward': 291.1599981626496, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_108\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_108\n",
            "\n",
            "\n",
            "[*] ROUND #109, EPS: 0.95 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.183462, -0.1     ]), 'NUM': [26, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2210\n",
            "[*] LOSS: 0.23438299 / Q: {'Eval-Q': 3.4745, 'Target-Q': 3.51021}\n",
            "[*] LOSS: 0.1781126 / Q: {'Eval-Q': 2.400196, 'Target-Q': 2.279712}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1583938970514485, 'total_reward': 307.5399979967624, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_109\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_109\n",
            "\n",
            "\n",
            "[*] ROUND #110, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.577647, -0.004167]), 'NUM': [17, 12]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.105]), 'NUM': [9, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2693\n",
            "[*] LOSS: 0.26426074 / Q: {'Eval-Q': 2.670498, 'Target-Q': 2.611246}\n",
            "[*] LOSS: 0.248447 / Q: {'Eval-Q': 2.598334, 'Target-Q': 2.452672}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.10657721421352882, 'total_reward': 293.60999827552587, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_110\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_110\n",
            "\n",
            "\n",
            "[*] ROUND #111, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.000405, 0.045   ]), 'NUM': [37, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2552\n",
            "[*] LOSS: 0.41178417 / Q: {'Eval-Q': 3.23056, 'Target-Q': 3.0175}\n",
            "[*] LOSS: 0.3130886 / Q: {'Eval-Q': 2.804983, 'Target-Q': 2.678541}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.13094811865959305, 'total_reward': 302.27499807905406, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_111\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_111\n",
            "\n",
            "\n",
            "[*] ROUND #112, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.469194, 0.024643]), 'NUM': [31, 14]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3068\n",
            "[*] LOSS: 0.70728487 / Q: {'Eval-Q': 4.154587, 'Target-Q': 4.104345}\n",
            "[*] LOSS: 0.49382946 / Q: {'Eval-Q': 3.767048, 'Target-Q': 3.68359}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.10205625281536382, 'total_reward': 295.55499798897654, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_112\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_112\n",
            "\n",
            "\n",
            "[*] ROUND #113, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.273333, 0.411875]), 'NUM': [18, 24]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2411\n",
            "[*] LOSS: 0.40894723 / Q: {'Eval-Q': 3.258642, 'Target-Q': 3.05674}\n",
            "[*] LOSS: 0.17821996 / Q: {'Eval-Q': 2.90647, 'Target-Q': 2.812155}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.14596045718486964, 'total_reward': 228.86499849986285, 'kill': 49}\n",
            "\n",
            "\n",
            "[*] ROUND #114, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.039444, 0.01375 ]), 'NUM': [9, 16]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2018\n",
            "[*] LOSS: 0.25241148 / Q: {'Eval-Q': 4.7228, 'Target-Q': 4.705521}\n",
            "[*] LOSS: 0.26300764 / Q: {'Eval-Q': 3.57994, 'Target-Q': 3.406113}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1846047738641425, 'total_reward': 237.32999863941222, 'kill': 53}\n",
            "\n",
            "\n",
            "[*] ROUND #115, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.17431 , 0.008125]), 'NUM': [29, 8]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2596\n",
            "[*] LOSS: 0.35878524 / Q: {'Eval-Q': 3.241318, 'Target-Q': 2.996633}\n",
            "[*] LOSS: 0.5306357 / Q: {'Eval-Q': 3.629813, 'Target-Q': 3.325444}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12506416409834864, 'total_reward': 296.7949983365834, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_115\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_115\n",
            "\n",
            "\n",
            "[*] ROUND #116, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.668667, 0.219286]), 'NUM': [15, 21]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2214\n",
            "[*] LOSS: 0.21252958 / Q: {'Eval-Q': 3.605612, 'Target-Q': 3.438712}\n",
            "[*] LOSS: 0.5401241 / Q: {'Eval-Q': 4.643088, 'Target-Q': 4.076484}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.16911394902753035, 'total_reward': 234.04999854043126, 'kill': 55}\n",
            "\n",
            "\n",
            "[*] ROUND #117, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.278333, 0.038571]), 'NUM': [18, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2452\n",
            "[*] LOSS: 0.5538654 / Q: {'Eval-Q': 4.268496, 'Target-Q': 3.87426}\n",
            "[*] LOSS: 0.57302713 / Q: {'Eval-Q': 3.703632, 'Target-Q': 3.642608}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.13968643321075974, 'total_reward': 319.9849984012544, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_117\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_117\n",
            "\n",
            "\n",
            "[*] ROUND #118, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.741538, 0.236364]), 'NUM': [13, 22]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2073\n",
            "[*] LOSS: 0.6097401 / Q: {'Eval-Q': 3.660795, 'Target-Q': 3.51777}\n",
            "[*] LOSS: 0.20760041 / Q: {'Eval-Q': 3.182246, 'Target-Q': 3.064031}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.13484403383252502, 'total_reward': 205.15499882679433, 'kill': 45}\n",
            "\n",
            "\n",
            "[*] ROUND #119, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.011667, 0.005   ]), 'NUM': [6, 20]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2006\n",
            "[*] LOSS: 0.5027813 / Q: {'Eval-Q': 3.60784, 'Target-Q': 3.337306}\n",
            "[*] LOSS: 0.9793614 / Q: {'Eval-Q': 3.82142, 'Target-Q': 3.984453}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08534254987338481, 'total_reward': 185.18999909516424, 'kill': 44}\n",
            "\n",
            "\n",
            "[*] ROUND #120, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([1.596667, 0.2675  ]), 'NUM': [3, 18]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1836\n",
            "[*] LOSS: 0.6871287 / Q: {'Eval-Q': 3.546878, 'Target-Q': 3.552832}\n",
            "[*] LOSS: 0.8341264 / Q: {'Eval-Q': 4.039124, 'Target-Q': 3.659129}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.18753026610863313, 'total_reward': 220.1399989053607, 'kill': 47}\n",
            "\n",
            "\n",
            "[*] ROUND #121, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.014062, 0.215833]), 'NUM': [16, 24]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2246\n",
            "[*] LOSS: 0.2457217 / Q: {'Eval-Q': 3.539922, 'Target-Q': 3.443469}\n",
            "[*] LOSS: 0.35081702 / Q: {'Eval-Q': 3.852592, 'Target-Q': 3.586518}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07057500735671343, 'total_reward': 187.08999903686345, 'kill': 44}\n",
            "\n",
            "\n",
            "[*] ROUND #122, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.03    , 0.371923]), 'NUM': [3, 13]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1880\n",
            "[*] LOSS: 0.33420846 / Q: {'Eval-Q': 4.894626, 'Target-Q': 4.375955}\n",
            "[*] LOSS: 1.4860529 / Q: {'Eval-Q': 4.150982, 'Target-Q': 3.891672}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.14808564344924915, 'total_reward': 231.31999885104597, 'kill': 52}\n",
            "\n",
            "\n",
            "[*] ROUND #123, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005  ,  0.00125]), 'NUM': [3, 32]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1823\n",
            "[*] LOSS: 0.4329138 / Q: {'Eval-Q': 4.32365, 'Target-Q': 4.135822}\n",
            "[*] LOSS: 0.5785876 / Q: {'Eval-Q': 4.639256, 'Target-Q': 4.35498}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08058921162200476, 'total_reward': 145.3049992993474, 'kill': 32}\n",
            "\n",
            "\n",
            "[*] ROUND #124, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.0125  , 0.231364]), 'NUM': [6, 22]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 20]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2024\n",
            "[*] LOSS: 0.39423376 / Q: {'Eval-Q': 4.404914, 'Target-Q': 4.408975}\n",
            "[*] LOSS: 0.5100887 / Q: {'Eval-Q': 4.53995, 'Target-Q': 3.953104}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.06260843571236935, 'total_reward': 197.29999899398535, 'kill': 45}\n",
            "\n",
            "\n",
            "[*] ROUND #125, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.9755  , 0.218182]), 'NUM': [10, 22]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.095, -0.005]), 'NUM': [1, 19]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2318\n",
            "[*] LOSS: 0.2623225 / Q: {'Eval-Q': 3.955191, 'Target-Q': 3.748736}\n",
            "[*] LOSS: 1.2997364 / Q: {'Eval-Q': 4.773793, 'Target-Q': 4.424472}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.021372535249060704, 'total_reward': 195.8299989933148, 'kill': 45}\n",
            "\n",
            "\n",
            "[*] ROUND #126, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.028333, 0.011   ]), 'NUM': [9, 25]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2077\n",
            "[*] LOSS: 0.36203772 / Q: {'Eval-Q': 4.177297, 'Target-Q': 4.088556}\n",
            "[*] LOSS: 0.7185859 / Q: {'Eval-Q': 5.11276, 'Target-Q': 4.984428}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12418863693004194, 'total_reward': 187.6349988989532, 'kill': 42}\n",
            "\n",
            "\n",
            "[*] ROUND #127, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.036667,  0.222273]), 'NUM': [3, 22]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1838\n",
            "[*] LOSS: 0.2828025 / Q: {'Eval-Q': 3.879164, 'Target-Q': 3.885927}\n",
            "[*] LOSS: 0.36450768 / Q: {'Eval-Q': 4.573268, 'Target-Q': 4.087453}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.14108906565190021, 'total_reward': 198.7299990169704, 'kill': 42}\n",
            "\n",
            "\n",
            "[*] ROUND #128, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.0075, -0.005 ]), 'NUM': [8, 26]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.001]), 'NUM': [1, 25]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2129\n",
            "[*] LOSS: 0.38244337 / Q: {'Eval-Q': 5.233608, 'Target-Q': 4.8066}\n",
            "[*] LOSS: 0.3696388 / Q: {'Eval-Q': 4.838422, 'Target-Q': 4.516016}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.049604426036919655, 'total_reward': 173.9749990515411, 'kill': 39}\n",
            "\n",
            "\n",
            "[*] ROUND #129, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.0525  ,  0.173571]), 'NUM': [2, 28]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1725\n",
            "[*] LOSS: 1.2760835 / Q: {'Eval-Q': 5.71317, 'Target-Q': 5.4229}\n",
            "[*] LOSS: 0.698903 / Q: {'Eval-Q': 4.500291, 'Target-Q': 4.34026}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.10103328323658724, 'total_reward': 157.0949992192909, 'kill': 36}\n",
            "\n",
            "\n",
            "[*] ROUND #130, EPS: 0.94 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.0575  , 0.004524]), 'NUM': [8, 21]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1965\n",
            "[*] LOSS: 0.8666693 / Q: {'Eval-Q': 4.937523, 'Target-Q': 4.750116}\n",
            "[*] LOSS: 0.4753029 / Q: {'Eval-Q': 4.58192, 'Target-Q': 4.536594}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1618645970245217, 'total_reward': 210.19499878957868, 'kill': 46}\n",
            "\n",
            "\n",
            "[*] ROUND #131, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.00375, 0.199  ]), 'NUM': [12, 25]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2148\n",
            "[*] LOSS: 0.6813027 / Q: {'Eval-Q': 5.043412, 'Target-Q': 5.141261}\n",
            "[*] LOSS: 0.50472313 / Q: {'Eval-Q': 5.170677, 'Target-Q': 5.017127}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09211946491605451, 'total_reward': 198.6799989482388, 'kill': 41}\n",
            "\n",
            "\n",
            "[*] ROUND #132, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.191552, 0.015333]), 'NUM': [29, 15]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2755\n",
            "[*] LOSS: 0.56028783 / Q: {'Eval-Q': 4.901245, 'Target-Q': 4.425529}\n",
            "[*] LOSS: 0.3308249 / Q: {'Eval-Q': 5.297276, 'Target-Q': 4.885762}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12388343361842746, 'total_reward': 297.4349984386936, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_132\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_132\n",
            "\n",
            "\n",
            "[*] ROUND #133, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.003333,  0.203333]), 'NUM': [3, 24]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1798\n",
            "[*] LOSS: 0.48436388 / Q: {'Eval-Q': 4.938478, 'Target-Q': 4.983709}\n",
            "[*] LOSS: 0.66013277 / Q: {'Eval-Q': 5.35141, 'Target-Q': 5.215843}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09765548530130637, 'total_reward': 165.12999920733273, 'kill': 40}\n",
            "\n",
            "\n",
            "[*] ROUND #134, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.032143,  0.61375 ]), 'NUM': [7, 16]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 15]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 15]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 15]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 15]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 15]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 15]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 15]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2594\n",
            "[*] LOSS: 0.75555664 / Q: {'Eval-Q': 4.237849, 'Target-Q': 4.359658}\n",
            "[*] LOSS: 0.90022784 / Q: {'Eval-Q': 5.245363, 'Target-Q': 4.534886}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.016386076468303683, 'total_reward': 229.03999877441674, 'kill': 49}\n",
            "\n",
            "\n",
            "[*] ROUND #135, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.02    ,  0.336552]), 'NUM': [6, 29]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1848\n",
            "[*] LOSS: 0.7625094 / Q: {'Eval-Q': 4.946796, 'Target-Q': 4.686598}\n",
            "[*] LOSS: 0.59227824 / Q: {'Eval-Q': 4.137295, 'Target-Q': 3.790457}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12219067645654713, 'total_reward': 173.8799991570413, 'kill': 38}\n",
            "\n",
            "\n",
            "[*] ROUND #136, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.105]), 'NUM': [21, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2903\n",
            "[*] LOSS: 0.8829849 / Q: {'Eval-Q': 4.710094, 'Target-Q': 4.564852}\n",
            "[*] LOSS: 0.2796693 / Q: {'Eval-Q': 4.88514, 'Target-Q': 4.517204}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09509403563188357, 'total_reward': 293.69999832380563, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_136\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_136\n",
            "\n",
            "\n",
            "[*] ROUND #137, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.466429, -0.003   ]), 'NUM': [21, 5]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005,  0.095]), 'NUM': [21, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005,  0.095]), 'NUM': [21, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.105]), 'NUM': [21, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5901\n",
            "[*] LOSS: 0.59805804 / Q: {'Eval-Q': 4.101967, 'Target-Q': 4.085712}\n",
            "[*] LOSS: 0.85653186 / Q: {'Eval-Q': 4.988918, 'Target-Q': 5.272835}\n",
            "[*] LOSS: 0.3414301 / Q: {'Eval-Q': 5.158016, 'Target-Q': 4.455723}\n",
            "[*] LOSS: 1.1382363 / Q: {'Eval-Q': 4.462212, 'Target-Q': 4.632197}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03590129099771634, 'total_reward': 296.50999873969704, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_137\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_137\n",
            "\n",
            "\n",
            "[*] ROUND #138, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 28]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1761\n",
            "[*] LOSS: 0.460873 / Q: {'Eval-Q': 4.97309, 'Target-Q': 4.730401}\n",
            "[*] LOSS: 0.5039799 / Q: {'Eval-Q': 4.452938, 'Target-Q': 4.213877}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.06582972245186396, 'total_reward': 164.41499922052026, 'kill': 36}\n",
            "\n",
            "\n",
            "[*] ROUND #139, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.975   , 0.004318]), 'NUM': [5, 22]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1885\n",
            "[*] LOSS: 0.60277283 / Q: {'Eval-Q': 4.389358, 'Target-Q': 4.079729}\n",
            "[*] LOSS: 0.5985083 / Q: {'Eval-Q': 4.996886, 'Target-Q': 4.94545}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.090785625355817, 'total_reward': 189.6949990252033, 'kill': 44}\n",
            "\n",
            "\n",
            "[*] ROUND #140, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.018077, 0.011667]), 'NUM': [13, 6]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4166\n",
            "[*] LOSS: 0.570788 / Q: {'Eval-Q': 5.77146, 'Target-Q': 5.784386}\n",
            "[*] LOSS: 0.3387006 / Q: {'Eval-Q': 4.387362, 'Target-Q': 4.441057}\n",
            "[*] LOSS: 0.4284656 / Q: {'Eval-Q': 4.910346, 'Target-Q': 5.064765}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.042297235928616464, 'total_reward': 300.9349986007437, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_140\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_140\n",
            "\n",
            "\n",
            "[*] ROUND #141, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.045833, 0.300882]), 'NUM': [6, 17]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 15]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 14]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 14]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 14]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 14]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 14]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 14]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2205\n",
            "[*] LOSS: 0.40960848 / Q: {'Eval-Q': 4.802458, 'Target-Q': 4.436805}\n",
            "[*] LOSS: 0.38072056 / Q: {'Eval-Q': 5.009522, 'Target-Q': 4.590143}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.039945599675573484, 'total_reward': 235.58999890834093, 'kill': 50}\n",
            "\n",
            "\n",
            "[*] ROUND #142, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.163966, -0.036667]), 'NUM': [29, 3]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.105]), 'NUM': [28, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.105]), 'NUM': [28, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.105]), 'NUM': [28, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.105]), 'NUM': [28, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.105]), 'NUM': [28, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 12117\n",
            "[*] LOSS: 0.38550362 / Q: {'Eval-Q': 3.976202, 'Target-Q': 3.680912}\n",
            "[*] LOSS: 0.57514226 / Q: {'Eval-Q': 4.551512, 'Target-Q': 4.688367}\n",
            "[*] LOSS: 0.5560742 / Q: {'Eval-Q': 4.868987, 'Target-Q': 4.422376}\n",
            "[*] LOSS: 0.87156314 / Q: {'Eval-Q': 4.850294, 'Target-Q': 4.310352}\n",
            "[*] LOSS: 0.56125826 / Q: {'Eval-Q': 4.64238, 'Target-Q': 4.259671}\n",
            "[*] LOSS: 0.46761787 / Q: {'Eval-Q': 4.683584, 'Target-Q': 3.945971}\n",
            "[*] LOSS: 0.28436238 / Q: {'Eval-Q': 4.23006, 'Target-Q': 4.304981}\n",
            "[*] LOSS: 1.0972313 / Q: {'Eval-Q': 4.51986, 'Target-Q': 4.368156}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.013906714281271023, 'total_reward': 257.09499931149185, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_142\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_142\n",
            "\n",
            "\n",
            "[*] ROUND #143, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005   , -0.038333]), 'NUM': [34, 3]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [34, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [34, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.002059,  0.095   ]), 'NUM': [34, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7712\n",
            "[*] LOSS: 0.413244 / Q: {'Eval-Q': 4.154878, 'Target-Q': 4.038656}\n",
            "[*] LOSS: 0.5271293 / Q: {'Eval-Q': 4.714122, 'Target-Q': 4.460931}\n",
            "[*] LOSS: 0.46750915 / Q: {'Eval-Q': 3.5611, 'Target-Q': 3.677032}\n",
            "[*] LOSS: 0.45398736 / Q: {'Eval-Q': 4.629662, 'Target-Q': 4.595612}\n",
            "[*] LOSS: 0.74275714 / Q: {'Eval-Q': 4.4126, 'Target-Q': 4.389197}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.028411782459714225, 'total_reward': 280.78999883681536, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_143\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_143\n",
            "\n",
            "\n",
            "[*] ROUND #144, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 1]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4180\n",
            "[*] LOSS: 0.43174356 / Q: {'Eval-Q': 4.304066, 'Target-Q': 4.284042}\n",
            "[*] LOSS: 1.0450997 / Q: {'Eval-Q': 4.82288, 'Target-Q': 4.544778}\n",
            "[*] LOSS: 0.32793045 / Q: {'Eval-Q': 3.955421, 'Target-Q': 3.996931}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.06067037213480161, 'total_reward': 305.6799983140081, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_144\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_144\n",
            "\n",
            "\n",
            "[*] ROUND #145, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 3]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3145\n",
            "[*] LOSS: 0.39169472 / Q: {'Eval-Q': 4.34697, 'Target-Q': 4.162959}\n",
            "[*] LOSS: 0.35139957 / Q: {'Eval-Q': 3.668428, 'Target-Q': 3.493573}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08026395894168027, 'total_reward': 303.21499811299145, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_145\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_145\n",
            "\n",
            "\n",
            "[*] ROUND #146, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.217727, -0.036667]), 'NUM': [22, 3]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.00975,  2.395  ]), 'NUM': [20, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3995\n",
            "[*] LOSS: 0.25939155 / Q: {'Eval-Q': 4.327531, 'Target-Q': 4.254321}\n",
            "[*] LOSS: 0.8124938 / Q: {'Eval-Q': 3.866535, 'Target-Q': 3.757979}\n",
            "[*] LOSS: 0.57408637 / Q: {'Eval-Q': 4.854038, 'Target-Q': 4.45164}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.057064903789733445, 'total_reward': 306.24999830592424, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_146\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_146\n",
            "\n",
            "\n",
            "[*] ROUND #147, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.004583,  0.345   ]), 'NUM': [12, 14]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 8]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 8]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 8]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 8]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 8]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 8]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 8]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3605\n",
            "[*] LOSS: 0.944276 / Q: {'Eval-Q': 3.874024, 'Target-Q': 3.667521}\n",
            "[*] LOSS: 0.2915402 / Q: {'Eval-Q': 4.408428, 'Target-Q': 4.420804}\n",
            "[*] LOSS: 0.2754574 / Q: {'Eval-Q': 3.143498, 'Target-Q': 3.028053}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02133138991969692, 'total_reward': 257.67499884497374, 'kill': 56}\n",
            "\n",
            "\n",
            "[*] ROUND #148, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.043   ,  0.510789]), 'NUM': [5, 19]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 18]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 18]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 18]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 18]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 18]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2105\n",
            "[*] LOSS: 0.38676378 / Q: {'Eval-Q': 3.910748, 'Target-Q': 3.876857}\n",
            "[*] LOSS: 0.16341613 / Q: {'Eval-Q': 3.560651, 'Target-Q': 3.478665}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03229179795821318, 'total_reward': 205.48999888263643, 'kill': 46}\n",
            "\n",
            "\n",
            "[*] ROUND #149, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.5155  , 0.359643]), 'NUM': [10, 14]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 7]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 7]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 7]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 7]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 7]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3070\n",
            "[*] LOSS: 0.5755722 / Q: {'Eval-Q': 4.049086, 'Target-Q': 4.006697}\n",
            "[*] LOSS: 0.5203994 / Q: {'Eval-Q': 4.476991, 'Target-Q': 4.21104}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03072860360977824, 'total_reward': 249.55499873310328, 'kill': 57}\n",
            "\n",
            "\n",
            "[*] ROUND #150, EPS: 0.93 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.010588,  0.277647]), 'NUM': [17, 17]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.015556,  0.6075  ]), 'NUM': [9, 8]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 5]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 4]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 4]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005,  0.02 ]), 'NUM': [6, 4]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 4]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 4]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4645\n",
            "[*] LOSS: 0.31344342 / Q: {'Eval-Q': 3.805418, 'Target-Q': 3.614486}\n",
            "[*] LOSS: 0.26146287 / Q: {'Eval-Q': 2.946104, 'Target-Q': 2.791083}\n",
            "[*] LOSS: 0.30386722 / Q: {'Eval-Q': 3.715169, 'Target-Q': 3.616971}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02660663308606749, 'total_reward': 284.96499883942306, 'kill': 60}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_150\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_150\n",
            "\n",
            "\n",
            "[*] ROUND #151, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 22]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 22]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 22]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 22]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 22]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 22]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1977\n",
            "[*] LOSS: 0.35596284 / Q: {'Eval-Q': 3.707458, 'Target-Q': 3.693116}\n",
            "[*] LOSS: 0.701735 / Q: {'Eval-Q': 3.328613, 'Target-Q': 3.334887}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.024094412784470033, 'total_reward': 198.53499895893037, 'kill': 42}\n",
            "\n",
            "\n",
            "[*] ROUND #152, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.045, -0.005]), 'NUM': [2, 28]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 27]}\n",
            "> step #150, info: {'Ave-Reward': array([ 0.095   , -0.001296]), 'NUM': [1, 27]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 27]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 27]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 27]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 27]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1997\n",
            "[*] LOSS: 1.3386939 / Q: {'Eval-Q': 3.446918, 'Target-Q': 3.044331}\n",
            "[*] LOSS: 0.25467503 / Q: {'Eval-Q': 3.400307, 'Target-Q': 3.224927}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.017291959145802067, 'total_reward': 160.83499916642904, 'kill': 37}\n",
            "\n",
            "\n",
            "[*] ROUND #153, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 36]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1552\n",
            "[*] LOSS: 0.38576663 / Q: {'Eval-Q': 4.20014, 'Target-Q': 4.085889}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.0612004100238667, 'total_reward': 120.95999940298498, 'kill': 28}\n",
            "\n",
            "\n",
            "[*] ROUND #154, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 36]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 36]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 36]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1729\n",
            "[*] LOSS: 0.4182407 / Q: {'Eval-Q': 3.22821, 'Target-Q': 3.092964}\n",
            "[*] LOSS: 0.4168775 / Q: {'Eval-Q': 3.595955, 'Target-Q': 3.401608}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.023558188311030875, 'total_reward': 124.87499935459346, 'kill': 28}\n",
            "\n",
            "\n",
            "[*] ROUND #155, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.120641, -0.02875 ]), 'NUM': [39, 4]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005,  0.045]), 'NUM': [39, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [37, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [37, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 15485\n",
            "[*] LOSS: 0.949049 / Q: {'Eval-Q': 2.511392, 'Target-Q': 2.674487}\n",
            "[*] LOSS: 0.28994498 / Q: {'Eval-Q': 2.986957, 'Target-Q': 2.670873}\n",
            "[*] LOSS: 0.31893766 / Q: {'Eval-Q': 3.038918, 'Target-Q': 2.968394}\n",
            "[*] LOSS: 0.22289601 / Q: {'Eval-Q': 2.699379, 'Target-Q': 2.785044}\n",
            "[*] LOSS: 0.2729122 / Q: {'Eval-Q': 3.193292, 'Target-Q': 2.952357}\n",
            "[*] LOSS: 0.41420633 / Q: {'Eval-Q': 2.753302, 'Target-Q': 2.622398}\n",
            "[*] LOSS: 0.29891014 / Q: {'Eval-Q': 3.219299, 'Target-Q': 3.134656}\n",
            "[*] LOSS: 0.4536879 / Q: {'Eval-Q': 3.825646, 'Target-Q': 3.69618}\n",
            "[*] LOSS: 0.55901945 / Q: {'Eval-Q': 4.139258, 'Target-Q': 4.104013}\n",
            "[*] LOSS: 0.39123607 / Q: {'Eval-Q': 2.88049, 'Target-Q': 2.964935}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.011224342124380145, 'total_reward': 249.61499982327223, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_155\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_155\n",
            "\n",
            "\n",
            "[*] ROUND #156, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 27]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1675\n",
            "[*] LOSS: 0.24327104 / Q: {'Eval-Q': 3.404597, 'Target-Q': 3.50262}\n",
            "[*] LOSS: 0.2950373 / Q: {'Eval-Q': 3.330339, 'Target-Q': 3.255752}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.159599766850371, 'total_reward': 153.54499919060618, 'kill': 37}\n",
            "\n",
            "\n",
            "[*] ROUND #157, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 34]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 34]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 34]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1705\n",
            "[*] LOSS: 0.45855916 / Q: {'Eval-Q': 3.023637, 'Target-Q': 2.753206}\n",
            "[*] LOSS: 0.15219167 / Q: {'Eval-Q': 3.444764, 'Target-Q': 3.225781}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.036957902915345786, 'total_reward': 141.09499926492572, 'kill': 30}\n",
            "\n",
            "\n",
            "[*] ROUND #158, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.723571, -0.004722]), 'NUM': [7, 18]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005   , -0.012692]), 'NUM': [3, 13]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 11]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 11]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 11]}\n",
            "> step #300, info: {'Ave-Reward': array([0.045   , 0.004091]), 'NUM': [2, 11]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 11]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 11]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2634\n",
            "[*] LOSS: 0.33454004 / Q: {'Eval-Q': 3.766494, 'Target-Q': 3.576119}\n",
            "[*] LOSS: 0.37371677 / Q: {'Eval-Q': 3.177959, 'Target-Q': 3.089968}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03883825975974605, 'total_reward': 247.04499869514257, 'kill': 53}\n",
            "\n",
            "\n",
            "[*] ROUND #159, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.180714, 0.045625]), 'NUM': [28, 8]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005,  0.045]), 'NUM': [26, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.001154, -0.005   ]), 'NUM': [26, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 11392\n",
            "[*] LOSS: 0.32626343 / Q: {'Eval-Q': 2.612423, 'Target-Q': 2.640302}\n",
            "[*] LOSS: 0.26583198 / Q: {'Eval-Q': 2.995306, 'Target-Q': 2.983459}\n",
            "[*] LOSS: 0.34881186 / Q: {'Eval-Q': 2.40989, 'Target-Q': 2.546629}\n",
            "[*] LOSS: 0.22062166 / Q: {'Eval-Q': 2.487433, 'Target-Q': 2.444607}\n",
            "[*] LOSS: 0.06723416 / Q: {'Eval-Q': 2.67042, 'Target-Q': 2.460614}\n",
            "[*] LOSS: 0.6169032 / Q: {'Eval-Q': 2.523888, 'Target-Q': 2.607144}\n",
            "[*] LOSS: 0.67717665 / Q: {'Eval-Q': 3.230233, 'Target-Q': 3.081121}\n",
            "[*] LOSS: 0.32372808 / Q: {'Eval-Q': 2.535326, 'Target-Q': 2.624886}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.013783911259708482, 'total_reward': 246.22999926283956, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_159\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_159\n",
            "\n",
            "\n",
            "[*] ROUND #160, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005   , -0.001429]), 'NUM': [6, 28]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 26]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 26]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005   , -0.001154]), 'NUM': [2, 26]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2351\n",
            "[*] LOSS: 0.286652 / Q: {'Eval-Q': 3.462693, 'Target-Q': 3.521958}\n",
            "[*] LOSS: 0.19537812 / Q: {'Eval-Q': 2.7419, 'Target-Q': 2.657035}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02366493957585116, 'total_reward': 175.95999911706895, 'kill': 38}\n",
            "\n",
            "\n",
            "[*] ROUND #161, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 36]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 36]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 36]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 36]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 36]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 36]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 36]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2122\n",
            "[*] LOSS: 0.30461237 / Q: {'Eval-Q': 2.745584, 'Target-Q': 2.927144}\n",
            "[*] LOSS: 0.1909452 / Q: {'Eval-Q': 3.112036, 'Target-Q': 3.071275}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.007099867672487479, 'total_reward': 118.0049994373694, 'kill': 28}\n",
            "\n",
            "\n",
            "[*] ROUND #162, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [29, 6]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.001429, -0.005   ]), 'NUM': [28, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 12193\n",
            "[*] LOSS: 0.25478053 / Q: {'Eval-Q': 2.373, 'Target-Q': 2.394424}\n",
            "[*] LOSS: 0.65478265 / Q: {'Eval-Q': 3.18481, 'Target-Q': 3.297848}\n",
            "[*] LOSS: 0.27534842 / Q: {'Eval-Q': 3.358448, 'Target-Q': 3.205703}\n",
            "[*] LOSS: 0.27012283 / Q: {'Eval-Q': 3.303805, 'Target-Q': 2.930585}\n",
            "[*] LOSS: 0.2288417 / Q: {'Eval-Q': 2.875794, 'Target-Q': 2.856948}\n",
            "[*] LOSS: 0.21133634 / Q: {'Eval-Q': 3.490868, 'Target-Q': 3.330647}\n",
            "[*] LOSS: 0.48253763 / Q: {'Eval-Q': 3.70869, 'Target-Q': 3.712745}\n",
            "[*] LOSS: 0.12853535 / Q: {'Eval-Q': 2.396808, 'Target-Q': 2.133558}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01340188334064857, 'total_reward': 263.4149995846674, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_162\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_162\n",
            "\n",
            "\n",
            "[*] ROUND #163, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.399375, 0.58375 ]), 'NUM': [24, 8]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 9254\n",
            "[*] LOSS: 0.1747818 / Q: {'Eval-Q': 2.857444, 'Target-Q': 2.811325}\n",
            "[*] LOSS: 0.22035828 / Q: {'Eval-Q': 3.363224, 'Target-Q': 3.268158}\n",
            "[*] LOSS: 0.20784253 / Q: {'Eval-Q': 3.358172, 'Target-Q': 3.408455}\n",
            "[*] LOSS: 0.43687615 / Q: {'Eval-Q': 3.155518, 'Target-Q': 2.851743}\n",
            "[*] LOSS: 0.13555688 / Q: {'Eval-Q': 2.903983, 'Target-Q': 2.922425}\n",
            "[*] LOSS: 0.40574145 / Q: {'Eval-Q': 3.325774, 'Target-Q': 3.561182}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01597494997274835, 'total_reward': 276.4499992271885, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_163\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_163\n",
            "\n",
            "\n",
            "[*] ROUND #164, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.004615,  0.811667]), 'NUM': [26, 12]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 4]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 3]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 3]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 8177\n",
            "[*] LOSS: 0.51586896 / Q: {'Eval-Q': 2.963225, 'Target-Q': 2.602473}\n",
            "[*] LOSS: 0.49138975 / Q: {'Eval-Q': 2.762423, 'Target-Q': 2.507857}\n",
            "[*] LOSS: 0.16551904 / Q: {'Eval-Q': 3.025836, 'Target-Q': 3.018877}\n",
            "[*] LOSS: 0.1117255 / Q: {'Eval-Q': 3.117123, 'Target-Q': 2.567561}\n",
            "[*] LOSS: 0.3552585 / Q: {'Eval-Q': 3.138658, 'Target-Q': 3.129891}\n",
            "[*] LOSS: 0.19333346 / Q: {'Eval-Q': 2.974234, 'Target-Q': 2.68826}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01567946718833107, 'total_reward': 266.25499910861254, 'kill': 62}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_164\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_164\n",
            "\n",
            "\n",
            "[*] ROUND #165, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.002222,  0.02    ]), 'NUM': [36, 4]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 3]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 3]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 3]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 3]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 3]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 15110\n",
            "[*] LOSS: 0.142303 / Q: {'Eval-Q': 1.782242, 'Target-Q': 1.640635}\n",
            "[*] LOSS: 0.27726665 / Q: {'Eval-Q': 2.299826, 'Target-Q': 2.082976}\n",
            "[*] LOSS: 0.13265665 / Q: {'Eval-Q': 2.134562, 'Target-Q': 2.043086}\n",
            "[*] LOSS: 0.1691074 / Q: {'Eval-Q': 1.745068, 'Target-Q': 1.712949}\n",
            "[*] LOSS: 0.31050518 / Q: {'Eval-Q': 2.450295, 'Target-Q': 2.379321}\n",
            "[*] LOSS: 0.099056 / Q: {'Eval-Q': 2.191509, 'Target-Q': 2.05168}\n",
            "[*] LOSS: 0.08855075 / Q: {'Eval-Q': 2.44257, 'Target-Q': 2.514341}\n",
            "[*] LOSS: 0.18423325 / Q: {'Eval-Q': 2.534094, 'Target-Q': 2.215648}\n",
            "[*] LOSS: 0.26402867 / Q: {'Eval-Q': 2.237238, 'Target-Q': 1.93993}\n",
            "[*] LOSS: 0.17809094 / Q: {'Eval-Q': 2.49574, 'Target-Q': 2.272358}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.010785258100441241, 'total_reward': 230.6899998569861, 'kill': 61}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_165\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_165\n",
            "\n",
            "\n",
            "[*] ROUND #166, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.011667, 0.      ]), 'NUM': [6, 20]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.02 , -0.005]), 'NUM': [4, 19]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 16]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 15]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 14]}\n",
            "> step #300, info: {'Ave-Reward': array([ 0.028333, -0.005   ]), 'NUM': [3, 14]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 14]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 14]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2973\n",
            "[*] LOSS: 0.17122069 / Q: {'Eval-Q': 2.868737, 'Target-Q': 2.835557}\n",
            "[*] LOSS: 0.1302581 / Q: {'Eval-Q': 2.261548, 'Target-Q': 2.195123}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03628844939550722, 'total_reward': 219.74499885737896, 'kill': 50}\n",
            "\n",
            "\n",
            "[*] ROUND #167, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 1.628333, -0.004853]), 'NUM': [3, 34]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 33]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 33]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 33]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 33]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 33]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 33]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 33]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2431\n",
            "[*] LOSS: 0.39530647 / Q: {'Eval-Q': 1.96641, 'Target-Q': 1.970647}\n",
            "[*] LOSS: 0.104524836 / Q: {'Eval-Q': 2.009175, 'Target-Q': 2.051565}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.012428484103866759, 'total_reward': 117.25499945320189, 'kill': 31}\n",
            "\n",
            "\n",
            "[*] ROUND #168, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.2075  , 0.006667]), 'NUM': [24, 9]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 10252\n",
            "[*] LOSS: 0.20586379 / Q: {'Eval-Q': 2.261083, 'Target-Q': 1.891409}\n",
            "[*] LOSS: 0.18056789 / Q: {'Eval-Q': 2.308996, 'Target-Q': 2.405794}\n",
            "[*] LOSS: 0.17624277 / Q: {'Eval-Q': 1.913422, 'Target-Q': 1.826418}\n",
            "[*] LOSS: 0.2659508 / Q: {'Eval-Q': 1.881606, 'Target-Q': 1.812615}\n",
            "[*] LOSS: 0.5909474 / Q: {'Eval-Q': 2.120736, 'Target-Q': 2.211485}\n",
            "[*] LOSS: 0.20341283 / Q: {'Eval-Q': 2.653826, 'Target-Q': 2.341537}\n",
            "[*] LOSS: 0.07993284 / Q: {'Eval-Q': 1.70981, 'Target-Q': 1.783567}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01612311031084258, 'total_reward': 250.84499914105982, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_168\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_168\n",
            "\n",
            "\n",
            "[*] ROUND #169, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.02    , -0.001875]), 'NUM': [4, 32]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 32]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 32]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 32]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 31]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 31]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 31]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2626\n",
            "[*] LOSS: 0.56989115 / Q: {'Eval-Q': 1.966171, 'Target-Q': 1.861794}\n",
            "[*] LOSS: 0.15958655 / Q: {'Eval-Q': 1.788803, 'Target-Q': 1.694321}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01649397880928148, 'total_reward': 145.27999923657626, 'kill': 33}\n",
            "\n",
            "\n",
            "[*] ROUND #170, EPS: 0.92 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.028333, 0.004375]), 'NUM': [6, 32]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 31]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 31]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 31]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 31]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 31]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 31]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2457\n",
            "[*] LOSS: 0.15177527 / Q: {'Eval-Q': 2.272915, 'Target-Q': 2.267246}\n",
            "[*] LOSS: 0.109325044 / Q: {'Eval-Q': 1.646211, 'Target-Q': 1.663072}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.013312262310994551, 'total_reward': 143.82499925326556, 'kill': 33}\n",
            "\n",
            "\n",
            "[*] ROUND #171, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005,  0.02 ]), 'NUM': [37, 4]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [37, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [37, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [37, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [37, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.055]), 'NUM': [37, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005,  0.045]), 'NUM': [37, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [37, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 15420\n",
            "[*] LOSS: 0.21156386 / Q: {'Eval-Q': 2.432892, 'Target-Q': 2.396381}\n",
            "[*] LOSS: 0.06418306 / Q: {'Eval-Q': 1.729323, 'Target-Q': 1.685453}\n",
            "[*] LOSS: 0.18219644 / Q: {'Eval-Q': 1.694509, 'Target-Q': 1.657705}\n",
            "[*] LOSS: 0.09722201 / Q: {'Eval-Q': 1.989027, 'Target-Q': 1.408224}\n",
            "[*] LOSS: 0.05904763 / Q: {'Eval-Q': 1.443617, 'Target-Q': 1.442251}\n",
            "[*] LOSS: 0.07820644 / Q: {'Eval-Q': 1.42133, 'Target-Q': 1.447986}\n",
            "[*] LOSS: 0.13969645 / Q: {'Eval-Q': 1.907527, 'Target-Q': 1.68772}\n",
            "[*] LOSS: 0.08423588 / Q: {'Eval-Q': 1.634525, 'Target-Q': 1.551117}\n",
            "[*] LOSS: 0.16808061 / Q: {'Eval-Q': 2.442599, 'Target-Q': 2.173444}\n",
            "[*] LOSS: 0.23395903 / Q: {'Eval-Q': 2.208155, 'Target-Q': 2.27616}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.012100191276939955, 'total_reward': 243.33499973174185, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_171\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_171\n",
            "\n",
            "\n",
            "[*] ROUND #172, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [37, 2]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 15154\n",
            "[*] LOSS: 0.42728114 / Q: {'Eval-Q': 3.004002, 'Target-Q': 3.035373}\n",
            "[*] LOSS: 0.41394097 / Q: {'Eval-Q': 1.576465, 'Target-Q': 1.417299}\n",
            "[*] LOSS: 0.19290861 / Q: {'Eval-Q': 2.009931, 'Target-Q': 2.026924}\n",
            "[*] LOSS: 0.12330479 / Q: {'Eval-Q': 1.695722, 'Target-Q': 1.73518}\n",
            "[*] LOSS: 0.12907329 / Q: {'Eval-Q': 2.183144, 'Target-Q': 2.169949}\n",
            "[*] LOSS: 0.1279273 / Q: {'Eval-Q': 1.713869, 'Target-Q': 1.7847}\n",
            "[*] LOSS: 0.44634187 / Q: {'Eval-Q': 2.179013, 'Target-Q': 2.213167}\n",
            "[*] LOSS: 0.18890777 / Q: {'Eval-Q': 1.819875, 'Target-Q': 1.792857}\n",
            "[*] LOSS: 0.24166584 / Q: {'Eval-Q': 1.88866, 'Target-Q': 1.958057}\n",
            "[*] LOSS: 0.09494674 / Q: {'Eval-Q': 2.160192, 'Target-Q': 2.076663}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.010841780152730827, 'total_reward': 239.06999973300844, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_172\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_172\n",
            "\n",
            "\n",
            "[*] ROUND #173, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.550556, -0.011333]), 'NUM': [9, 15]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005 , -0.0175]), 'NUM': [3, 8]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 8]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 8]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 8]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 8]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 8]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 8]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3130\n",
            "[*] LOSS: 0.15209912 / Q: {'Eval-Q': 1.806456, 'Target-Q': 1.690272}\n",
            "[*] LOSS: 0.12707748 / Q: {'Eval-Q': 1.796071, 'Target-Q': 1.781702}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.026734746828438693, 'total_reward': 252.55499864090234, 'kill': 56}\n",
            "\n",
            "\n",
            "[*] ROUND #174, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.008043, 0.017222]), 'NUM': [23, 9]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 3]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 3]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 3]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 3]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 3]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 9574\n",
            "[*] LOSS: 0.13705465 / Q: {'Eval-Q': 2.058254, 'Target-Q': 2.005714}\n",
            "[*] LOSS: 0.15135881 / Q: {'Eval-Q': 1.581942, 'Target-Q': 1.568071}\n",
            "[*] LOSS: 0.10100115 / Q: {'Eval-Q': 1.632772, 'Target-Q': 1.595775}\n",
            "[*] LOSS: 0.0895546 / Q: {'Eval-Q': 1.821473, 'Target-Q': 1.577367}\n",
            "[*] LOSS: 0.07373781 / Q: {'Eval-Q': 1.786165, 'Target-Q': 1.730869}\n",
            "[*] LOSS: 0.26504162 / Q: {'Eval-Q': 2.032566, 'Target-Q': 1.706635}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.016709454843708685, 'total_reward': 258.84499908890575, 'kill': 61}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_174\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_174\n",
            "\n",
            "\n",
            "[*] ROUND #175, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.008793, 0.0075  ]), 'NUM': [29, 8]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2905\n",
            "[*] LOSS: 0.15266563 / Q: {'Eval-Q': 1.945593, 'Target-Q': 1.898157}\n",
            "[*] LOSS: 0.06979908 / Q: {'Eval-Q': 1.513799, 'Target-Q': 1.499828}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.10852750262545975, 'total_reward': 293.6499983081594, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_175\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_175\n",
            "\n",
            "\n",
            "[*] ROUND #176, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.001429, -0.005   ]), 'NUM': [28, 4]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 3]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.105]), 'NUM': [28, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 12165\n",
            "[*] LOSS: 0.09423074 / Q: {'Eval-Q': 1.859547, 'Target-Q': 1.797477}\n",
            "[*] LOSS: 0.7337979 / Q: {'Eval-Q': 1.66607, 'Target-Q': 1.806289}\n",
            "[*] LOSS: 0.025375044 / Q: {'Eval-Q': 1.35436, 'Target-Q': 1.323016}\n",
            "[*] LOSS: 0.18783593 / Q: {'Eval-Q': 1.674103, 'Target-Q': 1.539047}\n",
            "[*] LOSS: 0.60150534 / Q: {'Eval-Q': 2.022266, 'Target-Q': 2.175622}\n",
            "[*] LOSS: 0.10449841 / Q: {'Eval-Q': 1.345217, 'Target-Q': 1.324999}\n",
            "[*] LOSS: 0.17471008 / Q: {'Eval-Q': 1.876353, 'Target-Q': 1.885372}\n",
            "[*] LOSS: 0.10189477 / Q: {'Eval-Q': 1.527476, 'Target-Q': 1.480523}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.012962325217505644, 'total_reward': 256.9549995502457, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_176\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_176\n",
            "\n",
            "\n",
            "[*] ROUND #177, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.416957, -0.032143]), 'NUM': [23, 7]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 8158\n",
            "[*] LOSS: 0.13581057 / Q: {'Eval-Q': 1.792116, 'Target-Q': 1.555696}\n",
            "[*] LOSS: 0.36510175 / Q: {'Eval-Q': 1.6891, 'Target-Q': 1.768432}\n",
            "[*] LOSS: 0.1548709 / Q: {'Eval-Q': 1.975827, 'Target-Q': 2.070795}\n",
            "[*] LOSS: 0.44747937 / Q: {'Eval-Q': 1.489507, 'Target-Q': 1.61391}\n",
            "[*] LOSS: 0.16760433 / Q: {'Eval-Q': 1.624278, 'Target-Q': 1.214468}\n",
            "[*] LOSS: 0.11290918 / Q: {'Eval-Q': 1.451033, 'Target-Q': 1.49517}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.017731087038066834, 'total_reward': 268.74999902490526, 'kill': 62}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_177\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_177\n",
            "\n",
            "\n",
            "[*] ROUND #178, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.011667, 0.001667]), 'NUM': [12, 15]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 10]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 10]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 10]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 10]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 10]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 10]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 10]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4157\n",
            "[*] LOSS: 0.17254154 / Q: {'Eval-Q': 1.701548, 'Target-Q': 1.663937}\n",
            "[*] LOSS: 0.5238705 / Q: {'Eval-Q': 1.867577, 'Target-Q': 1.80677}\n",
            "[*] LOSS: 0.1280241 / Q: {'Eval-Q': 1.657022, 'Target-Q': 1.590504}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.020318928397797448, 'total_reward': 246.1049987981096, 'kill': 54}\n",
            "\n",
            "\n",
            "[*] ROUND #179, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.003571,  0.403333]), 'NUM': [7, 24]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 23]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005   , -0.000652]), 'NUM': [3, 23]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 23]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 22]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 22]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 22]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005   , -0.000455]), 'NUM': [1, 22]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2426\n",
            "[*] LOSS: 0.26696277 / Q: {'Eval-Q': 1.198352, 'Target-Q': 1.384967}\n",
            "[*] LOSS: 0.15220994 / Q: {'Eval-Q': 1.760977, 'Target-Q': 1.800318}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.028154937052114023, 'total_reward': 190.98499916493893, 'kill': 42}\n",
            "\n",
            "\n",
            "[*] ROUND #180, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.695   , -0.004706]), 'NUM': [7, 17]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 12]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 12]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 12]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 12]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 12]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 12]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 12]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2960\n",
            "[*] LOSS: 0.3444662 / Q: {'Eval-Q': 2.298905, 'Target-Q': 2.334948}\n",
            "[*] LOSS: 0.25833026 / Q: {'Eval-Q': 2.073832, 'Target-Q': 1.877743}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.033350771303841564, 'total_reward': 241.30499885231256, 'kill': 52}\n",
            "\n",
            "\n",
            "[*] ROUND #181, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.00375 , 0.476818]), 'NUM': [12, 11]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 7]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 7]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 7]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 6]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 6]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 6]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4112\n",
            "[*] LOSS: 0.33952084 / Q: {'Eval-Q': 2.371226, 'Target-Q': 2.049617}\n",
            "[*] LOSS: 0.20079504 / Q: {'Eval-Q': 1.861937, 'Target-Q': 1.741355}\n",
            "[*] LOSS: 0.16481128 / Q: {'Eval-Q': 2.048379, 'Target-Q': 1.910075}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02498923309138266, 'total_reward': 262.62999892700464, 'kill': 58}\n",
            "\n",
            "\n",
            "[*] ROUND #182, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.008043, 0.006111]), 'NUM': [23, 9]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4405\n",
            "[*] LOSS: 0.05701933 / Q: {'Eval-Q': 1.235421, 'Target-Q': 1.204336}\n",
            "[*] LOSS: 0.31408632 / Q: {'Eval-Q': 2.153185, 'Target-Q': 2.333518}\n",
            "[*] LOSS: 0.3976587 / Q: {'Eval-Q': 1.854401, 'Target-Q': 1.902321}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.05547025449884996, 'total_reward': 309.79499858338386, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_182\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_182\n",
            "\n",
            "\n",
            "[*] ROUND #183, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.  ,  0.62]), 'NUM': [21, 8]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.055]), 'NUM': [19, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 8902\n",
            "[*] LOSS: 0.16130115 / Q: {'Eval-Q': 2.033394, 'Target-Q': 1.862722}\n",
            "[*] LOSS: 0.25143296 / Q: {'Eval-Q': 2.028965, 'Target-Q': 1.981689}\n",
            "[*] LOSS: 0.12228463 / Q: {'Eval-Q': 1.470998, 'Target-Q': 1.231537}\n",
            "[*] LOSS: 0.07494126 / Q: {'Eval-Q': 1.88314, 'Target-Q': 1.550907}\n",
            "[*] LOSS: 0.14612323 / Q: {'Eval-Q': 1.708252, 'Target-Q': 1.586372}\n",
            "[*] LOSS: 0.30955788 / Q: {'Eval-Q': 2.602845, 'Target-Q': 2.139503}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.016491690463720017, 'total_reward': 269.6149992858991, 'kill': 62}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_183\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_183\n",
            "\n",
            "\n",
            "[*] ROUND #184, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [37, 18]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.043235, -0.005   ]), 'NUM': [34, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.052059, -0.005   ]), 'NUM': [34, 5]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.053485, -0.025   ]), 'NUM': [33, 5]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.044394, -0.005   ]), 'NUM': [33, 5]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.023182, -0.005   ]), 'NUM': [33, 4]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.032273, -0.03    ]), 'NUM': [33, 4]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.044394, -0.005   ]), 'NUM': [33, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 14195\n",
            "[*] LOSS: 0.2710933 / Q: {'Eval-Q': 1.901027, 'Target-Q': 1.872199}\n",
            "[*] LOSS: 0.06244377 / Q: {'Eval-Q': 1.952171, 'Target-Q': 1.884262}\n",
            "[*] LOSS: 0.14358526 / Q: {'Eval-Q': 2.242887, 'Target-Q': 2.077317}\n",
            "[*] LOSS: 0.43161613 / Q: {'Eval-Q': 2.590368, 'Target-Q': 2.497964}\n",
            "[*] LOSS: 0.12514715 / Q: {'Eval-Q': 1.849474, 'Target-Q': 1.759517}\n",
            "[*] LOSS: 0.040463302 / Q: {'Eval-Q': 1.503704, 'Target-Q': 1.280719}\n",
            "[*] LOSS: 0.072434835 / Q: {'Eval-Q': 1.182908, 'Target-Q': 1.179895}\n",
            "[*] LOSS: 0.08529503 / Q: {'Eval-Q': 1.653073, 'Target-Q': 1.458106}\n",
            "[*] LOSS: 0.2980702 / Q: {'Eval-Q': 1.763299, 'Target-Q': 1.925643}\n",
            "\n",
            "[INFO] {'ave_agent_reward': -0.01645097145261443, 'total_reward': -155.4200174389407, 'kill': 61}\n",
            "\n",
            "\n",
            "[*] ROUND #185, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.345357, 0.168269]), 'NUM': [14, 26]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005   ,  0.000263]), 'NUM': [7, 19]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 19]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 19]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 19]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 19]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 19]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 19]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3844\n",
            "[*] LOSS: 0.33127964 / Q: {'Eval-Q': 1.910564, 'Target-Q': 1.750394}\n",
            "[*] LOSS: 0.1986152 / Q: {'Eval-Q': 2.103903, 'Target-Q': 2.148615}\n",
            "[*] LOSS: 0.1805652 / Q: {'Eval-Q': 2.744279, 'Target-Q': 2.502168}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.017168192667308783, 'total_reward': 200.07999913208187, 'kill': 45}\n",
            "\n",
            "\n",
            "[*] ROUND #186, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.003333, 0.003696]), 'NUM': [12, 23]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005   , -0.010882]), 'NUM': [4, 17]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005 , -0.0175]), 'NUM': [3, 16]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005  , -0.02375]), 'NUM': [3, 16]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005 , -0.0175]), 'NUM': [3, 16]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005 , -0.0175]), 'NUM': [3, 16]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005 , -0.0175]), 'NUM': [3, 16]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005 , -0.0175]), 'NUM': [3, 16]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3215\n",
            "[*] LOSS: 0.11657606 / Q: {'Eval-Q': 1.893448, 'Target-Q': 1.801034}\n",
            "[*] LOSS: 0.24127367 / Q: {'Eval-Q': 2.237685, 'Target-Q': 2.109003}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.022651425142868536, 'total_reward': 205.22999885957688, 'kill': 48}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_186\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_186\n",
            "\n",
            "\n",
            "[*] ROUND #187, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1814\n",
            "[*] LOSS: 0.25116894 / Q: {'Eval-Q': 1.627065, 'Target-Q': 1.568448}\n",
            "[*] LOSS: 0.8517047 / Q: {'Eval-Q': 2.396808, 'Target-Q': 2.468487}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.10824745110903304, 'total_reward': 178.34999891836196, 'kill': 38}\n",
            "\n",
            "\n",
            "[*] ROUND #188, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005   , -0.001154]), 'NUM': [3, 26]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2186\n",
            "[*] LOSS: 0.3159907 / Q: {'Eval-Q': 2.174479, 'Target-Q': 1.902039}\n",
            "[*] LOSS: 0.29971987 / Q: {'Eval-Q': 2.347512, 'Target-Q': 2.2914}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.016435066231260353, 'total_reward': 172.6849991409108, 'kill': 38}\n",
            "\n",
            "\n",
            "[*] ROUND #189, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.011667, 0.012647]), 'NUM': [6, 17]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 14]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 14]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 14]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 14]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 14]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 14]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 14]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2965\n",
            "[*] LOSS: 0.28202984 / Q: {'Eval-Q': 1.928459, 'Target-Q': 1.737237}\n",
            "[*] LOSS: 0.49895242 / Q: {'Eval-Q': 3.012562, 'Target-Q': 2.647352}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.018582770179765288, 'total_reward': 224.0799989933148, 'kill': 50}\n",
            "\n",
            "\n",
            "[*] ROUND #190, EPS: 0.91 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.009524,  2.445   ]), 'NUM': [21, 2]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 9150\n",
            "[*] LOSS: 0.5953989 / Q: {'Eval-Q': 3.258904, 'Target-Q': 3.201331}\n",
            "[*] LOSS: 0.19307908 / Q: {'Eval-Q': 2.011043, 'Target-Q': 1.836257}\n",
            "[*] LOSS: 0.14571242 / Q: {'Eval-Q': 1.844766, 'Target-Q': 1.553186}\n",
            "[*] LOSS: 0.17072378 / Q: {'Eval-Q': 2.261757, 'Target-Q': 2.035907}\n",
            "[*] LOSS: 0.40382352 / Q: {'Eval-Q': 1.946539, 'Target-Q': 2.065664}\n",
            "[*] LOSS: 0.24310645 / Q: {'Eval-Q': 1.862892, 'Target-Q': 1.744391}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01623085833023909, 'total_reward': 260.86999901756644, 'kill': 62}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_190\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_190\n",
            "\n",
            "\n",
            "[*] ROUND #191, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.32625, 0.0025 ]), 'NUM': [16, 14]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.014091, -0.005   ]), 'NUM': [11, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5686\n",
            "[*] LOSS: 0.11220851 / Q: {'Eval-Q': 2.378746, 'Target-Q': 2.166282}\n",
            "[*] LOSS: 0.12113552 / Q: {'Eval-Q': 2.246008, 'Target-Q': 2.210171}\n",
            "[*] LOSS: 0.2482868 / Q: {'Eval-Q': 1.609232, 'Target-Q': 1.598633}\n",
            "[*] LOSS: 0.17611594 / Q: {'Eval-Q': 1.517628, 'Target-Q': 1.215868}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.022722460365790266, 'total_reward': 261.7399988891557, 'kill': 60}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_191\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_191\n",
            "\n",
            "\n",
            "[*] ROUND #192, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.000962,  0.828333]), 'NUM': [26, 6]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [24, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [24, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [24, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [24, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [24, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [24, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [24, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 10647\n",
            "[*] LOSS: 0.29045397 / Q: {'Eval-Q': 2.110947, 'Target-Q': 2.351515}\n",
            "[*] LOSS: 0.18126795 / Q: {'Eval-Q': 2.193304, 'Target-Q': 2.293002}\n",
            "[*] LOSS: 0.2897992 / Q: {'Eval-Q': 2.192676, 'Target-Q': 2.041988}\n",
            "[*] LOSS: 0.11786234 / Q: {'Eval-Q': 1.984502, 'Target-Q': 1.787359}\n",
            "[*] LOSS: 0.19470242 / Q: {'Eval-Q': 1.672758, 'Target-Q': 1.642359}\n",
            "[*] LOSS: 0.5254295 / Q: {'Eval-Q': 2.119989, 'Target-Q': 1.919323}\n",
            "[*] LOSS: 0.5009303 / Q: {'Eval-Q': 1.986532, 'Target-Q': 1.796115}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.016287946170156397, 'total_reward': 253.96499942336231, 'kill': 62}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_192\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_192\n",
            "\n",
            "\n",
            "[*] ROUND #193, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.817222, -0.003636]), 'NUM': [18, 11]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 4]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 4]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 4]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 4]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 4]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 4]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 4]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7674\n",
            "[*] LOSS: 0.17748047 / Q: {'Eval-Q': 1.353543, 'Target-Q': 1.321617}\n",
            "[*] LOSS: 0.30479264 / Q: {'Eval-Q': 1.935081, 'Target-Q': 2.075206}\n",
            "[*] LOSS: 0.37264663 / Q: {'Eval-Q': 2.624722, 'Target-Q': 2.272056}\n",
            "[*] LOSS: 0.22137117 / Q: {'Eval-Q': 1.752602, 'Target-Q': 1.780086}\n",
            "[*] LOSS: 0.18487822 / Q: {'Eval-Q': 1.914441, 'Target-Q': 1.696078}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.019318549474740668, 'total_reward': 261.06999905221164, 'kill': 60}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_193\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_193\n",
            "\n",
            "\n",
            "[*] ROUND #194, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.702143, 0.002333]), 'NUM': [14, 15]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 10]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 10]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 10]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 9]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 9]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 9]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 9]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5273\n",
            "[*] LOSS: 0.053077135 / Q: {'Eval-Q': 1.358599, 'Target-Q': 1.314965}\n",
            "[*] LOSS: 0.16863048 / Q: {'Eval-Q': 1.817345, 'Target-Q': 1.834677}\n",
            "[*] LOSS: 0.18770319 / Q: {'Eval-Q': 1.824698, 'Target-Q': 1.644942}\n",
            "[*] LOSS: 0.11368941 / Q: {'Eval-Q': 1.559323, 'Target-Q': 1.573813}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.021850159769874065, 'total_reward': 252.30999890062958, 'kill': 55}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_194\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_194\n",
            "\n",
            "\n",
            "[*] ROUND #195, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.002692, 0.014048]), 'NUM': [13, 21]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 18]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 18]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 18]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 18]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 18]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 18]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 18]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3514\n",
            "[*] LOSS: 0.17713755 / Q: {'Eval-Q': 1.643282, 'Target-Q': 1.569489}\n",
            "[*] LOSS: 0.09873731 / Q: {'Eval-Q': 1.713376, 'Target-Q': 1.734291}\n",
            "[*] LOSS: 0.19836234 / Q: {'Eval-Q': 2.160473, 'Target-Q': 1.769685}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.015008921213471168, 'total_reward': 207.82999878097326, 'kill': 46}\n",
            "\n",
            "\n",
            "[*] ROUND #196, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.003077, 0.371923]), 'NUM': [13, 13]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 7]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 7]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 7]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 7]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 7]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4367\n",
            "[*] LOSS: 0.19731295 / Q: {'Eval-Q': 2.582728, 'Target-Q': 2.548614}\n",
            "[*] LOSS: 0.8168881 / Q: {'Eval-Q': 2.270215, 'Target-Q': 2.554231}\n",
            "[*] LOSS: 0.2181356 / Q: {'Eval-Q': 1.939535, 'Target-Q': 1.780561}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.029123193607224405, 'total_reward': 267.0499987611547, 'kill': 57}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_196\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_196\n",
            "\n",
            "\n",
            "[*] ROUND #197, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 4.67727e-01, -4.35000e-04]), 'NUM': [11, 23]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 16]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3204\n",
            "[*] LOSS: 0.143915 / Q: {'Eval-Q': 2.086988, 'Target-Q': 2.126247}\n",
            "[*] LOSS: 0.1254095 / Q: {'Eval-Q': 2.19769, 'Target-Q': 1.866654}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.018109083175688503, 'total_reward': 194.0849984958768, 'kill': 48}\n",
            "\n",
            "\n",
            "[*] ROUND #198, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.003333, -0.000833]), 'NUM': [12, 24]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 18]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 17]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 17]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 17]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 17]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 17]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 17]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3124\n",
            "[*] LOSS: 0.14703228 / Q: {'Eval-Q': 2.248298, 'Target-Q': 2.175516}\n",
            "[*] LOSS: 0.054354854 / Q: {'Eval-Q': 1.371808, 'Target-Q': 1.376011}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.026718838587900754, 'total_reward': 191.78499857801944, 'kill': 47}\n",
            "\n",
            "\n",
            "[*] ROUND #199, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.695   , 0.006053]), 'NUM': [14, 19]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 12]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 12]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 12]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 12]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 12]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 12]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 12]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3881\n",
            "[*] LOSS: 0.2766277 / Q: {'Eval-Q': 2.346934, 'Target-Q': 2.422208}\n",
            "[*] LOSS: 0.28189075 / Q: {'Eval-Q': 2.676893, 'Target-Q': 2.536081}\n",
            "[*] LOSS: 0.24097806 / Q: {'Eval-Q': 2.262949, 'Target-Q': 2.168478}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.023062757588446548, 'total_reward': 235.289998521097, 'kill': 52}\n",
            "\n",
            "\n",
            "[*] ROUND #200, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.0075  , 0.000263]), 'NUM': [8, 19]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 14]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 14]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 14]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 14]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 14]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 14]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 14]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3729\n",
            "[*] LOSS: 0.23202449 / Q: {'Eval-Q': 2.336619, 'Target-Q': 2.169376}\n",
            "[*] LOSS: 0.44933844 / Q: {'Eval-Q': 2.500969, 'Target-Q': 2.521064}\n",
            "[*] LOSS: 0.12930588 / Q: {'Eval-Q': 2.12056, 'Target-Q': 2.096655}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.021889178540872148, 'total_reward': 226.74999885447323, 'kill': 50}\n",
            "\n",
            "\n",
            "[*] ROUND #201, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.01025 , 0.379615]), 'NUM': [20, 13]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 7]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 7]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 7]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 7]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 7]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 8049\n",
            "[*] LOSS: 0.1863562 / Q: {'Eval-Q': 2.004124, 'Target-Q': 1.937815}\n",
            "[*] LOSS: 0.18862072 / Q: {'Eval-Q': 1.69923, 'Target-Q': 1.572003}\n",
            "[*] LOSS: 0.059750468 / Q: {'Eval-Q': 1.232871, 'Target-Q': 1.161457}\n",
            "[*] LOSS: 0.13506979 / Q: {'Eval-Q': 2.130976, 'Target-Q': 2.072175}\n",
            "[*] LOSS: 0.18607317 / Q: {'Eval-Q': 1.659286, 'Target-Q': 1.679559}\n",
            "[*] LOSS: 0.2195198 / Q: {'Eval-Q': 2.528432, 'Target-Q': 2.179968}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.018028636955325528, 'total_reward': 242.68999912776053, 'kill': 57}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_201\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_201\n",
            "\n",
            "\n",
            "[*] ROUND #202, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.028333, 0.0075  ]), 'NUM': [6, 16]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 13]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 13]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005   ,  0.002692]), 'NUM': [3, 13]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.055, -0.005]), 'NUM': [2, 13]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 13]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.105, -0.005]), 'NUM': [1, 13]}\n",
            "> step #400, info: {'Ave-Reward': array([ 0.095, -0.005]), 'NUM': [1, 13]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2635\n",
            "[*] LOSS: 0.3226819 / Q: {'Eval-Q': 2.338242, 'Target-Q': 2.219851}\n",
            "[*] LOSS: 0.24171115 / Q: {'Eval-Q': 2.278477, 'Target-Q': 2.329014}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.019078620913046365, 'total_reward': 211.73999823909253, 'kill': 51}\n",
            "\n",
            "\n",
            "[*] ROUND #203, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.252895, 0.006389]), 'NUM': [19, 18]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 6]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 6]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 6]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 6]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 6]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 6]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 6]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5822\n",
            "[*] LOSS: 0.45118573 / Q: {'Eval-Q': 2.418918, 'Target-Q': 2.493985}\n",
            "[*] LOSS: 0.2312318 / Q: {'Eval-Q': 2.047651, 'Target-Q': 1.850017}\n",
            "[*] LOSS: 0.415415 / Q: {'Eval-Q': 2.221301, 'Target-Q': 2.28355}\n",
            "[*] LOSS: 0.12588729 / Q: {'Eval-Q': 2.553047, 'Target-Q': 2.246922}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01941065634402983, 'total_reward': 239.95999852009118, 'kill': 58}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_203\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_203\n",
            "\n",
            "\n",
            "[*] ROUND #204, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.010385, -0.005   ]), 'NUM': [13, 30]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.811667, -0.008958]), 'NUM': [6, 24]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005   , -0.000652]), 'NUM': [5, 23]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 23]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 23]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 23]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 23]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 23]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3392\n",
            "[*] LOSS: 0.2589636 / Q: {'Eval-Q': 1.59337, 'Target-Q': 1.669942}\n",
            "[*] LOSS: 0.23673055 / Q: {'Eval-Q': 2.241944, 'Target-Q': 2.172308}\n",
            "[*] LOSS: 0.23523366 / Q: {'Eval-Q': 2.38482, 'Target-Q': 2.381889}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.016791288543266402, 'total_reward': 191.84999879356474, 'kill': 41}\n",
            "\n",
            "\n",
            "[*] ROUND #205, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.020625, 0.17    ]), 'NUM': [8, 28]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 25]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 25]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 25]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 25]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 25]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 25]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.009]), 'NUM': [2, 25]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2624\n",
            "[*] LOSS: 0.16427797 / Q: {'Eval-Q': 2.226182, 'Target-Q': 2.093899}\n",
            "[*] LOSS: 0.9739729 / Q: {'Eval-Q': 2.294778, 'Target-Q': 2.459439}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.014845317498800616, 'total_reward': 169.98999902047217, 'kill': 39}\n",
            "\n",
            "\n",
            "[*] ROUND #206, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011129,  0.985   ]), 'NUM': [31, 10]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.009348, -0.005   ]), 'NUM': [23, 4]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005   ,  0.028333]), 'NUM': [20, 3]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 3]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 3]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 3]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 3]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005   ,  0.028333]), 'NUM': [19, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 9440\n",
            "[*] LOSS: 0.5849733 / Q: {'Eval-Q': 2.239986, 'Target-Q': 2.320527}\n",
            "[*] LOSS: 0.042754672 / Q: {'Eval-Q': 1.174632, 'Target-Q': 1.086029}\n",
            "[*] LOSS: 0.37510794 / Q: {'Eval-Q': 3.021187, 'Target-Q': 2.923768}\n",
            "[*] LOSS: 0.30457002 / Q: {'Eval-Q': 1.45594, 'Target-Q': 1.418641}\n",
            "[*] LOSS: 0.50329643 / Q: {'Eval-Q': 2.453078, 'Target-Q': 2.287727}\n",
            "[*] LOSS: 0.1319959 / Q: {'Eval-Q': 1.759285, 'Target-Q': 1.693514}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.011749213353241292, 'total_reward': 234.1249979576096, 'kill': 61}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_206\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_206\n",
            "\n",
            "\n",
            "[*] ROUND #207, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.018462, 0.175769]), 'NUM': [13, 26]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 15]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 15]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 15]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 15]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 15]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 15]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 15]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3512\n",
            "[*] LOSS: 0.2094334 / Q: {'Eval-Q': 1.72618, 'Target-Q': 1.656812}\n",
            "[*] LOSS: 0.21025267 / Q: {'Eval-Q': 1.805055, 'Target-Q': 1.684114}\n",
            "[*] LOSS: 0.5024845 / Q: {'Eval-Q': 1.64018, 'Target-Q': 1.822226}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02483788528511778, 'total_reward': 225.0449989112094, 'kill': 49}\n",
            "\n",
            "\n",
            "[*] ROUND #208, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 11]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013333, -0.005   ]), 'NUM': [12, 8]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.013333, -0.005   ]), 'NUM': [12, 7]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 7]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.014091, -0.005   ]), 'NUM': [11, 7]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.014091, -0.005   ]), 'NUM': [11, 7]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 7]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6193\n",
            "[*] LOSS: 0.22578222 / Q: {'Eval-Q': 2.120681, 'Target-Q': 2.094405}\n",
            "[*] LOSS: 0.14321356 / Q: {'Eval-Q': 1.987812, 'Target-Q': 1.846171}\n",
            "[*] LOSS: 0.1475771 / Q: {'Eval-Q': 1.660002, 'Target-Q': 1.547181}\n",
            "[*] LOSS: 0.13172854 / Q: {'Eval-Q': 2.535224, 'Target-Q': 2.583978}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.012683456991761438, 'total_reward': 234.99999771639705, 'kill': 57}\n",
            "\n",
            "\n",
            "[*] ROUND #209, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.004706,  0.173571]), 'NUM': [17, 28]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.03 , -0.005]), 'NUM': [4, 19]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 19]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 19]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 19]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 19]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 19]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 19]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3421\n",
            "[*] LOSS: 0.12767178 / Q: {'Eval-Q': 1.87643, 'Target-Q': 1.847875}\n",
            "[*] LOSS: 0.06892711 / Q: {'Eval-Q': 1.96372, 'Target-Q': 2.016572}\n",
            "[*] LOSS: 0.19732773 / Q: {'Eval-Q': 2.265461, 'Target-Q': 1.987765}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.021297547217375187, 'total_reward': 209.09999866224825, 'kill': 45}\n",
            "\n",
            "\n",
            "[*] ROUND #210, EPS: 0.90 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.143548, 0.4755  ]), 'NUM': [31, 10]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2946\n",
            "[*] LOSS: 0.5439224 / Q: {'Eval-Q': 2.5349, 'Target-Q': 2.484268}\n",
            "[*] LOSS: 0.46527675 / Q: {'Eval-Q': 3.457489, 'Target-Q': 3.059753}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.0939619928004388, 'total_reward': 265.94999620411545, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_210\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_210\n",
            "\n",
            "\n",
            "[*] ROUND #211, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.468182, 0.1952  ]), 'NUM': [11, 25]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 21]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 21]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 20]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.02 , -0.005]), 'NUM': [4, 20]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 20]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 20]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 20]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3561\n",
            "[*] LOSS: 0.1608662 / Q: {'Eval-Q': 2.683926, 'Target-Q': 2.409029}\n",
            "[*] LOSS: 0.5226276 / Q: {'Eval-Q': 2.032951, 'Target-Q': 1.770562}\n",
            "[*] LOSS: 0.18397121 / Q: {'Eval-Q': 2.68195, 'Target-Q': 2.423755}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.015204549876593039, 'total_reward': 180.59499812684953, 'kill': 44}\n",
            "\n",
            "\n",
            "[*] ROUND #212, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.321667, -0.010937]), 'NUM': [15, 16]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.014091, -0.005   ]), 'NUM': [11, 11]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.014091, -0.005   ]), 'NUM': [11, 11]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.014091, -0.005   ]), 'NUM': [11, 11]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.014091, -0.005   ]), 'NUM': [11, 11]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.014091, -0.005   ]), 'NUM': [11, 11]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.014091, -0.005   ]), 'NUM': [11, 11]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.014091, -0.005   ]), 'NUM': [11, 11]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5961\n",
            "[*] LOSS: 0.2947349 / Q: {'Eval-Q': 2.743086, 'Target-Q': 2.619662}\n",
            "[*] LOSS: 0.20022628 / Q: {'Eval-Q': 2.76041, 'Target-Q': 2.572424}\n",
            "[*] LOSS: 0.52265024 / Q: {'Eval-Q': 2.659972, 'Target-Q': 2.846781}\n",
            "[*] LOSS: 0.25034377 / Q: {'Eval-Q': 2.909535, 'Target-Q': 2.810195}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.009049054109871562, 'total_reward': 184.65999716613442, 'kill': 53}\n",
            "\n",
            "\n",
            "[*] ROUND #213, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.253421, 0.424375]), 'NUM': [19, 24]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3330\n",
            "[*] LOSS: 0.13431202 / Q: {'Eval-Q': 1.645774, 'Target-Q': 1.607093}\n",
            "[*] LOSS: 0.13098001 / Q: {'Eval-Q': 1.814507, 'Target-Q': 1.867946}\n",
            "[*] LOSS: 0.14700931 / Q: {'Eval-Q': 1.851124, 'Target-Q': 1.959821}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.019637706053401604, 'total_reward': 226.1549987597391, 'kill': 48}\n",
            "\n",
            "\n",
            "[*] ROUND #214, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.0055, 0.195 ]), 'NUM': [10, 25]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 22]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 22]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.055, -0.005]), 'NUM': [2, 22]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.055, -0.005]), 'NUM': [2, 22]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 22]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 22]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 22]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2647\n",
            "[*] LOSS: 0.31597802 / Q: {'Eval-Q': 2.99217, 'Target-Q': 2.615884}\n",
            "[*] LOSS: 0.35185122 / Q: {'Eval-Q': 2.257869, 'Target-Q': 2.117959}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.011069242605332392, 'total_reward': 165.27999825868756, 'kill': 42}\n",
            "\n",
            "\n",
            "[*] ROUND #215, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.00525 , 0.428333]), 'NUM': [20, 12]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 10]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [14, 10]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 10]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 10]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 10]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 10]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [12, 10]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6844\n",
            "[*] LOSS: 0.15540765 / Q: {'Eval-Q': 2.556232, 'Target-Q': 2.523232}\n",
            "[*] LOSS: 0.38550398 / Q: {'Eval-Q': 2.531984, 'Target-Q': 2.398134}\n",
            "[*] LOSS: 0.18814905 / Q: {'Eval-Q': 2.259557, 'Target-Q': 2.229947}\n",
            "[*] LOSS: 0.12451842 / Q: {'Eval-Q': 1.965915, 'Target-Q': 1.815206}\n",
            "[*] LOSS: 0.15916044 / Q: {'Eval-Q': 2.978208, 'Target-Q': 2.989383}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.014249497971188851, 'total_reward': 233.63999910838902, 'kill': 54}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_215\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_215\n",
            "\n",
            "\n",
            "[*] ROUND #216, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005   ,  0.006765]), 'NUM': [15, 17]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.021667, -0.005   ]), 'NUM': [12, 10]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 10]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 10]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 10]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 10]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 10]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 10]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6016\n",
            "[*] LOSS: 0.30392176 / Q: {'Eval-Q': 1.491471, 'Target-Q': 1.5244}\n",
            "[*] LOSS: 0.1774014 / Q: {'Eval-Q': 2.375336, 'Target-Q': 2.07893}\n",
            "[*] LOSS: 0.40928057 / Q: {'Eval-Q': 2.238266, 'Target-Q': 2.149634}\n",
            "[*] LOSS: 0.20139782 / Q: {'Eval-Q': 2.250492, 'Target-Q': 2.305623}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.017282873091509256, 'total_reward': 224.284998674877, 'kill': 54}\n",
            "\n",
            "\n",
            "[*] ROUND #217, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.023571, -0.010263]), 'NUM': [7, 19]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.024,  0.345]), 'NUM': [5, 14]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 14]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 14]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 14]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 14]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 14]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005   , -0.019286]), 'NUM': [4, 14]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3257\n",
            "[*] LOSS: 0.3330557 / Q: {'Eval-Q': 2.404804, 'Target-Q': 2.392522}\n",
            "[*] LOSS: 0.4048874 / Q: {'Eval-Q': 1.91998, 'Target-Q': 1.920674}\n",
            "[*] LOSS: 0.20736782 / Q: {'Eval-Q': 2.092929, 'Target-Q': 2.175338}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.030005073357553404, 'total_reward': 227.71499893441796, 'kill': 50}\n",
            "\n",
            "\n",
            "[*] ROUND #218, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 15]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.0145,  0.595 ]), 'NUM': [10, 8]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 8]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005   , -0.038333]), 'NUM': [6, 6]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005   , -0.038333]), 'NUM': [6, 6]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005   , -0.038333]), 'NUM': [6, 6]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005   , -0.038333]), 'NUM': [6, 6]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005   , -0.038333]), 'NUM': [6, 6]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4761\n",
            "[*] LOSS: 0.47989962 / Q: {'Eval-Q': 2.38069, 'Target-Q': 2.244941}\n",
            "[*] LOSS: 0.1774142 / Q: {'Eval-Q': 2.62397, 'Target-Q': 2.185827}\n",
            "[*] LOSS: 0.9824048 / Q: {'Eval-Q': 2.032723, 'Target-Q': 2.355957}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.021677707199537006, 'total_reward': 264.4849987151101, 'kill': 58}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_218\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_218\n",
            "\n",
            "\n",
            "[*] ROUND #219, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 28]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1887\n",
            "[*] LOSS: 0.33558017 / Q: {'Eval-Q': 2.120268, 'Target-Q': 2.043541}\n",
            "[*] LOSS: 0.14844078 / Q: {'Eval-Q': 2.039598, 'Target-Q': 2.09071}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.13039821149580422, 'total_reward': 178.08499893639237, 'kill': 38}\n",
            "\n",
            "\n",
            "[*] ROUND #220, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.006111, -0.001154]), 'NUM': [9, 26]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 20]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 20]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 20]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 20]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 20]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 20]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 20]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3358\n",
            "[*] LOSS: 0.59306115 / Q: {'Eval-Q': 1.829665, 'Target-Q': 1.733986}\n",
            "[*] LOSS: 0.16738313 / Q: {'Eval-Q': 2.414485, 'Target-Q': 2.421275}\n",
            "[*] LOSS: 0.26379076 / Q: {'Eval-Q': 2.754512, 'Target-Q': 2.358315}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01443450298518842, 'total_reward': 182.20999891776592, 'kill': 44}\n",
            "\n",
            "\n",
            "[*] ROUND #221, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 27]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1918\n",
            "[*] LOSS: 0.19858178 / Q: {'Eval-Q': 1.99447, 'Target-Q': 1.562091}\n",
            "[*] LOSS: 0.2948824 / Q: {'Eval-Q': 2.946788, 'Target-Q': 2.848531}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09204318353830702, 'total_reward': 177.92999892868102, 'kill': 37}\n",
            "\n",
            "\n",
            "[*] ROUND #222, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.004091, -0.000455]), 'NUM': [11, 22]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 16]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3173\n",
            "[*] LOSS: 0.16458964 / Q: {'Eval-Q': 2.805174, 'Target-Q': 2.200202}\n",
            "[*] LOSS: 0.77913696 / Q: {'Eval-Q': 2.319393, 'Target-Q': 2.49887}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.026343973852041, 'total_reward': 221.13999868929386, 'kill': 48}\n",
            "\n",
            "\n",
            "[*] ROUND #223, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.001296,  0.005   ]), 'NUM': [27, 10]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2811\n",
            "[*] LOSS: 0.35797617 / Q: {'Eval-Q': 2.344351, 'Target-Q': 2.287067}\n",
            "[*] LOSS: 0.26687562 / Q: {'Eval-Q': 2.796692, 'Target-Q': 2.547349}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11829586462926149, 'total_reward': 308.13499798905104, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_223\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_223\n",
            "\n",
            "\n",
            "[*] ROUND #224, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.013636,  0.158333]), 'NUM': [11, 30]}\n",
            "> step #100, info: {'Ave-Reward': array([0.015, 0.   ]), 'NUM': [5, 20]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 19]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005   , -0.015526]), 'NUM': [5, 19]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 19]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005   , -0.010263]), 'NUM': [5, 19]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 19]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 19]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3662\n",
            "[*] LOSS: 0.21163931 / Q: {'Eval-Q': 2.298434, 'Target-Q': 2.358739}\n",
            "[*] LOSS: 0.12705322 / Q: {'Eval-Q': 2.142902, 'Target-Q': 2.099544}\n",
            "[*] LOSS: 0.29308814 / Q: {'Eval-Q': 3.306632, 'Target-Q': 3.305306}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.029438317848522518, 'total_reward': 182.48499917145818, 'kill': 45}\n",
            "\n",
            "\n",
            "[*] ROUND #225, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.000833, 0.212391]), 'NUM': [18, 23]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 15]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 15]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005   ,  0.001667]), 'NUM': [10, 15]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 14]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 14]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 14]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 14]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5745\n",
            "[*] LOSS: 0.18446273 / Q: {'Eval-Q': 1.984253, 'Target-Q': 1.919539}\n",
            "[*] LOSS: 0.35869282 / Q: {'Eval-Q': 2.770718, 'Target-Q': 2.281257}\n",
            "[*] LOSS: 0.29547852 / Q: {'Eval-Q': 2.732523, 'Target-Q': 2.671561}\n",
            "[*] LOSS: 0.16101034 / Q: {'Eval-Q': 2.440765, 'Target-Q': 2.37149}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.019427248201280195, 'total_reward': 230.24499905109406, 'kill': 50}\n",
            "\n",
            "\n",
            "[*] ROUND #226, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.002895, 0.025   ]), 'NUM': [38, 10]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 11718\n",
            "[*] LOSS: 0.32487157 / Q: {'Eval-Q': 1.582102, 'Target-Q': 1.610992}\n",
            "[*] LOSS: 0.16280708 / Q: {'Eval-Q': 2.54236, 'Target-Q': 2.314701}\n",
            "[*] LOSS: 0.24555254 / Q: {'Eval-Q': 2.033902, 'Target-Q': 1.954612}\n",
            "[*] LOSS: 0.13382524 / Q: {'Eval-Q': 2.444034, 'Target-Q': 2.519614}\n",
            "[*] LOSS: 0.42760283 / Q: {'Eval-Q': 3.051074, 'Target-Q': 2.964779}\n",
            "[*] LOSS: 0.12722352 / Q: {'Eval-Q': 2.157278, 'Target-Q': 1.97576}\n",
            "[*] LOSS: 0.25196886 / Q: {'Eval-Q': 2.107059, 'Target-Q': 1.873423}\n",
            "[*] LOSS: 0.24292828 / Q: {'Eval-Q': 2.294893, 'Target-Q': 2.074451}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.012100764705571601, 'total_reward': 252.29999898094684, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_226\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_226\n",
            "\n",
            "\n",
            "[*] ROUND #227, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.006429, 0.011667]), 'NUM': [35, 12]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 8]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [27, 8]}\n",
            "> step #200, info: {'Ave-Reward': array([ 0.176481, -0.018571]), 'NUM': [27, 7]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [25, 6]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [25, 6]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [25, 6]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [25, 6]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 11745\n",
            "[*] LOSS: 0.16681802 / Q: {'Eval-Q': 2.884808, 'Target-Q': 2.86807}\n",
            "[*] LOSS: 0.46233493 / Q: {'Eval-Q': 2.901732, 'Target-Q': 2.735435}\n",
            "[*] LOSS: 0.2033328 / Q: {'Eval-Q': 2.222151, 'Target-Q': 2.303753}\n",
            "[*] LOSS: 0.17060298 / Q: {'Eval-Q': 2.438459, 'Target-Q': 2.125404}\n",
            "[*] LOSS: 0.3080162 / Q: {'Eval-Q': 1.734665, 'Target-Q': 1.774932}\n",
            "[*] LOSS: 0.175764 / Q: {'Eval-Q': 1.905369, 'Target-Q': 1.951968}\n",
            "[*] LOSS: 0.17402464 / Q: {'Eval-Q': 2.822729, 'Target-Q': 2.836492}\n",
            "[*] LOSS: 0.45865333 / Q: {'Eval-Q': 2.025286, 'Target-Q': 1.913935}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.010953889949342988, 'total_reward': 235.36999925598502, 'kill': 58}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_227\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_227\n",
            "\n",
            "\n",
            "[*] ROUND #228, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.007   , 0.002143]), 'NUM': [25, 14]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005   ,  0.009286]), 'NUM': [15, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [14, 7]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [14, 7]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 7]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 7]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 7]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7097\n",
            "[*] LOSS: 0.2795338 / Q: {'Eval-Q': 2.228408, 'Target-Q': 2.163195}\n",
            "[*] LOSS: 0.38521427 / Q: {'Eval-Q': 1.649394, 'Target-Q': 1.612129}\n",
            "[*] LOSS: 0.14368358 / Q: {'Eval-Q': 2.325699, 'Target-Q': 2.275988}\n",
            "[*] LOSS: 0.34303424 / Q: {'Eval-Q': 2.042787, 'Target-Q': 1.913976}\n",
            "[*] LOSS: 0.2297489 / Q: {'Eval-Q': 2.126285, 'Target-Q': 2.209175}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.015227115624216353, 'total_reward': 244.06999884732068, 'kill': 57}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_228\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_228\n",
            "\n",
            "\n",
            "[*] ROUND #229, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.31375 , 0.000526]), 'NUM': [16, 19]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 11]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 11]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 10]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 10]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 10]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 10]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 10]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4698\n",
            "[*] LOSS: 0.2565114 / Q: {'Eval-Q': 2.007172, 'Target-Q': 1.919773}\n",
            "[*] LOSS: 0.38394105 / Q: {'Eval-Q': 1.733734, 'Target-Q': 1.780561}\n",
            "[*] LOSS: 0.1898062 / Q: {'Eval-Q': 2.121371, 'Target-Q': 2.096673}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.020326953630823538, 'total_reward': 218.09499843697995, 'kill': 54}\n",
            "\n",
            "\n",
            "[*] ROUND #230, EPS: 0.89 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.177143, -0.004615]), 'NUM': [28, 13]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.014091, -0.005   ]), 'NUM': [22, 8]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.009762, -0.005   ]), 'NUM': [21, 8]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.009762, -0.005   ]), 'NUM': [21, 8]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.01 , -0.005]), 'NUM': [20, 8]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.01 , -0.005]), 'NUM': [20, 8]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.01 , -0.005]), 'NUM': [20, 8]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.01 , -0.005]), 'NUM': [20, 8]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 9725\n",
            "[*] LOSS: 0.19987532 / Q: {'Eval-Q': 1.994977, 'Target-Q': 1.862999}\n",
            "[*] LOSS: 0.26923794 / Q: {'Eval-Q': 2.3261, 'Target-Q': 2.0683}\n",
            "[*] LOSS: 0.2800215 / Q: {'Eval-Q': 1.757103, 'Target-Q': 1.670078}\n",
            "[*] LOSS: 0.10896375 / Q: {'Eval-Q': 1.582105, 'Target-Q': 1.593223}\n",
            "[*] LOSS: 0.14806542 / Q: {'Eval-Q': 2.029281, 'Target-Q': 1.89106}\n",
            "[*] LOSS: 0.1263696 / Q: {'Eval-Q': 1.525867, 'Target-Q': 1.494326}\n",
            "[*] LOSS: 0.15771937 / Q: {'Eval-Q': 1.862537, 'Target-Q': 1.816155}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.007305488244411724, 'total_reward': 191.7949974136427, 'kill': 56}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_230\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_230\n",
            "\n",
            "\n",
            "[*] ROUND #231, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.015143, 0.289444]), 'NUM': [35, 18]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 8]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 8]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 8]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 8]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 8]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 8]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 8]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 9092\n",
            "[*] LOSS: 0.08344522 / Q: {'Eval-Q': 1.734666, 'Target-Q': 1.529191}\n",
            "[*] LOSS: 0.14824551 / Q: {'Eval-Q': 1.106134, 'Target-Q': 1.13081}\n",
            "[*] LOSS: 0.4650623 / Q: {'Eval-Q': 1.862344, 'Target-Q': 1.707094}\n",
            "[*] LOSS: 0.21435842 / Q: {'Eval-Q': 1.776556, 'Target-Q': 1.508951}\n",
            "[*] LOSS: 0.105956554 / Q: {'Eval-Q': 0.906373, 'Target-Q': 0.776105}\n",
            "[*] LOSS: 0.13763283 / Q: {'Eval-Q': 1.472066, 'Target-Q': 1.476733}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.010797157073288528, 'total_reward': 218.06999864522368, 'kill': 56}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_231\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_231\n",
            "\n",
            "\n",
            "[*] ROUND #232, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.015556,  0.172115]), 'NUM': [9, 26]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.024   ,  0.217727]), 'NUM': [5, 22]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 21]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 21]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 21]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 21]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 21]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 21]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3210\n",
            "[*] LOSS: 0.29804835 / Q: {'Eval-Q': 2.424534, 'Target-Q': 2.277877}\n",
            "[*] LOSS: 0.09460747 / Q: {'Eval-Q': 1.947333, 'Target-Q': 1.632007}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.016049651621283207, 'total_reward': 162.05499878525734, 'kill': 43}\n",
            "\n",
            "\n",
            "[*] ROUND #233, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.021667,  0.000882]), 'NUM': [6, 34]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 32]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 32]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2115\n",
            "[*] LOSS: 0.11007153 / Q: {'Eval-Q': 1.595126, 'Target-Q': 1.680303}\n",
            "[*] LOSS: 0.19600374 / Q: {'Eval-Q': 1.738343, 'Target-Q': 1.509632}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03182793750284102, 'total_reward': 132.7449991637841, 'kill': 32}\n",
            "\n",
            "\n",
            "[*] ROUND #234, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005   , -0.000455]), 'NUM': [21, 44]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.012143, -0.005   ]), 'NUM': [14, 37]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [14, 35]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [14, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005   , -0.008125]), 'NUM': [13, 32]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 30]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005   , -0.011667]), 'NUM': [13, 30]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 30]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6954\n",
            "[*] LOSS: 0.43586266 / Q: {'Eval-Q': 2.074474, 'Target-Q': 2.196316}\n",
            "[*] LOSS: 0.29188326 / Q: {'Eval-Q': 1.331141, 'Target-Q': 1.448664}\n",
            "[*] LOSS: 0.39653334 / Q: {'Eval-Q': 1.490882, 'Target-Q': 1.582152}\n",
            "[*] LOSS: 0.13588095 / Q: {'Eval-Q': 1.195284, 'Target-Q': 1.240085}\n",
            "[*] LOSS: 0.17591856 / Q: {'Eval-Q': 0.930577, 'Target-Q': 0.976887}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.012288482219570422, 'total_reward': 124.884999377653, 'kill': 34}\n",
            "\n",
            "\n",
            "[*] ROUND #235, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.006765, 0.045   ]), 'NUM': [34, 10]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [30, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [30, 3]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [30, 3]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [30, 3]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [30, 3]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [30, 3]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [30, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 12991\n",
            "[*] LOSS: 0.2694775 / Q: {'Eval-Q': 1.59376, 'Target-Q': 1.489269}\n",
            "[*] LOSS: 0.29237327 / Q: {'Eval-Q': 1.370521, 'Target-Q': 1.103259}\n",
            "[*] LOSS: 0.12116457 / Q: {'Eval-Q': 1.328378, 'Target-Q': 1.432163}\n",
            "[*] LOSS: 0.022907086 / Q: {'Eval-Q': 1.240439, 'Target-Q': 1.235138}\n",
            "[*] LOSS: 0.102455445 / Q: {'Eval-Q': 1.477281, 'Target-Q': 1.443353}\n",
            "[*] LOSS: 0.07083013 / Q: {'Eval-Q': 1.163177, 'Target-Q': 1.017678}\n",
            "[*] LOSS: 0.061626565 / Q: {'Eval-Q': 0.905169, 'Target-Q': 0.827387}\n",
            "[*] LOSS: 0.08885118 / Q: {'Eval-Q': 0.819058, 'Target-Q': 0.812354}\n",
            "[*] LOSS: 0.26945817 / Q: {'Eval-Q': 1.761342, 'Target-Q': 1.79814}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.0117158029952943, 'total_reward': 231.91499940678477, 'kill': 61}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_235\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_235\n",
            "\n",
            "\n",
            "[*] ROUND #236, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.3525  , 0.143636]), 'NUM': [14, 33]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 25]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 25]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 25]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 25]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 25]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 25]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 25]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3362\n",
            "[*] LOSS: 0.056698352 / Q: {'Eval-Q': 1.29815, 'Target-Q': 1.278917}\n",
            "[*] LOSS: 0.09443198 / Q: {'Eval-Q': 0.93089, 'Target-Q': 0.881855}\n",
            "[*] LOSS: 0.13016663 / Q: {'Eval-Q': 1.109006, 'Target-Q': 1.057318}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.023075299132805198, 'total_reward': 175.4899990838021, 'kill': 39}\n",
            "\n",
            "\n",
            "[*] ROUND #237, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.288871, 0.695714]), 'NUM': [31, 14]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 5]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 5]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 5]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 5]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 5]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 5]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 8704\n",
            "[*] LOSS: 0.21321073 / Q: {'Eval-Q': 1.045293, 'Target-Q': 1.124326}\n",
            "[*] LOSS: 0.19476692 / Q: {'Eval-Q': 1.525044, 'Target-Q': 1.487595}\n",
            "[*] LOSS: 0.05505885 / Q: {'Eval-Q': 1.017909, 'Target-Q': 0.953232}\n",
            "[*] LOSS: 0.1641569 / Q: {'Eval-Q': 1.254232, 'Target-Q': 1.19919}\n",
            "[*] LOSS: 0.2719037 / Q: {'Eval-Q': 1.328839, 'Target-Q': 1.40092}\n",
            "[*] LOSS: 0.2980071 / Q: {'Eval-Q': 1.189918, 'Target-Q': 1.070257}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01397440301273186, 'total_reward': 256.10999891906977, 'kill': 59}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_237\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_237\n",
            "\n",
            "\n",
            "[*] ROUND #238, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.001897, -0.005   ]), 'NUM': [29, 10]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [25, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [25, 5]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [25, 5]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [25, 5]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [25, 5]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [25, 5]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [25, 5]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 11130\n",
            "[*] LOSS: 0.049542118 / Q: {'Eval-Q': 1.070172, 'Target-Q': 1.033847}\n",
            "[*] LOSS: 0.40034673 / Q: {'Eval-Q': 1.571152, 'Target-Q': 1.542901}\n",
            "[*] LOSS: 0.087207936 / Q: {'Eval-Q': 1.219381, 'Target-Q': 1.372134}\n",
            "[*] LOSS: 0.08531387 / Q: {'Eval-Q': 0.918549, 'Target-Q': 0.931294}\n",
            "[*] LOSS: 0.22214535 / Q: {'Eval-Q': 1.198537, 'Target-Q': 1.315122}\n",
            "[*] LOSS: 0.09592064 / Q: {'Eval-Q': 1.151094, 'Target-Q': 1.176194}\n",
            "[*] LOSS: 0.08441133 / Q: {'Eval-Q': 1.203559, 'Target-Q': 1.22364}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.012490403249478811, 'total_reward': 244.244999374263, 'kill': 59}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_238\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_238\n",
            "\n",
            "\n",
            "[*] ROUND #239, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.140455, 0.016   ]), 'NUM': [33, 5]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2737\n",
            "[*] LOSS: 0.041058958 / Q: {'Eval-Q': 0.804661, 'Target-Q': 0.743525}\n",
            "[*] LOSS: 0.072829984 / Q: {'Eval-Q': 1.401404, 'Target-Q': 1.222832}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.10993127959330273, 'total_reward': 290.3849983345717, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_239\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_239\n",
            "\n",
            "\n",
            "[*] ROUND #240, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.2075  , 0.011944]), 'NUM': [24, 18]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [14, 6]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [14, 6]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [14, 6]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [14, 6]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [14, 6]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [14, 6]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [14, 6]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7343\n",
            "[*] LOSS: 0.19619094 / Q: {'Eval-Q': 0.980436, 'Target-Q': 1.036966}\n",
            "[*] LOSS: 0.17323186 / Q: {'Eval-Q': 1.39044, 'Target-Q': 1.387295}\n",
            "[*] LOSS: 0.1785371 / Q: {'Eval-Q': 0.726238, 'Target-Q': 0.791303}\n",
            "[*] LOSS: 0.14573696 / Q: {'Eval-Q': 1.19353, 'Target-Q': 1.102786}\n",
            "[*] LOSS: 0.043411147 / Q: {'Eval-Q': 0.863202, 'Target-Q': 0.68845}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.015673262205062682, 'total_reward': 254.83499917946756, 'kill': 58}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_240\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_240\n",
            "\n",
            "\n",
            "[*] ROUND #241, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005   ,  0.002143]), 'NUM': [34, 14]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.000652, -0.005   ]), 'NUM': [23, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [23, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.013696, -0.005   ]), 'NUM': [23, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 10614\n",
            "[*] LOSS: 0.16646554 / Q: {'Eval-Q': 1.094001, 'Target-Q': 0.990559}\n",
            "[*] LOSS: 0.33099312 / Q: {'Eval-Q': 1.494236, 'Target-Q': 1.547263}\n",
            "[*] LOSS: 0.14389133 / Q: {'Eval-Q': 0.880752, 'Target-Q': 0.788544}\n",
            "[*] LOSS: 0.24918173 / Q: {'Eval-Q': 0.925193, 'Target-Q': 1.016117}\n",
            "[*] LOSS: 0.053736426 / Q: {'Eval-Q': 0.916598, 'Target-Q': 0.886024}\n",
            "[*] LOSS: 0.22254112 / Q: {'Eval-Q': 1.5616, 'Target-Q': 1.621338}\n",
            "[*] LOSS: 0.08604489 / Q: {'Eval-Q': 1.220886, 'Target-Q': 1.225883}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.012964132520019036, 'total_reward': 248.33499911241233, 'kill': 62}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_241\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_241\n",
            "\n",
            "\n",
            "[*] ROUND #242, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.023571, 0.008636]), 'NUM': [14, 22]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 13]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 13]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 13]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 13]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 13]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 13]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 13]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4461\n",
            "[*] LOSS: 0.26574817 / Q: {'Eval-Q': 1.396778, 'Target-Q': 1.521395}\n",
            "[*] LOSS: 0.11322917 / Q: {'Eval-Q': 1.47464, 'Target-Q': 1.498794}\n",
            "[*] LOSS: 0.09908954 / Q: {'Eval-Q': 1.538729, 'Target-Q': 1.47689}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.022270575811462044, 'total_reward': 234.67999894171953, 'kill': 51}\n",
            "\n",
            "\n",
            "[*] ROUND #243, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 23]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 20]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 20]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 20]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 20]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 20]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 20]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 20]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2808\n",
            "[*] LOSS: 0.1242016 / Q: {'Eval-Q': 1.365821, 'Target-Q': 1.174759}\n",
            "[*] LOSS: 0.11873911 / Q: {'Eval-Q': 1.295655, 'Target-Q': 1.267036}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.017491093375602413, 'total_reward': 199.16499910969287, 'kill': 44}\n",
            "\n",
            "\n",
            "[*] ROUND #244, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.006765, -0.005   ]), 'NUM': [17, 16]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 6]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 6]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 6]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 6]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 6]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 6]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 6]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5019\n",
            "[*] LOSS: 0.2659894 / Q: {'Eval-Q': 1.760499, 'Target-Q': 1.668599}\n",
            "[*] LOSS: 0.14522325 / Q: {'Eval-Q': 2.059524, 'Target-Q': 2.031984}\n",
            "[*] LOSS: 0.4654756 / Q: {'Eval-Q': 1.65021, 'Target-Q': 1.365769}\n",
            "[*] LOSS: 0.1276656 / Q: {'Eval-Q': 1.281433, 'Target-Q': 1.30003}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.020375881369426828, 'total_reward': 257.28499875962734, 'kill': 58}\n",
            "\n",
            "\n",
            "[*] ROUND #245, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.023571, 0.025769]), 'NUM': [21, 13]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 4]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5881\n",
            "[*] LOSS: 0.32599303 / Q: {'Eval-Q': 0.929335, 'Target-Q': 1.044758}\n",
            "[*] LOSS: 0.24232289 / Q: {'Eval-Q': 1.194149, 'Target-Q': 0.996288}\n",
            "[*] LOSS: 0.26005566 / Q: {'Eval-Q': 1.176845, 'Target-Q': 1.347217}\n",
            "[*] LOSS: 0.026280705 / Q: {'Eval-Q': 1.157847, 'Target-Q': 1.194888}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01702183439392194, 'total_reward': 275.8649988314137, 'kill': 60}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_245\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_245\n",
            "\n",
            "\n",
            "[*] ROUND #246, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.000556, 0.018529]), 'NUM': [18, 17]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 9]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.019286, -0.005   ]), 'NUM': [7, 8]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.019286, -0.005   ]), 'NUM': [7, 8]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.019286, -0.005   ]), 'NUM': [7, 8]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.019286, -0.005   ]), 'NUM': [7, 8]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.019286, -0.005   ]), 'NUM': [7, 8]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.019286, -0.005   ]), 'NUM': [7, 8]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4579\n",
            "[*] LOSS: 0.24499422 / Q: {'Eval-Q': 1.560954, 'Target-Q': 1.701625}\n",
            "[*] LOSS: 0.09344493 / Q: {'Eval-Q': 1.275722, 'Target-Q': 1.284054}\n",
            "[*] LOSS: 0.20053342 / Q: {'Eval-Q': 1.199187, 'Target-Q': 1.121249}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.011948580090182492, 'total_reward': 231.88999762013555, 'kill': 56}\n",
            "\n",
            "\n",
            "[*] ROUND #247, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.005   , 0.003333]), 'NUM': [20, 12]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 5]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 5]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 5]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 5]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 5]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 5]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7661\n",
            "[*] LOSS: 0.10405113 / Q: {'Eval-Q': 1.573731, 'Target-Q': 1.399518}\n",
            "[*] LOSS: 0.07111181 / Q: {'Eval-Q': 1.249637, 'Target-Q': 1.143468}\n",
            "[*] LOSS: 0.072900556 / Q: {'Eval-Q': 1.225161, 'Target-Q': 1.073453}\n",
            "[*] LOSS: 0.45781174 / Q: {'Eval-Q': 1.035848, 'Target-Q': 1.052443}\n",
            "[*] LOSS: 0.10931793 / Q: {'Eval-Q': 1.38967, 'Target-Q': 1.366866}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.016472346740841134, 'total_reward': 250.13499876391143, 'kill': 59}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_247\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_247\n",
            "\n",
            "\n",
            "[*] ROUND #248, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.01375 , 0.004091]), 'NUM': [16, 11]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 4]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 4]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5621\n",
            "[*] LOSS: 0.111127526 / Q: {'Eval-Q': 1.262057, 'Target-Q': 1.326755}\n",
            "[*] LOSS: 0.07065401 / Q: {'Eval-Q': 1.277641, 'Target-Q': 1.309328}\n",
            "[*] LOSS: 0.12875196 / Q: {'Eval-Q': 1.036795, 'Target-Q': 1.01807}\n",
            "[*] LOSS: 0.18571573 / Q: {'Eval-Q': 1.585652, 'Target-Q': 1.383093}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.018545485986266482, 'total_reward': 270.769998553209, 'kill': 60}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_248\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_248\n",
            "\n",
            "\n",
            "[*] ROUND #249, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.028667, 0.171667]), 'NUM': [15, 30]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 14]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 14]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 14]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 14]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 14]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 14]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 14]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4302\n",
            "[*] LOSS: 0.21410319 / Q: {'Eval-Q': 1.319766, 'Target-Q': 1.226031}\n",
            "[*] LOSS: 0.075663306 / Q: {'Eval-Q': 1.346462, 'Target-Q': 1.320585}\n",
            "[*] LOSS: 0.1784737 / Q: {'Eval-Q': 1.293387, 'Target-Q': 1.328579}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.027009237747802103, 'total_reward': 223.6799991140142, 'kill': 50}\n",
            "\n",
            "\n",
            "[*] ROUND #250, EPS: 0.88 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.116757, 0.976   ]), 'NUM': [37, 5]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.0175,  0.045 ]), 'NUM': [24, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 10337\n",
            "[*] LOSS: 0.36686492 / Q: {'Eval-Q': 1.38242, 'Target-Q': 1.38274}\n",
            "[*] LOSS: 0.04032741 / Q: {'Eval-Q': 1.378175, 'Target-Q': 1.364269}\n",
            "[*] LOSS: 0.061942287 / Q: {'Eval-Q': 0.653405, 'Target-Q': 0.669832}\n",
            "[*] LOSS: 0.19549197 / Q: {'Eval-Q': 1.294233, 'Target-Q': 1.3008}\n",
            "[*] LOSS: 0.37697762 / Q: {'Eval-Q': 1.367985, 'Target-Q': 1.414075}\n",
            "[*] LOSS: 0.108761236 / Q: {'Eval-Q': 1.484458, 'Target-Q': 1.517435}\n",
            "[*] LOSS: 0.2195738 / Q: {'Eval-Q': 1.636181, 'Target-Q': 1.566485}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.008887459024441885, 'total_reward': 234.92999779898673, 'kill': 62}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_250\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_250\n",
            "\n",
            "\n",
            "[*] ROUND #251, EPS: 0.87 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.403333, 0.00875 ]), 'NUM': [24, 8]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 4]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 4]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 4]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 4]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 4]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 4]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 4]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7541\n",
            "[*] LOSS: 0.0672577 / Q: {'Eval-Q': 1.470815, 'Target-Q': 1.419513}\n",
            "[*] LOSS: 0.094200104 / Q: {'Eval-Q': 1.651029, 'Target-Q': 1.562697}\n",
            "[*] LOSS: 0.15160358 / Q: {'Eval-Q': 1.268902, 'Target-Q': 1.302278}\n",
            "[*] LOSS: 0.39442796 / Q: {'Eval-Q': 1.270138, 'Target-Q': 1.25566}\n",
            "[*] LOSS: 0.39025855 / Q: {'Eval-Q': 1.963677, 'Target-Q': 1.989333}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01444712567952265, 'total_reward': 266.93999854195863, 'kill': 60}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_251\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_251\n",
            "\n",
            "\n",
            "[*] ROUND #252, EPS: 0.87 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.014286, 0.428333]), 'NUM': [21, 12]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 7]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 7]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 7]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 7]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 7]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7482\n",
            "[*] LOSS: 0.081893496 / Q: {'Eval-Q': 1.44825, 'Target-Q': 1.43882}\n",
            "[*] LOSS: 0.110395715 / Q: {'Eval-Q': 1.038411, 'Target-Q': 1.070274}\n",
            "[*] LOSS: 0.2195156 / Q: {'Eval-Q': 1.362697, 'Target-Q': 1.205974}\n",
            "[*] LOSS: 0.14516477 / Q: {'Eval-Q': 1.289956, 'Target-Q': 1.355058}\n",
            "[*] LOSS: 0.12588297 / Q: {'Eval-Q': 1.253234, 'Target-Q': 1.271866}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.014812692930127817, 'total_reward': 257.6349991019815, 'kill': 57}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_252\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_252\n",
            "\n",
            "\n",
            "[*] ROUND #253, EPS: 0.87 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.045, -0.005]), 'NUM': [2, 36]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1775\n",
            "[*] LOSS: 0.07468184 / Q: {'Eval-Q': 1.222199, 'Target-Q': 0.928423}\n",
            "[*] LOSS: 0.15848549 / Q: {'Eval-Q': 1.15051, 'Target-Q': 1.066876}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08076537190671272, 'total_reward': 103.74499948136508, 'kill': 28}\n",
            "\n",
            "\n",
            "[*] ROUND #254, EPS: 0.87 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.322386, 0.621875]), 'NUM': [44, 8]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.007778, -0.005   ]), 'NUM': [36, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.007778, -0.005   ]), 'NUM': [36, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007778, -0.005   ]), 'NUM': [36, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.007778, -0.005   ]), 'NUM': [36, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007778, -0.005   ]), 'NUM': [36, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.007778, -0.005   ]), 'NUM': [36, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.007778, -0.005   ]), 'NUM': [36, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 15388\n",
            "[*] LOSS: 0.070969336 / Q: {'Eval-Q': 1.431574, 'Target-Q': 1.394011}\n",
            "[*] LOSS: 0.23705369 / Q: {'Eval-Q': 1.533732, 'Target-Q': 1.513296}\n",
            "[*] LOSS: 0.2387025 / Q: {'Eval-Q': 1.209473, 'Target-Q': 1.3061}\n",
            "[*] LOSS: 0.24640399 / Q: {'Eval-Q': 1.030592, 'Target-Q': 1.100479}\n",
            "[*] LOSS: 0.24916615 / Q: {'Eval-Q': 1.255974, 'Target-Q': 1.118292}\n",
            "[*] LOSS: 0.57470465 / Q: {'Eval-Q': 1.392066, 'Target-Q': 1.511382}\n",
            "[*] LOSS: 0.116916545 / Q: {'Eval-Q': 0.85765, 'Target-Q': 0.805998}\n",
            "[*] LOSS: 0.14134638 / Q: {'Eval-Q': 0.897654, 'Target-Q': 0.819091}\n",
            "[*] LOSS: 0.23144734 / Q: {'Eval-Q': 1.222799, 'Target-Q': 1.157361}\n",
            "[*] LOSS: 0.39565983 / Q: {'Eval-Q': 1.452712, 'Target-Q': 1.262091}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.00809524568406226, 'total_reward': 208.6999980667606, 'kill': 62}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_254\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_254\n",
            "\n",
            "\n",
            "[*] ROUND #255, EPS: 0.87 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.308065, -0.023   ]), 'NUM': [31, 5]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2501\n",
            "[*] LOSS: 0.05452592 / Q: {'Eval-Q': 1.002167, 'Target-Q': 1.016162}\n",
            "[*] LOSS: 0.2848618 / Q: {'Eval-Q': 1.108186, 'Target-Q': 1.172195}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1300206480317518, 'total_reward': 307.969997856766, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_255\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_255\n",
            "\n",
            "\n",
            "[*] ROUND #256, EPS: 0.87 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.179808, -0.004167]), 'NUM': [26, 6]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 9896\n",
            "[*] LOSS: 0.058281027 / Q: {'Eval-Q': 0.630282, 'Target-Q': 0.607513}\n",
            "[*] LOSS: 0.049785264 / Q: {'Eval-Q': 0.67762, 'Target-Q': 0.558963}\n",
            "[*] LOSS: 0.2672334 / Q: {'Eval-Q': 1.475084, 'Target-Q': 1.51643}\n",
            "[*] LOSS: 0.21468651 / Q: {'Eval-Q': 1.215295, 'Target-Q': 1.171993}\n",
            "[*] LOSS: 0.6096508 / Q: {'Eval-Q': 1.475116, 'Target-Q': 1.336049}\n",
            "[*] LOSS: 0.07887136 / Q: {'Eval-Q': 1.010539, 'Target-Q': 0.86745}\n",
            "[*] LOSS: 0.16475981 / Q: {'Eval-Q': 1.260191, 'Target-Q': 1.112167}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.015299125365810516, 'total_reward': 252.92999864183366, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_256\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_256\n",
            "\n",
            "\n",
            "[*] ROUND #257, EPS: 0.87 NUMBER: [64, 64]\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2215\n",
            "[*] LOSS: 0.08621892 / Q: {'Eval-Q': 1.343242, 'Target-Q': 1.251874}\n",
            "[*] LOSS: 0.3758452 / Q: {'Eval-Q': 0.923575, 'Target-Q': 1.029405}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.141431219573364, 'total_reward': 297.2849980024621, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_257\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_257\n",
            "\n",
            "\n",
            "[*] ROUND #258, EPS: 0.87 NUMBER: [64, 64]\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1618\n",
            "[*] LOSS: 0.47152016 / Q: {'Eval-Q': 1.644144, 'Target-Q': 1.555812}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1579728680387815, 'total_reward': 167.82999918609858, 'kill': 36}\n",
            "\n",
            "\n",
            "[*] ROUND #259, EPS: 0.87 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.008333,  1.955   ]), 'NUM': [27, 5]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 3]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 3]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 3]}\n",
            "> step #300, info: {'Ave-Reward': array([ 0.267222, -0.036667]), 'NUM': [18, 3]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 8532\n",
            "[*] LOSS: 0.14424327 / Q: {'Eval-Q': 0.952376, 'Target-Q': 0.91098}\n",
            "[*] LOSS: 0.12565838 / Q: {'Eval-Q': 0.819204, 'Target-Q': 0.877739}\n",
            "[*] LOSS: 0.47993553 / Q: {'Eval-Q': 1.11147, 'Target-Q': 1.08892}\n",
            "[*] LOSS: 0.13395263 / Q: {'Eval-Q': 1.269288, 'Target-Q': 1.284668}\n",
            "[*] LOSS: 0.12511213 / Q: {'Eval-Q': 1.62891, 'Target-Q': 1.516761}\n",
            "[*] LOSS: 0.09273295 / Q: {'Eval-Q': 1.222812, 'Target-Q': 1.128007}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.014326136000795846, 'total_reward': 262.8699986329302, 'kill': 62}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_259\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_259\n",
            "\n",
            "\n",
            "[*] ROUND #260, EPS: 0.87 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.002917, -0.005   ]), 'NUM': [48, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3403\n",
            "[*] LOSS: 0.14430991 / Q: {'Eval-Q': 1.857613, 'Target-Q': 1.529725}\n",
            "[*] LOSS: 0.0718063 / Q: {'Eval-Q': 0.585295, 'Target-Q': 0.608413}\n",
            "[*] LOSS: 0.09758863 / Q: {'Eval-Q': 0.745846, 'Target-Q': 0.737438}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08861084824238495, 'total_reward': 314.86499818693846, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_260\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_260\n",
            "\n",
            "\n",
            "[*] ROUND #261, EPS: 0.87 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([1.106667, 0.169444]), 'NUM': [9, 27]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 14]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 14]}\n",
            "> step #200, info: {'Ave-Reward': array([ 0.011667, -0.005   ]), 'NUM': [6, 14]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 14]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 14]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 14]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 14]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4019\n",
            "[*] LOSS: 0.21179105 / Q: {'Eval-Q': 1.164836, 'Target-Q': 1.289169}\n",
            "[*] LOSS: 0.10522396 / Q: {'Eval-Q': 0.669992, 'Target-Q': 0.681466}\n",
            "[*] LOSS: 0.09577649 / Q: {'Eval-Q': 1.087428, 'Target-Q': 1.091791}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03683156808598934, 'total_reward': 214.39499904308468, 'kill': 50}\n",
            "\n",
            "\n",
            "[*] ROUND #262, EPS: 0.87 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.036667,  0.095   ]), 'NUM': [3, 43]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 1672\n",
            "[*] LOSS: 0.2252958 / Q: {'Eval-Q': 1.265218, 'Target-Q': 1.241489}\n",
            "[*] LOSS: 0.12061474 / Q: {'Eval-Q': 1.134249, 'Target-Q': 1.237393}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.16221491908254773, 'total_reward': 98.45999955106527, 'kill': 22}\n",
            "\n",
            "\n",
            "[*] ROUND #263, EPS: 0.87 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.095, -0.005]), 'NUM': [2, 36]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.055, -0.005]), 'NUM': [2, 31]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.055, -0.005]), 'NUM': [2, 31]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 27]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 23]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 23]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 23]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 23]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2297\n",
            "[*] LOSS: 0.2437368 / Q: {'Eval-Q': 1.2687, 'Target-Q': 1.333855}\n",
            "[*] LOSS: 0.15720616 / Q: {'Eval-Q': 1.125066, 'Target-Q': 1.070482}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07612796289609318, 'total_reward': 181.72499837726355, 'kill': 41}\n",
            "\n",
            "\n",
            "[*] ROUND #264, EPS: 0.87 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.179615, 0.03    ]), 'NUM': [26, 3]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [22, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 9984\n",
            "[*] LOSS: 0.13864896 / Q: {'Eval-Q': 1.259853, 'Target-Q': 1.219621}\n",
            "[*] LOSS: 0.11298472 / Q: {'Eval-Q': 1.715293, 'Target-Q': 1.736958}\n",
            "[*] LOSS: 0.30933845 / Q: {'Eval-Q': 1.550202, 'Target-Q': 1.547208}\n",
            "[*] LOSS: 0.24815601 / Q: {'Eval-Q': 0.805776, 'Target-Q': 0.893668}\n",
            "[*] LOSS: 0.2044625 / Q: {'Eval-Q': 1.266104, 'Target-Q': 1.096282}\n",
            "[*] LOSS: 0.118826956 / Q: {'Eval-Q': 1.34735, 'Target-Q': 1.337009}\n",
            "[*] LOSS: 0.0471813 / Q: {'Eval-Q': 0.874286, 'Target-Q': 0.881963}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.014224111413270584, 'total_reward': 281.88999921269715, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_264\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_264\n",
            "\n",
            "\n",
            "[*] ROUND #265, EPS: 0.87 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.242632, 0.033125]), 'NUM': [19, 8]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5542\n",
            "[*] LOSS: 0.11831871 / Q: {'Eval-Q': 1.130339, 'Target-Q': 1.138604}\n",
            "[*] LOSS: 0.15545024 / Q: {'Eval-Q': 1.231419, 'Target-Q': 1.106704}\n",
            "[*] LOSS: 0.3998448 / Q: {'Eval-Q': 0.937073, 'Target-Q': 1.066924}\n",
            "[*] LOSS: 0.3576536 / Q: {'Eval-Q': 1.198209, 'Target-Q': 1.187949}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.018160944788016317, 'total_reward': 266.1599985733628, 'kill': 60}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_265\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_265\n",
            "\n",
            "\n",
            "[*] ROUND #266, EPS: 0.87 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.395208, 0.977   ]), 'NUM': [24, 5]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 3]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 3]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 3]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 3]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 3]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 8177\n",
            "[*] LOSS: 0.111434855 / Q: {'Eval-Q': 0.942936, 'Target-Q': 0.922868}\n",
            "[*] LOSS: 0.14906038 / Q: {'Eval-Q': 1.395472, 'Target-Q': 1.406758}\n",
            "[*] LOSS: 0.21606639 / Q: {'Eval-Q': 1.714162, 'Target-Q': 1.76638}\n",
            "[*] LOSS: 0.12202636 / Q: {'Eval-Q': 0.832202, 'Target-Q': 0.865548}\n",
            "[*] LOSS: 0.03595491 / Q: {'Eval-Q': 0.81083, 'Target-Q': 0.839998}\n",
            "[*] LOSS: 0.117153935 / Q: {'Eval-Q': 1.546576, 'Target-Q': 1.447854}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.014243310087440366, 'total_reward': 255.04999867081642, 'kill': 61}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_266\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_266\n",
            "\n",
            "\n",
            "[*] ROUND #267, EPS: 0.87 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.013182, 0.011667]), 'NUM': [11, 12]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.021667, -0.005   ]), 'NUM': [6, 6]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.021667, -0.005   ]), 'NUM': [6, 6]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.021667, -0.005   ]), 'NUM': [6, 6]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.021667, -0.005   ]), 'NUM': [6, 6]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.021667, -0.005   ]), 'NUM': [6, 6]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 6]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.021667, -0.005   ]), 'NUM': [6, 6]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4011\n",
            "[*] LOSS: 0.035031505 / Q: {'Eval-Q': 0.894462, 'Target-Q': 0.851782}\n",
            "[*] LOSS: 0.20387648 / Q: {'Eval-Q': 1.161501, 'Target-Q': 1.210596}\n",
            "[*] LOSS: 0.1106612 / Q: {'Eval-Q': 1.092497, 'Target-Q': 1.175384}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.0135106864197818, 'total_reward': 243.23499758262187, 'kill': 58}\n",
            "\n",
            "\n",
            "[*] ROUND #268, EPS: 0.87 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.267778, 1.106667]), 'NUM': [18, 9]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 5]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 5]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 5]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 5]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 5]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.016111, -0.005   ]), 'NUM': [9, 5]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5270\n",
            "[*] LOSS: 0.10287705 / Q: {'Eval-Q': 1.676772, 'Target-Q': 1.656358}\n",
            "[*] LOSS: 0.275307 / Q: {'Eval-Q': 1.388668, 'Target-Q': 1.47578}\n",
            "[*] LOSS: 0.09362086 / Q: {'Eval-Q': 1.407172, 'Target-Q': 1.463705}\n",
            "[*] LOSS: 0.20320854 / Q: {'Eval-Q': 1.876716, 'Target-Q': 1.94327}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.017971599615738455, 'total_reward': 269.42499843705446, 'kill': 59}\n",
            "\n",
            "\n",
            "[*] ROUND #269, EPS: 0.87 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.015, -0.005]), 'NUM': [5, 15]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 10]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 10]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 10]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 10]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 10]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 10]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 10]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3155\n",
            "[*] LOSS: 0.32354543 / Q: {'Eval-Q': 1.58653, 'Target-Q': 1.551732}\n",
            "[*] LOSS: 0.34598744 / Q: {'Eval-Q': 1.198703, 'Target-Q': 1.153501}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03070661295129974, 'total_reward': 244.6249988321215, 'kill': 54}\n",
            "\n",
            "\n",
            "[*] ROUND #270, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.002143, 0.013182]), 'NUM': [14, 11]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.016111, -0.005   ]), 'NUM': [9, 7]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 7]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 7]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 7]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 7]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5124\n",
            "[*] LOSS: 0.06112502 / Q: {'Eval-Q': 1.092058, 'Target-Q': 1.088934}\n",
            "[*] LOSS: 0.13086277 / Q: {'Eval-Q': 1.651421, 'Target-Q': 1.540968}\n",
            "[*] LOSS: 0.26864547 / Q: {'Eval-Q': 1.45702, 'Target-Q': 1.267081}\n",
            "[*] LOSS: 0.3687474 / Q: {'Eval-Q': 1.881734, 'Target-Q': 1.742212}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.020190085615826567, 'total_reward': 264.9549989020452, 'kill': 57}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_270\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_270\n",
            "\n",
            "\n",
            "[*] ROUND #271, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.031364, 0.004091]), 'NUM': [11, 11]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 7]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 7]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 7]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 7]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005   ,  0.011667]), 'NUM': [7, 6]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 6]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4400\n",
            "[*] LOSS: 0.08522898 / Q: {'Eval-Q': 1.267691, 'Target-Q': 1.22929}\n",
            "[*] LOSS: 0.08017228 / Q: {'Eval-Q': 1.207743, 'Target-Q': 1.088431}\n",
            "[*] LOSS: 0.10663392 / Q: {'Eval-Q': 1.310354, 'Target-Q': 1.195617}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.019994462927294007, 'total_reward': 263.78999861795455, 'kill': 58}\n",
            "\n",
            "\n",
            "[*] ROUND #272, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.061667, 0.01375 ]), 'NUM': [9, 16]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 6]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 6]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 6]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 6]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 6]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 6]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 6]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3601\n",
            "[*] LOSS: 0.3895827 / Q: {'Eval-Q': 1.320831, 'Target-Q': 1.372272}\n",
            "[*] LOSS: 0.46854645 / Q: {'Eval-Q': 2.06496, 'Target-Q': 1.580834}\n",
            "[*] LOSS: 0.44511616 / Q: {'Eval-Q': 1.557864, 'Target-Q': 1.565346}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03231809655908178, 'total_reward': 260.189998652786, 'kill': 58}\n",
            "\n",
            "\n",
            "[*] ROUND #273, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.24025 , 0.683125]), 'NUM': [20, 8]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 6]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 6]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 6]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.0175, -0.005 ]), 'NUM': [8, 6]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 6]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 6]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 5]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5024\n",
            "[*] LOSS: 0.11074028 / Q: {'Eval-Q': 0.808886, 'Target-Q': 0.749195}\n",
            "[*] LOSS: 0.16135117 / Q: {'Eval-Q': 1.398714, 'Target-Q': 1.354819}\n",
            "[*] LOSS: 0.07983798 / Q: {'Eval-Q': 1.298619, 'Target-Q': 1.379821}\n",
            "[*] LOSS: 0.4323251 / Q: {'Eval-Q': 1.387687, 'Target-Q': 1.485877}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.015575673091660691, 'total_reward': 272.25999841373414, 'kill': 59}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_273\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_273\n",
            "\n",
            "\n",
            "[*] ROUND #274, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.07   , 0.00125]), 'NUM': [4, 16]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 15]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 15]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 15]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 15]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 15]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 15]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 15]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2473\n",
            "[*] LOSS: 0.3743597 / Q: {'Eval-Q': 1.600999, 'Target-Q': 1.333501}\n",
            "[*] LOSS: 0.27035436 / Q: {'Eval-Q': 1.829968, 'Target-Q': 1.701797}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.024544703183092428, 'total_reward': 214.04499895311892, 'kill': 49}\n",
            "\n",
            "\n",
            "[*] ROUND #275, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 18]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 18]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 18]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 18]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 18]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 18]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 18]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2120\n",
            "[*] LOSS: 0.26050264 / Q: {'Eval-Q': 1.952373, 'Target-Q': 2.018084}\n",
            "[*] LOSS: 0.22105178 / Q: {'Eval-Q': 1.669495, 'Target-Q': 1.541448}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.022508152626552057, 'total_reward': 162.1149991080165, 'kill': 46}\n",
            "\n",
            "\n",
            "[*] ROUND #276, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.209091, 1.095556]), 'NUM': [22, 9]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 5]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 5]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 5]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 5]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 5]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 5]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6792\n",
            "[*] LOSS: 0.28147268 / Q: {'Eval-Q': 1.736041, 'Target-Q': 1.670206}\n",
            "[*] LOSS: 0.15520829 / Q: {'Eval-Q': 1.77741, 'Target-Q': 1.744823}\n",
            "[*] LOSS: 0.21242228 / Q: {'Eval-Q': 1.413695, 'Target-Q': 1.342292}\n",
            "[*] LOSS: 0.122044116 / Q: {'Eval-Q': 1.580272, 'Target-Q': 1.302595}\n",
            "[*] LOSS: 0.16844332 / Q: {'Eval-Q': 1.281429, 'Target-Q': 1.29796}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.015575168186395849, 'total_reward': 261.09499851055443, 'kill': 59}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_276\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_276\n",
            "\n",
            "\n",
            "[*] ROUND #277, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.005714, 0.049545]), 'NUM': [28, 11]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [14, 6]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [12, 6]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 6]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 6]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 6]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 6]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 6]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6695\n",
            "[*] LOSS: 0.18803155 / Q: {'Eval-Q': 1.500791, 'Target-Q': 1.487042}\n",
            "[*] LOSS: 0.2160928 / Q: {'Eval-Q': 1.632344, 'Target-Q': 1.626986}\n",
            "[*] LOSS: 0.1154079 / Q: {'Eval-Q': 2.124424, 'Target-Q': 2.005701}\n",
            "[*] LOSS: 0.41769898 / Q: {'Eval-Q': 2.004746, 'Target-Q': 1.883169}\n",
            "[*] LOSS: 0.280114 / Q: {'Eval-Q': 1.576782, 'Target-Q': 1.452139}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.012610450233345425, 'total_reward': 257.98999861907214, 'kill': 58}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_277\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_277\n",
            "\n",
            "\n",
            "[*] ROUND #278, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.9855  , 0.224048]), 'NUM': [10, 21]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 10]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 10]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 9]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 9]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005   , -0.016111]), 'NUM': [6, 9]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 9]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005   ,  0.006111]), 'NUM': [6, 9]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3898\n",
            "[*] LOSS: 0.10642168 / Q: {'Eval-Q': 1.213018, 'Target-Q': 1.264692}\n",
            "[*] LOSS: 0.28604403 / Q: {'Eval-Q': 1.998535, 'Target-Q': 2.119528}\n",
            "[*] LOSS: 0.09837561 / Q: {'Eval-Q': 1.864151, 'Target-Q': 1.662227}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.0350000223171409, 'total_reward': 243.89999897126108, 'kill': 55}\n",
            "\n",
            "\n",
            "[*] ROUND #279, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.028333, -0.001296]), 'NUM': [9, 27]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.025   ,  0.003333]), 'NUM': [5, 12]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 9]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 9]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 9]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 9]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 9]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 8]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3092\n",
            "[*] LOSS: 0.3106118 / Q: {'Eval-Q': 1.733291, 'Target-Q': 1.75316}\n",
            "[*] LOSS: 0.17073146 / Q: {'Eval-Q': 1.383423, 'Target-Q': 1.314053}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.051490372293052394, 'total_reward': 258.1449986221269, 'kill': 56}\n",
            "\n",
            "\n",
            "[*] ROUND #280, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.018077, 0.023571]), 'NUM': [13, 14]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 8]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 7]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 7]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 7]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 7]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 7]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 4]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4396\n",
            "[*] LOSS: 0.3931238 / Q: {'Eval-Q': 1.4623, 'Target-Q': 1.361301}\n",
            "[*] LOSS: 0.11267394 / Q: {'Eval-Q': 0.928468, 'Target-Q': 0.981789}\n",
            "[*] LOSS: 0.5796826 / Q: {'Eval-Q': 1.647277, 'Target-Q': 1.80561}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03009019082217939, 'total_reward': 261.2149988440797, 'kill': 60}\n",
            "\n",
            "\n",
            "[*] ROUND #281, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.002333, 0.895   ]), 'NUM': [15, 11]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 5]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 5]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 5]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.025]), 'NUM': [8, 5]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.025]), 'NUM': [8, 5]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.025]), 'NUM': [8, 5]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4737\n",
            "[*] LOSS: 0.22981095 / Q: {'Eval-Q': 1.604269, 'Target-Q': 1.592086}\n",
            "[*] LOSS: 0.29829636 / Q: {'Eval-Q': 1.949624, 'Target-Q': 1.855696}\n",
            "[*] LOSS: 0.14908715 / Q: {'Eval-Q': 1.227373, 'Target-Q': 1.18914}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.022614860271449183, 'total_reward': 269.1949988985434, 'kill': 59}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_281\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_281\n",
            "\n",
            "\n",
            "[*] ROUND #282, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.289118, -0.004643]), 'NUM': [17, 14]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 9]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.021667, -0.005   ]), 'NUM': [6, 9]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 7]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 7]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 7]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 7]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4272\n",
            "[*] LOSS: 0.37343135 / Q: {'Eval-Q': 1.779202, 'Target-Q': 1.581354}\n",
            "[*] LOSS: 0.3298734 / Q: {'Eval-Q': 1.859215, 'Target-Q': 1.578853}\n",
            "[*] LOSS: 0.1712808 / Q: {'Eval-Q': 2.080802, 'Target-Q': 1.951748}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.019595979441250844, 'total_reward': 258.5299985241145, 'kill': 57}\n",
            "\n",
            "\n",
            "[*] ROUND #283, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.23    , 0.031071]), 'NUM': [20, 14]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 7]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005   ,  0.009286]), 'NUM': [9, 7]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005   , -0.033571]), 'NUM': [9, 7]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 7]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 7]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5229\n",
            "[*] LOSS: 0.08859432 / Q: {'Eval-Q': 1.602916, 'Target-Q': 1.536387}\n",
            "[*] LOSS: 0.22482672 / Q: {'Eval-Q': 1.578059, 'Target-Q': 1.648944}\n",
            "[*] LOSS: 0.39358073 / Q: {'Eval-Q': 1.770482, 'Target-Q': 1.606424}\n",
            "[*] LOSS: 0.19149439 / Q: {'Eval-Q': 1.10418, 'Target-Q': 1.069101}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01745062635181455, 'total_reward': 256.4349987693131, 'kill': 57}\n",
            "\n",
            "\n",
            "[*] ROUND #284, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.289722, 0.307059]), 'NUM': [18, 17]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005   ,  0.011667]), 'NUM': [6, 6]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 6]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 6]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 6]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 5]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 5]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 5]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3752\n",
            "[*] LOSS: 0.33230713 / Q: {'Eval-Q': 2.490368, 'Target-Q': 2.119189}\n",
            "[*] LOSS: 0.2260072 / Q: {'Eval-Q': 1.773334, 'Target-Q': 1.671728}\n",
            "[*] LOSS: 0.47985193 / Q: {'Eval-Q': 2.019689, 'Target-Q': 2.060149}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02672052451954484, 'total_reward': 270.13999863900244, 'kill': 59}\n",
            "\n",
            "\n",
            "[*] ROUND #285, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.028889, 0.283889]), 'NUM': [9, 18]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 11]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 11]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 11]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 11]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 9]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 7]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2956\n",
            "[*] LOSS: 0.10867231 / Q: {'Eval-Q': 1.470121, 'Target-Q': 1.089388}\n",
            "[*] LOSS: 0.06549908 / Q: {'Eval-Q': 1.058967, 'Target-Q': 1.014412}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.05455735683061143, 'total_reward': 276.7249986361712, 'kill': 57}\n",
            "\n",
            "\n",
            "[*] ROUND #286, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.004524, 0.015   ]), 'NUM': [21, 5]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 8575\n",
            "[*] LOSS: 0.19145684 / Q: {'Eval-Q': 1.529509, 'Target-Q': 1.501926}\n",
            "[*] LOSS: 0.37195915 / Q: {'Eval-Q': 1.472515, 'Target-Q': 1.462663}\n",
            "[*] LOSS: 0.10154767 / Q: {'Eval-Q': 1.358872, 'Target-Q': 1.428493}\n",
            "[*] LOSS: 0.15699819 / Q: {'Eval-Q': 1.530859, 'Target-Q': 1.558566}\n",
            "[*] LOSS: 0.3549856 / Q: {'Eval-Q': 1.435908, 'Target-Q': 1.462024}\n",
            "[*] LOSS: 0.092730656 / Q: {'Eval-Q': 1.927207, 'Target-Q': 1.767723}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.019428469342981557, 'total_reward': 270.9499987391755, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_286\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_286\n",
            "\n",
            "\n",
            "[*] ROUND #287, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.278333, 0.016786]), 'NUM': [18, 14]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 8]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 7]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 7]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 7]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 7]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 7]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4688\n",
            "[*] LOSS: 0.2716761 / Q: {'Eval-Q': 1.465391, 'Target-Q': 1.460924}\n",
            "[*] LOSS: 0.5325429 / Q: {'Eval-Q': 1.986146, 'Target-Q': 1.972373}\n",
            "[*] LOSS: 0.23424278 / Q: {'Eval-Q': 1.716003, 'Target-Q': 1.64343}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.017949729354997553, 'total_reward': 250.0549983549863, 'kill': 57}\n",
            "\n",
            "\n",
            "[*] ROUND #288, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.330714, 0.0025  ]), 'NUM': [14, 14]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 7]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005   ,  0.009286]), 'NUM': [4, 7]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 5]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 5]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 5]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 5]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3656\n",
            "[*] LOSS: 0.24204531 / Q: {'Eval-Q': 1.624337, 'Target-Q': 1.605696}\n",
            "[*] LOSS: 0.40373266 / Q: {'Eval-Q': 1.604975, 'Target-Q': 1.450616}\n",
            "[*] LOSS: 0.11781951 / Q: {'Eval-Q': 1.76039, 'Target-Q': 1.454408}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.031693588198869166, 'total_reward': 276.5199986072257, 'kill': 59}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_288\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_288\n",
            "\n",
            "\n",
            "[*] ROUND #289, EPS: 0.86 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.004722,  0.495   ]), 'NUM': [18, 10]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005,  0.015]), 'NUM': [13, 5]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [12, 4]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [12, 4]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 4]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 4]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 4]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6309\n",
            "[*] LOSS: 0.21884447 / Q: {'Eval-Q': 1.540203, 'Target-Q': 1.674418}\n",
            "[*] LOSS: 0.4423447 / Q: {'Eval-Q': 1.86189, 'Target-Q': 1.977473}\n",
            "[*] LOSS: 0.17746088 / Q: {'Eval-Q': 1.686529, 'Target-Q': 1.61081}\n",
            "[*] LOSS: 0.13980925 / Q: {'Eval-Q': 1.466449, 'Target-Q': 1.531853}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.018032625249886736, 'total_reward': 267.61999881081283, 'kill': 60}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_289\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_289\n",
            "\n",
            "\n",
            "[*] ROUND #290, EPS: 0.85 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.006111, 0.023571]), 'NUM': [27, 7]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 8352\n",
            "[*] LOSS: 0.13747597 / Q: {'Eval-Q': 1.280257, 'Target-Q': 1.249554}\n",
            "[*] LOSS: 0.15779376 / Q: {'Eval-Q': 2.09107, 'Target-Q': 2.088024}\n",
            "[*] LOSS: 0.19457027 / Q: {'Eval-Q': 0.969936, 'Target-Q': 0.971458}\n",
            "[*] LOSS: 0.17729661 / Q: {'Eval-Q': 2.034988, 'Target-Q': 2.015213}\n",
            "[*] LOSS: 0.12525754 / Q: {'Eval-Q': 1.166933, 'Target-Q': 1.21714}\n",
            "[*] LOSS: 0.10307478 / Q: {'Eval-Q': 1.27251, 'Target-Q': 1.201655}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.014040299353247156, 'total_reward': 271.7749984720722, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_290\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_290\n",
            "\n",
            "\n",
            "[*] ROUND #291, EPS: 0.85 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.010556, -0.005   ]), 'NUM': [18, 10]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [12, 4]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [12, 4]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 4]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 4]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 4]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 4]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6128\n",
            "[*] LOSS: 0.119997315 / Q: {'Eval-Q': 1.82749, 'Target-Q': 1.59134}\n",
            "[*] LOSS: 0.4844732 / Q: {'Eval-Q': 1.566878, 'Target-Q': 1.38826}\n",
            "[*] LOSS: 0.21087947 / Q: {'Eval-Q': 1.431638, 'Target-Q': 1.518682}\n",
            "[*] LOSS: 0.15192752 / Q: {'Eval-Q': 1.18774, 'Target-Q': 1.164839}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.019630225835034907, 'total_reward': 262.0249984115362, 'kill': 61}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_291\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_291\n",
            "\n",
            "\n",
            "[*] ROUND #292, EPS: 0.85 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.458636, -0.004792]), 'NUM': [11, 24]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.025, -0.005]), 'NUM': [5, 15]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 15]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 15]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 15]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 15]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 15]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 15]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3867\n",
            "[*] LOSS: 0.18270385 / Q: {'Eval-Q': 1.25799, 'Target-Q': 1.1072}\n",
            "[*] LOSS: 0.2448838 / Q: {'Eval-Q': 1.405063, 'Target-Q': 1.49378}\n",
            "[*] LOSS: 0.1628456 / Q: {'Eval-Q': 1.854755, 'Target-Q': 1.810223}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.024821920430322945, 'total_reward': 221.45999884046614, 'kill': 49}\n",
            "\n",
            "\n",
            "[*] ROUND #293, EPS: 0.85 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.7025  , 0.248158]), 'NUM': [14, 19]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 14]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 14]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 12]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 12]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 12]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 12]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 12]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5255\n",
            "[*] LOSS: 0.22975947 / Q: {'Eval-Q': 0.993336, 'Target-Q': 0.955999}\n",
            "[*] LOSS: 0.12741089 / Q: {'Eval-Q': 1.430151, 'Target-Q': 1.360649}\n",
            "[*] LOSS: 0.28809994 / Q: {'Eval-Q': 1.442543, 'Target-Q': 1.428012}\n",
            "[*] LOSS: 0.12734203 / Q: {'Eval-Q': 1.387951, 'Target-Q': 1.403257}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.019401052146328136, 'total_reward': 225.39999884273857, 'kill': 52}\n",
            "\n",
            "\n",
            "[*] ROUND #294, EPS: 0.85 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.2675  , 0.307812]), 'NUM': [18, 16]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [12, 11]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [12, 11]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [12, 11]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [12, 11]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [12, 11]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [12, 11]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [12, 11]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6393\n",
            "[*] LOSS: 0.14982311 / Q: {'Eval-Q': 0.961524, 'Target-Q': 1.029371}\n",
            "[*] LOSS: 0.37820655 / Q: {'Eval-Q': 1.211056, 'Target-Q': 1.159733}\n",
            "[*] LOSS: 0.21445175 / Q: {'Eval-Q': 1.527057, 'Target-Q': 1.428683}\n",
            "[*] LOSS: 0.4169133 / Q: {'Eval-Q': 2.055108, 'Target-Q': 1.981768}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.015250687559508806, 'total_reward': 235.99499900080264, 'kill': 53}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_294\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_294\n",
            "\n",
            "\n",
            "[*] ROUND #295, EPS: 0.85 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.00125 , 0.036667]), 'NUM': [16, 12]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.02 , -0.005]), 'NUM': [4, 4]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 3]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 3]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 3]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 3]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 3]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3686\n",
            "[*] LOSS: 0.5126316 / Q: {'Eval-Q': 2.089433, 'Target-Q': 1.987654}\n",
            "[*] LOSS: 0.24820092 / Q: {'Eval-Q': 1.300569, 'Target-Q': 1.394153}\n",
            "[*] LOSS: 0.29796124 / Q: {'Eval-Q': 1.136039, 'Target-Q': 1.092457}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02900285100703916, 'total_reward': 281.3699980704114, 'kill': 61}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_295\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_295\n",
            "\n",
            "\n",
            "[*] ROUND #296, EPS: 0.85 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.338214, 0.302   ]), 'NUM': [14, 15]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2451\n",
            "[*] LOSS: 0.07801488 / Q: {'Eval-Q': 1.234634, 'Target-Q': 1.219618}\n",
            "[*] LOSS: 0.07640967 / Q: {'Eval-Q': 1.316755, 'Target-Q': 1.269005}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.15959409272613678, 'total_reward': 304.32499813847244, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_296\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_296\n",
            "\n",
            "\n",
            "[*] ROUND #297, EPS: 0.85 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.572778, -0.0125  ]), 'NUM': [9, 26]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.955   , -0.012917]), 'NUM': [5, 12]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 11]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 11]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 11]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 11]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 11]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 11]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3883\n",
            "[*] LOSS: 0.06583392 / Q: {'Eval-Q': 1.429883, 'Target-Q': 1.466573}\n",
            "[*] LOSS: 0.09172835 / Q: {'Eval-Q': 1.210167, 'Target-Q': 1.139188}\n",
            "[*] LOSS: 0.13629709 / Q: {'Eval-Q': 1.974868, 'Target-Q': 1.902467}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03815000084651038, 'total_reward': 252.47999868821353, 'kill': 53}\n",
            "\n",
            "\n",
            "[*] ROUND #298, EPS: 0.85 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.004167,  0.153621]), 'NUM': [6, 29]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.035   , -0.009762]), 'NUM': [5, 21]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.055, -0.005]), 'NUM': [2, 15]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.055, -0.005]), 'NUM': [2, 15]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.055, -0.005]), 'NUM': [2, 15]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.055, -0.005]), 'NUM': [2, 15]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.055, -0.005]), 'NUM': [2, 15]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.055, -0.005]), 'NUM': [2, 15]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 2979\n",
            "[*] LOSS: 0.118377626 / Q: {'Eval-Q': 1.506038, 'Target-Q': 1.307043}\n",
            "[*] LOSS: 0.25643438 / Q: {'Eval-Q': 1.035095, 'Target-Q': 1.102796}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.015644771004195927, 'total_reward': 194.61499764304608, 'kill': 49}\n",
            "\n",
            "\n",
            "[*] ROUND #299, EPS: 0.85 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.00125, -0.005  ]), 'NUM': [16, 11]}\n",
            "> step #100, info: {'Ave-Reward': array([0.006111, 0.028333]), 'NUM': [9, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5070\n",
            "[*] LOSS: 0.14529067 / Q: {'Eval-Q': 1.279476, 'Target-Q': 1.203597}\n",
            "[*] LOSS: 0.35805646 / Q: {'Eval-Q': 1.575649, 'Target-Q': 1.545493}\n",
            "[*] LOSS: 0.34884045 / Q: {'Eval-Q': 1.429262, 'Target-Q': 1.567688}\n",
            "[*] LOSS: 0.34907123 / Q: {'Eval-Q': 1.512022, 'Target-Q': 1.390905}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02463371801724408, 'total_reward': 290.92999847978354, 'kill': 62}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_299\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_299\n",
            "\n",
            "\n",
            "[*] ROUND #300, EPS: 0.85 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.025   , -0.020385]), 'NUM': [15, 13]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6349\n",
            "[*] LOSS: 0.20539483 / Q: {'Eval-Q': 0.900344, 'Target-Q': 0.886178}\n",
            "[*] LOSS: 0.21129674 / Q: {'Eval-Q': 1.470042, 'Target-Q': 1.178816}\n",
            "[*] LOSS: 0.4151889 / Q: {'Eval-Q': 2.072926, 'Target-Q': 1.98389}\n",
            "[*] LOSS: 0.1417778 / Q: {'Eval-Q': 1.481477, 'Target-Q': 1.293315}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.024037993531862093, 'total_reward': 268.6199981290847, 'kill': 63}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_300\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_300\n",
            "\n",
            "\n",
            "[*] ROUND #301, EPS: 0.85 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.187593, 0.001562]), 'NUM': [27, 16]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.010556, -0.005   ]), 'NUM': [18, 7]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.010556, -0.005   ]), 'NUM': [18, 7]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.010556, -0.005   ]), 'NUM': [18, 7]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 7]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 7]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 8993\n",
            "[*] LOSS: 0.11084683 / Q: {'Eval-Q': 1.13031, 'Target-Q': 1.172566}\n",
            "[*] LOSS: 0.21826015 / Q: {'Eval-Q': 1.30892, 'Target-Q': 1.331368}\n",
            "[*] LOSS: 0.116618626 / Q: {'Eval-Q': 1.444928, 'Target-Q': 1.35819}\n",
            "[*] LOSS: 0.12676007 / Q: {'Eval-Q': 1.071125, 'Target-Q': 1.018014}\n",
            "[*] LOSS: 0.09158218 / Q: {'Eval-Q': 0.802168, 'Target-Q': 0.782933}\n",
            "[*] LOSS: 0.1388555 / Q: {'Eval-Q': 1.692113, 'Target-Q': 1.442188}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01196094025660757, 'total_reward': 224.46499819960445, 'kill': 57}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_301\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_301\n",
            "\n",
            "\n",
            "[*] ROUND #302, EPS: 0.85 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.182708, 0.359643]), 'NUM': [24, 14]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.016111, -0.005   ]), 'NUM': [9, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.016111, -0.005   ]), 'NUM': [9, 3]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 3]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 3]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.016111, -0.005   ]), 'NUM': [9, 3]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 3]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5881\n",
            "[*] LOSS: 0.28165242 / Q: {'Eval-Q': 1.315783, 'Target-Q': 1.149467}\n",
            "[*] LOSS: 0.08317041 / Q: {'Eval-Q': 0.951368, 'Target-Q': 0.949192}\n",
            "[*] LOSS: 0.3240632 / Q: {'Eval-Q': 1.16263, 'Target-Q': 1.10692}\n",
            "[*] LOSS: 0.058121797 / Q: {'Eval-Q': 1.028662, 'Target-Q': 1.058034}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.015982630873189005, 'total_reward': 265.56999785639346, 'kill': 61}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_302\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_302\n",
            "\n",
            "\n",
            "[*] ROUND #303, EPS: 0.85 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([0.295   , 0.001562]), 'NUM': [16, 16]}\n",
            "> step #100, info: {'Ave-Reward': array([ 1.628333, -0.004545]), 'NUM': [3, 11]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 10]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 10]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 10]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 10]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 10]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 10]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3206\n",
            "[*] LOSS: 0.24325615 / Q: {'Eval-Q': 1.048888, 'Target-Q': 1.05602}\n",
            "[*] LOSS: 0.22018239 / Q: {'Eval-Q': 1.307724, 'Target-Q': 1.406773}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02213013252133987, 'total_reward': 253.7799980333075, 'kill': 54}\n",
            "\n",
            "\n",
            "[*] ROUND #304, EPS: 0.85 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 15]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 10]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 9]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 9]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 9]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 9]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 9]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 9]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3304\n",
            "[*] LOSS: 0.33244494 / Q: {'Eval-Q': 1.393563, 'Target-Q': 1.318525}\n",
            "[*] LOSS: 0.09430723 / Q: {'Eval-Q': 1.167852, 'Target-Q': 1.171504}\n",
            "[*] LOSS: 0.05847749 / Q: {'Eval-Q': 1.179494, 'Target-Q': 1.219848}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02021092942009524, 'total_reward': 251.58499818108976, 'kill': 55}\n",
            "\n",
            "\n",
            "[*] ROUND #305, EPS: 0.85 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011667, -0.021667]), 'NUM': [15, 12]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005 , -0.0175]), 'NUM': [8, 8]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005 , -0.0175]), 'NUM': [8, 8]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005 , -0.0175]), 'NUM': [8, 8]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005 , -0.0175]), 'NUM': [8, 8]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005 , -0.0175]), 'NUM': [8, 8]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005 , -0.0175]), 'NUM': [8, 8]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005 , -0.0175]), 'NUM': [8, 8]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5013\n",
            "[*] LOSS: 0.14409669 / Q: {'Eval-Q': 1.28214, 'Target-Q': 1.276267}\n",
            "[*] LOSS: 0.46649593 / Q: {'Eval-Q': 1.678486, 'Target-Q': 1.769732}\n",
            "[*] LOSS: 0.20770402 / Q: {'Eval-Q': 1.192136, 'Target-Q': 1.260534}\n",
            "[*] LOSS: 0.047021132 / Q: {'Eval-Q': 1.297794, 'Target-Q': 1.356439}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.017886615336140898, 'total_reward': 263.1149985268712, 'kill': 56}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_305\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_305\n",
            "\n",
            "\n",
            "[*] ROUND #306, EPS: 0.85 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.019286, -0.011667]), 'NUM': [14, 15]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 4]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 3]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 3]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 3]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 3]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 3]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3933\n",
            "[*] LOSS: 0.41025683 / Q: {'Eval-Q': 1.644399, 'Target-Q': 1.677555}\n",
            "[*] LOSS: 0.08080314 / Q: {'Eval-Q': 1.595153, 'Target-Q': 1.545567}\n",
            "[*] LOSS: 0.27571973 / Q: {'Eval-Q': 1.220029, 'Target-Q': 1.282447}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02957120564622839, 'total_reward': 283.6349982479587, 'kill': 61}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved at: /content/data/models/mfq-0/dqn_306\n",
            "[*] Model saved at: /content/data/models/mfq-1/dqn_306\n",
            "\n",
            "\n",
            "[*] ROUND #307, EPS: 0.85 NUMBER: [64, 64]\n",
            "> step #50, info: {'Ave-Reward': array([-0.01,  0.37]), 'NUM': [19, 12]}\n",
            "> step #100, info: {'Ave-Reward': array([0.017222, 0.015   ]), 'NUM': [9, 5]}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}